#+Latex_Class: copernicus
#+AUTHOR: 
#+LaTeX_CLASS_OPTIONS: [tc, manuscript]
#+Options: toc:nil ^:t {}:t

#+PROPERTY: header-args: :comments both
#+PROPERTY: header-args:sh :eval no-export :noweb yes
#+PROPERTY: header-args:jupyter-python :eval no-export :noweb yes
#+PROPERTY: header-args: :session ice_discharge

#+MACRO: JAKO @@latex:Sermeq Kujalleq@@
#+EXCLUDE_TAGS: noexport

#+BEGIN_EXPORT LaTeX
\title{Greenland Ice Sheet solid ice discharge from 1986 through July 2019}
\Author[1]{Kenneth D.}{Mankoff}
\Author[1]{William}{Colgan}
\Author[1]{Anne}{Solgaard}
\Author[1]{Nanna B.}{Karlsson}
\Author[1]{Andreas P.}{Ahlstrøm}
\Author[1]{Dirk}{van As}
\Author[1]{Jason E.}{Box}
\Author[2]{Shfaqat Abbas}{Khan}
\Author[1]{Kristian K.}{Kjeldsen}
\Author[3]{Jeremie}{Mouginot}
\Author[1]{Robert S.}{Fausto}
\affil[1]{Department of Glaciology and Climate, Geological Survey of Denmark and Greenland (GEUS), Copenhagen, Denmark}
\affil[2]{DTU Space, National Space Institute, Department of Geodesy, Technical University of Denmark, Kgs. Lyngby, Denmark}
\affil[3]{Department of Earth System Science, University of California, Irvine, CA, USA}
\runningtitle{Greenland ice sheet solid ice discharge from 1986 through 2018}
\runningauthor{K. D. Mankoff \textit{et al.}}
\correspondence{Ken Mankoff (kdm@geus.dk)}

\received{}
\pubdiscuss{}
\revised{}
\accepted{}
\published{}
%% These dates will be inserted by ACPD
\firstpage{1}
\maketitle

# \newcommand{\textcite}[1]{\citet{#1}}
# \newcommand{\autocite}[1]{\citep{#1}}
#+END_EXPORT


#+BEGIN_abstract
We present a 1986 through July 2019 estimate of Greenland Ice Sheet ice discharge. Our data include all discharging ice that flows faster than 100 m yr^{-1} and are generated through an automatic and adaptable method, as opposed to conventional hand-picked gates. We position gates near the present-year termini and estimate problematic bed topography (ice thickness) values where necessary. In addition to using annual time-varying ice thickness, our time series uses velocity maps that begin with sparse spatial and temporal coverage and ends with near-complete spatial coverage and six-day updates to velocity. The 2010 through 2018 average ice discharge through the flux-gates is ~488 \(\pm\)49 Gt yr^{-1}. The 10 % uncertainty stems primarily from uncertain ice bed location (ice thickness). We attribute the ~50 Gt yr^{-1} differences among our results and previous studies to our use of updated bed topography from BedMachine v3. Discharge is approximately steady from 1986 to 2000, increases sharply from 2000 to 2005, then is approximately steady again. However, regional and glacier variability is more pronounced, with recent decreases at most major glaciers and in all but one region offset by increases in the NW region. As part of the journal's living archive option, all input data, code, and results from this study will be updated when new input data are accessible and made freely available at doi:10.22008/promice/data/ice_discharge.
#+END_abstract

# For 2000 to 2018 mean see LINK: [[id:eca400fe-cffc-4e6b-8fc1-694945093adb][2000 to 2018 mean]]


* README                                                :noexport:

This document is an Emacs Org Mode plain-text file with code and text embedded. If you are viewing:

+ A DOC or PDF file, then it was generated by exporting from Org. Not all of the Org parts (code, results, comments, etc.) were exported. The Org source file is available upon request, and may be embedded in the PDF. Most non-Apple PDF viewers provide easy access to embedded or attached files.
 
+ A file with a =org= extension in something other than Emacs, then you are seeing the canonical version and the full source, but without any syntax highlighting, document structure, or the ability to execute the code blocks.

+ An =Org= file within Emacs, then this is the canonical version. You should be able to fully interact and reproduce the contents of this document, although it may require 3rd-party applications (Python, etc.) and a similar Emacs configuration. This is available upon request.

** Workflow

To recreate this work

+ check that you have the necessary software dependencies installed. See section: [[*Code][Code]].
+ Download and set up the necessary data files used throughout the [[*Input data][Input data]] section.
+ Open this file in Emacs Org Mode.
+ Tangle the embedded code blocks.
+ Execute =./runme.sh=

* Summary                                               :noexport:

# A short summary [...] 500-character (incl. spaces) non-technical text that may be used to promote your work to a broader audience. It should highlight your main conclusions and results, and what the implications are. If possible, please also summarize briefly why you did the research and how you did it.

We have produced an open and reproducible estimate of Greenland ice sheet solid ice discharge from 1986 through July 2019. Our results show three modes at the total ice-sheet scale: Steady discharge from 1986 through 2000, increasing discharge from 2000 through 2005, steady discharge from 2005 through 2019. The behavior of individual sectors and glaciers is more complicated. This work was done to provide a 100% reproducible estimate to help constrain mass balance and sea level rise estimates. 

* What's new in this version

#+BEGIN_SRC sh :results verbatim
ls ${DATADIR}/MEaSUREs/NSIDC-0731.001/|grep greenland_vel_mosaic|wc -l
#+END_SRC
#+RESULTS:
: 48

The data has been updated repeatedly between the first version of this paper citep:mankoff_2019_ice and this version. The data will continue to be updated, often sub-monthly, although reference papers will only come out once or twice a year. Therefore users are encouraged to regularly check for data updates at doi:10.22008/promice/data/ice_discharge when using the data.

A post-peer-review website is available at https://github.com/mankoff/ice_discharge where we document changes to the paper and use the GitHub Issues feature to collect suggested improvements to the paper, document those improvements as they are implemented, document problems that made it through review, and mention similar papers that have been published since this was accepted.

This update includes 48 additional MEaSUREs monthly average velocity maps from 2014-12-01 through 2018-11-30, and 49 Sentinel 1 velocity maps from 2018 through 2019-07-23, increases data density and coverage from 2014 through 2019, and extends the time series to 2019-07-23. Newly incorporated data come from incorporation of a new data set (MEaSUREs NSIDC 0731 citet:NSIDC_0731,joughin_2010_greenland,joughin_2018_greenland) and additional data from Sentinel 1.

Results show a continued steady total discharge. The contributions from the central west (CW) region continue to decrease, while the central east (CE) region continues to increase, and they are now approximately tied for the 3rd largest discharging sector. The top three individual contributing glaciers remain dynamic - Sermeq Kujalleq (Jakobshavn Isbræ) continues its rapid discharge decline, returning to, approximately, its discharge from year 2000. If the trend continues, within a year or two the top contributor to sea level rise will once again be Helheim  Gletsjer, then Kangerlussuaq Gletsjer, then Sermeq Kujalleq in third place. The near-term future behavior of Sermeq Kujalleq is not known, but recent changes at Kangerlussuaq suggests it is likely to increase its discharge citep:brough_2019_exceptional.

* Introduction
:PROPERTIES:
:ID:       194336aa-b363-4546-a0c9-dcc2a99affd0
:END:

The mass of the Greenland ice sheet is decreasing (e.g. citet:fettweis_2017_reconstructions,van-den-broeke_2017_greenland,wiese_2016_jpl,khan_2016_geodetic). Most ice sheet mass loss -- as iceberg discharge, submarine melting, and meltwater runoff -- enters the fjords and coastal seas, and therefore ice sheet mass loss directly contributes to sea-level rise citep:wcrp_2018,moon_2018_rising,nerem_2018_climate,chen_2017_increasing. Greenland's total ice loss can be estimated through a variety of independent methods, for example 'direct' mass change estimates from GRACE citep:wiese_2016_jpl or by using satellite altimetry to estimate surface elevation change, which is then converted into mass change (using a firn model, e.g. citet:khan_2016_geodetic). However, partitioning the mass loss between ice discharge (D) and surface mass balance (SMB) remains challenging (c.f. citet:rignot_2008_mass and citet:enderlin_2014_improved). Correctly assessing mass loss, as well as the attribution of this loss (SMB or D) is critical to understanding the process-level response of the Greenland ice sheet to climate change, and thus improving models of future ice-sheet changes and associated sea-level rise citep:moon_2018_rising.

The total mass of an ice-sheet, or a drainage basin, changes if the mass gain (SMB inputs, primarily snowfall) is not balanced by the mass loss (D and SMB outputs, the latter generally meltwater runoff). This change is typically termed ice-sheet mass balance (MB) and the formal expression for this rate of change in mass is (e.g. citet:cuffey_2010_the-physics),

#+NAME: eq:dMdt
\begin{equation}
\frac{\mathrm{d}M}{\mathrm{d}t} = \rho \int_A b \, \mathrm{d}A - \int_g Q \, \mathrm{d}g,
\end{equation}

where \(\rho\) is the average density of ice, \(b\) is an area mass balance, and \(Q\) is the discharge flux. The left hand side of the equation is the rate of change of mass, the first term on the right hand side is the area \(A\) integrated surface mass balance (SMB), and the second term is the discharge \(D\) mass flow rate that drains through gate \(g\). Equation [[eq:dMdt]] is often simplified to

#+NAME: eq:MB
\begin{equation}
MB = SMB - D
\end{equation}

where \(MB\) is the mass balance, and referred to as the "input-output" method (e.g. citet:khan_2015_greenland). Virtually all studies agree on the trend of Greenland mass balance, but large discrepancies persist in both the magnitude and attribution. Magnitude discrepancies include, for example, citet:kjeldsen_2015_spatial reporting a mass imbalance of -250 \(\pm\) 21 Gt yr^{-1} during 2003 to 2010, citet:ewert_2012_volume reporting -181 \(\pm\) 28 Gt yr^{-1} during 2003 to 2008, and citet:rignot_2008_mass reporting a mass imbalance of -265 \(\pm\) 19 Gt yr^{-1} during 2004 to 2008. Some of these differences may be due to different ice sheet area masks used in the studies. Attribution discrepancies include, for example, citet:enderlin_2014_improved attributing the majority (64 %) of mass loss to changes in SMB during the 2005 to 2009 period but citet:rignot_2008_mass attributing the majority (85 %) of mass loss to changes in D during the 2004 to 2008 period.

# Rignot 2008: TMB (2004-2007) = (231+293+265+267)/4 = 264 Gt yr-1  # Table 1
# Enderlin 2014: TMB (2005-2009) = 265 \pm 18 Gt yr-1 # text in Section 3
# Kjeldsen 2015: TMB (2003-2010) = 186 \pm 18.9 Gt yr-1 # Table 1

#+BEGIN_SRC jupyter-python :session tmp :exports none :results raw drawer
from uncertainties import unumpy
import numpy as np
np.mean(unumpy.uarray([231,293,265,267], [40,39,39,38]))
#+END_SRC

#+RESULTS:
: 264.0+/-19.503204864842086

# error %
# Rignot: 19.5/264*100 = 7.38636363636
# Enderlin: 18/265*100 = 6.79245283019
# Kjeldsen: 19/186*100 = 10.2150537634

# When ice-sheet wide MB does agree between two independent studies, there may be spatially and/or temporally compensating errors underlying this outward agreement. Spatial discrepancies include, for example, citet:enderlin_2014_improved and citet:king_2018_seasonal. Temporal discrepancies include, for example, ???. A substantial portion of underlying spatial and temporal discrepancies may be associated with sampling bias. Underlying sampling discrepancy includes, for example, demonstrating agreement between satellite gravimetry derived mass loss estimates that sample Greenland peripheral glaciers and an input-output assessment that does not sample these disproportionately high mass loss glaciers citep:xu_2016_improved,noel_2017_greenland.


Discharge may be calculated through several methods, including mass flow rate through gates (e.g. citet:enderlin_2014_improved,king_2018_seasonal), or solving as a residual from independent mass balance terms (e.g. citet:kjaer_2012_aerial,kjeldsen_2015_spatial). The gate method that we use in this study incorporates ice thickness and an estimated vertical profile from the observed surface velocity to calculate the discharge. A typical formulation of discharge across a gate \(D_g\) is,

#+NAME: eq:Q
\begin{equation}
D_g = \rho \, V \, H \, w,
\end{equation}

where \(\rho\) is the average density of ice, \(V\) is depth-average gate-perpendicular velocity, \(H\) is the ice thickness, and \(w\) is the gate width. Uncertainties in \(V\) and \(H\) naturally influence the estimated discharge. At fast-flowing outlet glaciers, \(V\) is typically assumed to be equal at all ice depths, and observed surface velocities can be directly translated into depth-averaged velocities (as in citet:enderlin_2014_improved,king_2018_seasonal). To minimize uncertainty from SMB or basal mass balance corrections downstream of a flux gate, the gate should be at the grounding line of the outlet glacier. Unfortunately, uncertainty in bed elevation (translating to ice thickness uncertainty) increases toward the grounding line.

Conventional methods of gate selection involve hand-picking gate locations, generally as linear features (e.g. citet:enderlin_2014_improved) or visually approximating ice-orthogonal gates at one point in time (e.g. citet:king_2018_seasonal). Manual gate definition is sub-optimal. For example, the largest discharging glaciers draw from an upstream radially-diffusing region that may not easily be represented by a single linear gate. Approximately flow-orthogonal curved gates may not be flow-orthogonal on the multi-decade time scale due to changing flow directions. Manual gate selection makes it difficult to update gate locations, corresponding with glacier termini retreat or advance, in a systematic and reproducible fashion. We therefore adopt an algorithmic approach to generate gates based on a range of criteria.

Here, we present a discharge dataset based on gates selected in a reproducible fashion by a new algorithm. Relative to previous studies, we employ ice velocity observation over a longer period with higher temporal frequency and denser spatial coverage. We use ice velocity from 1986 through 2019 including six-day velocities for the last ~500 days of the time series, and discharge at 200 m pixel resolution capturing all ice flowing faster than 100 m yr^{-1} that crosses glacier termini into fjords.

* Input data

Historically, discharge gates were selected along well-constrained flight-lines of airborne radar data citep:enderlin_2014_improved. Recent advances in ice thickness estimates through NASA Operation IceBridge citep:millan_2018_vulnerability, NASA Oceans Melting Greenland (OMG; citet:fenty_2016_oceans), fjord bathymetry citep:tinto_2015_bathymetry, and methods to estimate thickness from surface properties (e.g. citet:mcnabb_2012_using,james_2016_automated) have been combined into digital bed elevation models such as BedMachine v3 citep:morlighem_2017_bedmachine,NSIDC_BedMachine or released as independent datasets citep:millan_2018_vulnerability. From these advances, digital bed elevation models have become more robust at tidewater glacier termini and grounding lines. The incorporation of flight-line ice thickness data into higher-level products that include additional methods and data means gates are no longer limited to flight-lines (e.g. citet:king_2018_seasonal).

Ice velocity data are available with increasing spatial and temporal resolution (e.g. citet:vijay_2019_resolving). Until recently, ice velocity mosaics were limited to once per year during winter citep:joughin_2010_greenland, and they are still temporally limited, often to annual resolution, prior to 2000 (e.g. citet:mouginot_2018_1972to1990,mouginot_2018_1991to2000). Focusing on recent times, ice-sheet wide velocity mosaics from the Sentinel 1A & 1B are now available every six days (http://PROMICE.org). The increased availability of satellite data has improved ice velocity maps both spatially and temporally thereby decreasing the need to rely on spatial and temporal interpolation of velocities from annual/winter mosaics citep:andersen_2015_basin-scale,king_2018_seasonal.

The discharge gates in this study are generated using only surface speed and an ice mask. We use the MEaSUREs Greenland Ice Sheet Velocity Map from InSAR Data, Version 2 citep:joughin_2010_greenland,NSIDC_0478, hereafter termed "MEaSUREs 0478" due to the National Snow and Ice Data Center (NSIDC) date set ID number. We use the BedMachine v3 citep:morlighem_2017_bedmachine,NSIDC_BedMachine ice mask.

For ice thickness estimates, we use surface elevation from GIMP (citet:howat_2014_greenland,NSIDC_0715; NSIDC data set ID 0715), adjusted through time with surface elevation change from citet:khan_2016_geodetic and bed elevations from BedMachine v3 replaced by citet:millan_2018_vulnerability where available. Ice sector and region delineation is from citet:mouginot_2019_glacier. Ice velocity data are obtained from a variety of products including Sentinel 1A & 1B derived by PROMICE (see Appendix), MEaSUREs 0478, MEaSUREs 0646 citep:NSIDC_0646, citet:mouginot_2018_1972to1990, and citet:mouginot_2018_1991to2000. Official glacier names come from citet:bjork_2015_brief. Other glacier names come from citet:mouginot_2019_glacier. See Table [[tab:data]] for an overview of data sets used in this work.

#+BEGIN_SRC sh :results verbatim
echo "times: " $(head -n1 ./tmp/dat_100_5000.csv | tr ',' '\n' | grep "vel_eff" | wc -l)
echo "times: " $(head -n1 ./tmp/dat_100_5000.csv | tr ',' '\n' | grep "vel_eff" | grep -v 19|wc -l)
echo "times: " $(head -n1 ./tmp/dat_100_5000.csv | tr ',' '\n' | grep "vel_eff" | grep 2016|wc -l)
echo "times: " $(head -n1 ./tmp/dat_100_5000.csv | tr ',' '\n' | grep "vel_eff" | grep 2017|wc -l)
echo "times: " $(head -n1 ./tmp/dat_100_5000.csv | tr ',' '\n' | grep "vel_eff" | grep 2018|wc -l)
echo "times: " $(head -n1 ./tmp/dat_100_5000.csv | tr ',' '\n' | grep "vel_eff" | grep 2019|wc -l)
#+END_SRC
#+RESULTS:
: times:  404
: times:  313
: times:  36
: times:  69
: times:  42
: times:  17

This work uses 356 different velocity maps, biased toward the last 500 days of the time series when six-day ice velocities become available from the Sentinel-1 satellites. The temporal distribution is 1 to a few velocity map per year from 1986 to 2000, 9 to 13 velocity maps per year from 2000 through 2015, 36 in 2016, and 69 in 2017, 42 in 2018, and 17 in 2019.

#+LATEX_ATTR: :placement [!h]
#+CAPTION: Summary of data sources used in this work.
#+NAME: tab:data
| Property                       | Name used in this paper  | Reference                                                      |
|--------------------------------+--------------------------+----------------------------------------------------------------|
| Basal Topography               | BedMachine               | citet:morlighem_2017_bedmachine,NSIDC_BedMachine               |
| Basal Topography for Southeast |                          | citet:millan_2018_vulnerability                                |
| Surface Elevation              | GIMP 0715                | citet:howat_2014_greenland,NSIDC_0715                          |
| Surface Elevation Change       | Surface Elevation Change | citet:khan_2016_geodetic                                       |
| Baseline Velocity              | MEaSUREs 0478            | citet:NSIDC_0478                                               |
| Velocity                       | Sentinel                 | Appendix                                                       |
| Velocity                       | MEaSUREs 0646            | citet:NSIDC_0646                                               |
| Velocity                       | MEaSUREs 0731            | citet:NSIDC_0731,joughin_2010_greenland,joughin_2018_greenland |
| Velocity                       | pre-2000                 | citet:mouginot_2018_1972to1990,mouginot_2018_1991to2000        |
| Sectors & Regions              | Sectors & Regions        | citet:mouginot_2019_glacier                                    |
| Names                          |                          | citet:bjork_2015_brief,mouginot_2019_glacier                   |

* Methods
** Terminology 

We use the following terminology, most displayed in Fig. [[fig:overview]]: 
+ "Pixels" are individual 200 m x 200 m raster discharge grid cells. We use the nearest neighbor when combining data sets that have different grid properties.
+ "Gates" are contiguous (including diagonal) clusters of pixels.
+ "Sectors" are spatial areas that have 0, 1, or > 1 gate(s) plus any upstream source of ice that flows through the gate(s), and come from citet:mouginot_2019_glacier.
+ "Regions" are groups of sectors, also from citet:mouginot_2019_glacier, and labeled by approximate geographic region.
+ The "baseline" period is the average 2015, 2016, and 2017 winter velocity from MEaSUREs 0478.
+ "Coverage" is the percentage of total, region, sector, or gate discharge observed at any given time. By definition coverage is 100 % during the baseline period. From the baseline data, the contribution to total discharge of each pixel is calculated, and coverage is reported for all other maps that have missing observations (Fig. [[fig:coverage_schematic]]). Total estimated discharge is always reported because missing pixels are gap-filled (see "Missing and invalid data" section below).
+ "Fast-flowing ice" is defined as ice that flows more than 100 m yr^{-1}.
+ Names are reported using the official Greenlandic names from citet:bjork_2015_brief if a nearby name exists, then citet:mouginot_2019_glacier in parentheses.

Although we refer to solid ice discharge, and it is in the solid phase when it passes the gates and eventually reaches the termini, submarine melting does occur at the termini and some of the discharge enters the fjord as liquid water citep:enderlin_2013_submarine.

** Gate location

Gates are algorithmically generated for fast-flowing ice (greater than 100 m yr^{-1}) close to the ice sheet terminus determined by the baseline-period data. We apply a 2D inclusive mask to the baseline data for all ice flowing faster than 100 m yr^{-1}. We then select the mask edge where it is near the BedMachine ice mask (not including ice shelves), which effectively provides grounding line termini. We buffer the termini 5000 m in all directions creating ovals around the termini and once again down-select to fast-flowing ice pixels. This procedure results in gates 5000 m upstream from the baseline terminus that bisect the baseline fast-flowing ice. We manually mask some land- or lake-terminating glaciers which are initially selected by the algorithm due to fast flow and mask issues. 

We select a 100 m yr^{-1} speed cutoff because slower ice, taking longer to reach the terminus, is more influenced by SMB. For the influence of this threshold on our results see the Discussion section and Fig. [[fig:heatmap]]. 

We select gates at 5000 m upstream from the baseline termini, which means that gates are likely > 5000 m from the termini further back in the historical record citep:murray_2015_extensive,wood_2018_ocean-induced. The choice of a 5000 m buffer follows from the fact that it is near-terminus and thus avoids the need for (minor) SMB corrections downstream, yet is not too close to the terminus where discharge results are sensitive to the choice of distance-to-terminus value (Fig. [[fig:heatmap]]), which may be indicative of bed (ice thickness) errors.

** Thickness

We derive thickness from surface and bed elevation. We use GIMP 0715 surface elevations in all locations, and the BedMachine bed elevations in most locations, except southeast Greenland where we use the citet:millan_2018_vulnerability bed. The GIMP 0715 surface elevations are all time-stamped per pixel. We adjust the surface through time by linearly interpolating elevation changes from citet:khan_2016_geodetic, which covers the period from 1995 to 2016. We use the average of the first and last three years for earlier and later times, respectively. Finally, from the fixed bed and temporally varying surface, we calculate the time-dependent ice thickness at each gate pixel.

** Missing or invalid data

The baseline data provides velocity at all gate locations by definition, but individual non-baseline velocity maps often have missing or invalid data. Also, thickness provided by BedMachine is clearly incorrect in some places (e.g. fast-flowing ice that is 10 m thick, Fig. [[fig:h_v_histogram]]). We define invalid data and fill in missing data as described below.

*** Invalid velocity

We flag invalid (outlier) velocities by treating each pixel as an individual time series, applying a 30 point rolling window, flagging values more than 2 standard deviations outside the mean, and repeating this filter three times. We also drop the 1972 to 1985 years from citet:mouginot_2018_1972to1990 because there is low coverage and extremely high variability when using our algorithm.

This outlier detection method appears to correctly flag outliers (see citet:mankoff_2019_ice for un-filtered time series graphs), but likely also flags some true short-term velocity increases. The effect of this filter is a ~1% reduction in discharge most years, but more in years with high discharge -- a reduction of 3.2 % in 2013, 4.3 % in 2003, and more in the 1980s when the data is noisy. Any analysis using this data and focusing on individual glaciers or short-term changes (or lack there-of) should re-evaluate the upstream data sources.

*** Missing velocity
:PROPERTIES:
:CUSTOM_ID: sec:missing_velocity
:END:

We generate an ice speed time series by assigning the PROMICE, MEaSUREs 0478, MEaSUREs 0646, and pre-2000 products to their respective reported time stamps (even though these are time-span products), or to the middle of their time span when they cover a long period such as the annual maps from citet:mouginot_2018_1972to1990,mouginot_2018_1991to2000. We ignore that any individual velocity map or pixel has a time span, not a time stamp. Velocities are sampled only where there are gate pixels. Missing pixel velocities are linearly interpolated in time, except for missing data at the beginning of the time series which are back- and forward-filled with the temporally-nearest value for that pixel (Fig. [[fig:coverage_schematic]]). We do not spatially interpolate missing velocities because the spatial changes around a missing data point are most likely larger than the temporal changes. We visually represent the discharge contribution of directly observed pixels, termed coverage (Fig. [[fig:coverage_schematic]]) as time series graphs and opacity of dots and error bars in the figures. Therefore, the gap-filled discharge contribution at any given time is equal to 100 minus the coverage. Discharge is always reported as estimated total discharge even when coverage is less than 100 %.

*** Invalid thickness
:PROPERTIES:
:CUSTOM_ID: sec:invalid_thickness
:END:

The thickness data appear to be incorrect in some locations. For example, many locations have fast-flowing ice, but report ice thickness as 10 m or less (Fig. [[fig:h_v_histogram]], left panel). We accept all ice thickness greater than 20 m and construct from this a thickness versus log_{10} speed relationship. For all ice thickness less than or equal to 20 m thick we adjust thickness based this relationship (Fig. [[fig:h_v_histogram]], right panel). We selected the 20 m thickness cutoff after visually inspecting the velocity distribution (Fig. [[fig:h_v_histogram]], left panel). This thickness adjustment adds 20 Gt yr^{-1} to our baseline-period discharge estimate with no adjustment. In the Appendix and Table [[tab:thick_treatments]] we discuss the discharge contribution of these adjusted pixels, and a comparison among this and other thickness adjustments.


** Discharge

We calculate discharge per pixel using density (917 kg m^{-3}), filtered and filled ice speed, projection-corrected pixel width, and adjusted ice thickness derived from time-varying surface elevation and a fixed bed elevation (Eq. [[eq:Q]]). We assume that any change in surface elevation corresponds to a change in ice thickness and thereby neglect basal uplift, erosion, and melt, which combined are orders of magnitude less than surface melting (e.g. citet:cowton_2012_rapid,khan_2007_elastic). We also assume depth-averaged ice velocity is equal to the surface velocity.

We calculate discharge using the gate-orthogonal velocity at each pixel and at each timestamp -- all velocity estimates are gate-orthogonal at all times, regardless of gate position, orientation, or changing glacier velocity direction over time.

Annual averages are calculated by linearly interpolating to daily, then estimating annual. The difference between this method and averaging only the observed samples is ~3 % median (5 % average, and a maximum of 10 % when examining the entire ice sheet and all years in our data). It is occasionally larger at individual glaciers when a year has few widely-space samples of highly variable velocity.

*** Discharge Uncertainty
\label{sec:D_uncertainty}

A longer discussion related to our and others treatments of errors and uncertainty is in the Appendix, but here we describe how we estimate the uncertainty related to the ice discharge following a simplistic approach. This yields an uncertainty of the total ice discharge of approximately 10 % throughout the time series. 

At each pixel we estimate the maximum discharge, \(D_{\mathrm{max}}\), from 

#+NAME: eq:D_err_max
\begin{equation}
D_{\mathrm{max}} = \rho \, (V + \sigma_V) \, (H + \sigma_H) \, W,
\end{equation}

and minimum discharge, \(D_{\mathrm{min}}\), from

#+NAME: eq:D_err_min
\begin{equation}
D_{\mathrm{min}} = \rho \, (V - \sigma_V) \, (H - \sigma_H) \, W,
\end{equation}

where \(\rho\) is ice density, \(V\) is baseline velocity, \(\sigma_V\) is baseline velocity error, \(H\) is ice thickness, \(\sigma_H\) is ice thickness error, and \(W\) is the width at each pixel. Included in the thickness term is surface elevation change through time (\(\mathrm{d}H/\mathrm{d}t\)). When data sets do not come with error estimates we treat the error as 0.

We use \(\rho = 917\) kg m^{-3} because the gates are near the terminus in the ablation zone and ice thickness estimates should not include snow or firn, although regionally ice density may be < 917 kg m^{-3} due to crevasses. We ignore the velocity error \(\sigma_V\) because the proportional thickness error (\(\sigma_H/H\)) is an order of magnitude larger than the proportional velocity error (\(\sigma_V/V\)) yet both contribute linearly to the discharge. \(W\) is location-dependent due to the errors between our working map projection (EPSG 3413) and a more accurate spheroid model of the earth surface. We adjust linear gate width by up to ~4% in the north and ~-2.5% in the south of Greenland (area errors are up to 8%). On a pixel by pixel basis we used the provided thickness uncertainty except where we modified the thickness (H < 20 m) we prescribe an uncertainty of 0.5 times the adjusted thickness. Subsequently, the uncertainty on individual glacier-, sector-, region-, or ice sheet scale is obtained by summing, but not reducing by the square of the sums, the uncertainty related to each pixel. We are conservative with our thickness error estimates, by assuming the uncertainty range is from \(D_{\mathrm{min}}\) to \(D_{\mathrm{max}}\) and not reducing by the sum-of-squares of sectors or regions. 

* Results
** Gates

#+BEGIN_SRC sh :results verbatim :exports none
echo "pixels: " $(wc -l tmp/dat_100_5000.csv)

# head -n1 tmp/dat_100_5000.csv | tr ',' '\n' | cat -n | grep clump
echo "gates: " $(cut -d"," -f8 tmp/dat_100_5000.csv  | sort | uniq | wc -l)

# head -n1 dat/dat_100_5000.csv | tr ',' '\n' | cat -n | grep sectors
echo "sectors: " $(cut -d"," -f9 tmp/dat_100_5000.csv | sort -n | uniq | wc -l)

# times
echo "times: " $(head -n1 ./tmp/dat_100_5000.csv | tr ',' '\n' | grep "vel_eff" | wc -l)
#+END_SRC
#+RESULTS:
: pixels:  6003 tmp/dat_100_5000.csv
: gates:  277
: sectors:  177
: times:  319

Our gate placement algorithm generates 6002 pixels making up 276 gates, assigned to 176 ice-sheet sectors from citet:mouginot_2019_glacier. Previous similar studies have used 230 gates citep:king_2018_seasonal and 178 gates citep:enderlin_2014_improved.

The widest gate (~47 km) is Sermersuaq (Humboldt Gletsjer), the 2nd widest (~34 km) is Sermeq Kujalleq (Jakobshavn Isbræ). 23 additional glaciers have gate lengths longer than 10 km. The minimum gate width is 3 pixels (600 m) by definition in the algorithm. 

# See LINK: [[id:62f5d28c-c704-422d-9e9b-c0771d5b86ee][Table of thickness adjustments]]

The average unadjusted thickness gates is 405 m with a standard deviation of 260. The average thickness after adjustment is 439 m with a standard deviation of 225. A histogram of unadjusted and adjusted thickness at all gate locations is shown in Fig. [[fig:h_v_histogram]].

** Discharge
:PROPERTIES:
:CUSTOM_ID: sec:results_flow_rate
:END:

Our ice discharge dataset (Fig. [[fig:discharge_ts]]) reports a total discharge of 438 \(\pm\) 43 Gt in 1986, has a minimum of 421 \(\pm\) 42 Gt in 1995, increases to 452 \(\pm\) 45 in 2000, further to 504 \(\pm\) 49 Gt/yr in 2005, after which annual discharge remains approximately steady at 484 to 503 \(\pm\) ~50 Gt/yr during the 2005 to 2019 period. Annual maxima in ice discharged occurred in 2005 (504 \(\pm\) 49 Gt/yr), 2011 (499 \(\pm\) 50 Gt/yr), and 2014 (503 \(\pm\) 51 Gt/yr).

At the region scale, the SE glaciers (see Fig. [[fig:overview]] for regions) are responsible for 139 to 167 (\(\pm\) 11 %) Gt yr^{-1} of discharge (30 to 34 % of ice-sheet wide discharge) over the 1986 to 2019 period. By comparison, the predominantly land-terminating NO, NE and SW together were also responsible for 131 to 168 of discharge (~31 % of ice-sheet wide discharge) during this time (Fig. [[fig:discharge_ts_regions]]). The discharge from most regions has been approximately steady or declining for the past decade. The NW is the only region exhibiting a persistent increase in discharge -- From ~89 to 113 Gt yr^{-1} (21 % increase) over the 1998 through 2019 period (+ ~1 Gt yr^{-1} or + ~1 % yr^{-1}). This persistent increase in NW discharge offsets declining discharge from other regions. The largest contributing region, SE, contributed a high of 166 \(\pm\) 19 Gt in 2005, but dropped to 146, 154 and 154 \(\pm\) 18 Gt in 2016, 2017, and 2018 respectively.

# 100 - 89/113*100 = 21.2389380531
# (113-89+1)/(2017-1998+1) = 1.25
# (21/20) = 1.05

# In the NO, NE and SW regions, which contribute a minority of ice-sheet discharge, low coverage (large data gaps) is evident in the coverage chart (Fig. [[fig:discharge_ts_regions]], only NO of these three regions is shown for clarity), and as linear trends with data point centers and error bars transparent. These indicators of gap-filling are also evident in the NW region but only in 2014 & 2015. They are more clearly evident in the NE, NO, and SW (see data).

Focusing on the top eight contributors (mean of last year) at the individual sector or glacier scale (Fig. [[fig:discharge_ts_topfew]]), Sermeq Kujalleq (Jakobshavn Isbræ) has slowed down from an annual average high of ~52 Gt yr^{-1} in 2012 to ~45 Gt yr^{-1} in 2016 and ~38 Gt yr^{-1} in 2017, likely due to ocean cooling citep:khazendar_2019_interruption. We exclude Ikertivaq from the top 8 because that gate spans multiple sectors and outlets, while the other top dischargers are each a single outlet. The 2013 to 2016 slowdown of Sermeq Kujalleq (Fig. [[fig:discharge_ts_topfew]]) is compensated by the many glaciers that make up the NW region (Fig. [[fig:discharge_ts_regions]]). The large 2017 and 2018 reduction in discharge at Sermeq Kujalleq is partially offset by a large increase in the 2nd largest contributor, Helheim Gletsjer (Fig. [[fig:discharge_ts_topfew]]).

* Discussion

Different ice discharge estimates among studies likely stem from three categories: 1) changes in true discharge, 2) different input data (ice thickness and velocity), and 3) different assumptions and methods used to analyze data. Improved estimates of true discharge is the goal of this and many other studies, but changes in true discharge (category 1) can happen only when a work extends a time series into the future because historical discharge is fixed. Thus, any inter-study discrepancies in historical discharge must be due to category 2 (different data) or category 3 (different methods). Most studies use both updated data and new or different methods, but do not always provide sufficient information to disentangle the two. This is inefficient. To more quantitatively discuss inter-study discrepancies, it is imperative to explicitly consider all three potential causes of discrepancy. Only when results are fully reproducible -- meaning all necessary data and code are available (c.f. citet:mankoff_2017_past,rezvanbehbahani_2017_predicting) -- can new works confidently attribute discrepancies relative to old works. Therefore, in addition to providing new discharge estimates, we attempt to examine discrepancies among our estimates and other recent estimates. Without access to code and data from previous studies, it is challenging to take this examination beyond a qualitative discussion.

The algorithm-generated gates we present offer some advantages over traditional hand-picked gates. Our gates are shared publicly, are generated by code that can be audited by others, and are easily adjustable within the algorithmic parameter space. This allows both sensitivity testing of gate location (Fig. [[fig:heatmap]]) and allows gate positions to systematically evolve with glacier termini (not done here).

** Comparison with past studies                           :ignore:

The total ice discharge we estimate is ~10 % less than the total discharge of two previous estimates citep:enderlin_2014_improved,rignot_2008_mass, and similar to that of citet:king_2018_seasonal, who attributes their discrepancy with citet:enderlin_2014_improved to the latter using only summer velocities, which have higher annual average values than seasonally-comprehensive velocity products. The gate locations also differ among studies, and glaciers with baseline velocity less than 100 m yr^{-1} are not included in our study due to our velocity cutoff threshold, but this should not lead to substantially different discharge estimates (Fig. [[fig:heatmap]]).

Our gate selection algorithm also does not place gates in northeast Greenland at Storstrømmen, Bredebræ, or their confluence, because during the baseline period that surge glacier was in a slow phase. We do not manually add gates at these glaciers. The last surge ended in 1984 citep:reeh_1994_surge,mouginot_2018_insights, prior to the beginning of our time series, and these glaciers are therefore not likely to contribute substantial discharge even in the early period of discharge estimates.

We instead attribute the majority of our discrepancy with citet:enderlin_2014_improved to the use of differing bed topography in southeast Greenland. When we compare our top ten highest discharging glaciers in 2000 with those reported by citet:enderlin_2014_improved, we find that the Køge Bugt discharge reported by citet:enderlin_2014_improved is ~31 Gt, but our estimate is only ~16 Gt (and ~17 Gt in citet:king_2018_seasonal). The citet:bamber_2013_bed bed elevation dataset that likely uses the same bed data employed by citet:enderlin_2014_improved has a major depression in the central Køge Bugt bed. This region of enhanced ice thicknesses is not present in the BedMachine dataset that we and citet:king_2018_seasonal employ (Fig. [[fig:koge_bugt_bamber]]). If the Køge Bugt gates of citet:enderlin_2014_improved are in this location, then those gates overlie citet:bamber_2013_bed ice thicknesses that are about twice those reported in BedMachine v3. With all other values held constant, this results in roughly twice the discharge. Although we do not know whether BedMachine or citet:bamber_2013_bed is more correct, conservation of mass suggests that a substantial subglacial depression should be evident as either depressed surface elevation or velocity citep:morlighem_2016_improving.

We are unable to attribute the remaining discrepancy between our discharge estimates and those by citet:enderlin_2014_improved. It is likely a combination of differing seasonal velocity sampling citep:king_2018_seasonal, our evolving surface elevation from citet:khan_2016_geodetic, or other previously-unpublished algorithmic or data differences, of which many possibilities exist.

Our ice discharge estimates agree well with the most recently published discharge estimate citep:king_2018_seasonal, except that our discharge is slightly less. We note that our uncertainty estimates include the citet:king_2018_seasonal estimates, but the opposite does not appear be true. The minor differences are likely due to different methods. citet:king_2018_seasonal use seasonally varying ice thicknesses, derived from seasonally varying surface elevations, and a Monte Carlo method to temporally interpolate missing velocity data to produce discharge estimates. In comparison, we use linear interpolation of both yearly surface elevation estimates and temporal data gaps. It is not clear whether linear or higher-order statistical approaches are best-suited for interpolation as annual cycles begin to shift, as is the case with Sermeq Kujalleq (Jakobshavn Isbræ) after 2015. There are benefits and deficiencies with both methods. Linear interpolation may alias large changes if there are no other observations nearby in time. Statistical models of past glacier behavior may not be appropriate when glacier behavior changes.

It is unlikely that discharge estimates using gates that are only approximately flow-orthogonal and time-invariant citep:king_2018_seasonal have large errors due to this, because it is unlikely that glacier flow direction changes significantly, but our gate-orthogonal treatment may be the cause of some differences among our approach and other works. Discharge calculated using non-orthogonal methodology would overestimate true discharge.

* Data availability

This work in its entirety is available at doi:10.22008/promice/data/ice_discharge citep:GEUS_discharge_paper. The glacier-scale, sector, region, and Greenland summed ice sheet discharge dataset is available at doi:10.22008/promice/data/ice_discharge/d/v0.0.1 citep:GEUS_discharge_paper_d, where it will be updated as more velocity data become available. The gates can be found at doi:10.22008/promice/data/ice_discharge/gates/v0.0.1 citep:GEUS_discharge_paper_gates, the code at doi:10.22008/promice/data/ice_discharge/code/v0.0.1 citep:GEUS_discharge_paper_code, and the surface elevation change at doi:10.22008/promice/data/DTU/surface_elevation_change/v1.0.0 citep:GEUS_discharge_paper_elevation_change.

* Conclusions

We have presented a novel dataset of flux gates and 1986 through July 2019 glacier-scale ice discharge estimate for the Greenland ice sheet. These data are underpinned by an algorithm that both selects gates for ice flux and then computes ice discharges. 

Our results are similar to the most recent discharge estimate citep:king_2018_seasonal but begin in 1986 - although there is low coverage and few samples prior to 2000. From our discharge estimate we show that over the past ~30 years, ice sheet discharge was ~430 Gt yr^{-1} prior to 2000, rose to over 500 Gt yr^{-1} from 2000 to 2005, and has held roughly steady since 2005 at near 500 Gt yr^{-1}. However, when viewed at a region or sector scale, the system appears more dynamic with spatial and temporal increases and decreases canceling each other out to produce the more stable ice sheet discharge. We note that there does not appear to be any dynamic connection among the regions, and any increase in one region that was offset by a decrease in another has likely been due to chance. If in coming years when changes occur the signals have matching signs, then ice sheet discharge would decrease or increase, rather than remain fairly steady.

The application of our flux-gate algorithm shows that ice-sheet wide discharge varies by ~30 Gt yr^{-1} due only to gate position, or ~40 Gt due to gate position and cutoff velocity (Fig. [[fig:heatmap]]). This variance is approximately equal to the uncertainty associated with ice-sheet wide discharge estimates reported in many studies (e.g. citet:rignot_2008_mass,andersen_2015_basin-scale,kjeldsen_2015_spatial). We highlight a major discrepancy with the ice discharge data of citet:enderlin_2014_improved and we suspect this discharge discrepancy -- most pronounced in southeast Greenland -- is associated with the choice of digital bed elevation model, specifically a deep hole in the bed at Køge Bugt.

Transparency in data and methodology are critical to move beyond a focus of estimating discharge quantities, towards more operational mass loss products with realistic errors and uncertainty estimates. The convention of devoting a paragraph, or even page, to methods is insufficient given the complexity, pace, and importance of Greenland ice sheet research. Therefore the flux gates, discharge data, and the algorithm used to generate the gates, discharge, and all figures from this manuscript are freely available. We hope that the flux gates, data, and code we provide here is a step toward helping others both improve their work and discover the errors in ours.

* Other                                                   :ignore:

#+BEGIN_authorcontribution
\ KDM conceived of the algorithm approach, and wrote the code. KDM , WIC, and RSF iterated over the algorithm results and methods. ASO provided the velocity data. SAK supplied the surface elevation change data. All authors contributed to the scientific discussion, writing, and editing of the manuscript. 
#+END_authorcontribution


#+BEGIN_competinginterests
\ The authors declare that they have no conflict of interest.
#+END_competinginterests


#+BEGIN_acknowledgements
Funding was provided by the Programme for Monitoring of the Greenland Ice Sheet (PROMICE). Ice velocity maps were produced from ESA Sentinel-1 remote sensing data as part of PROMICE and were provided by the Geological Survey of Denmark and Greenland (GEUS) at http://www.promice.org. Parts of this work were funded by the INTAROS project under the European Union's Horizon 2020 research and innovation program under grant agreement No. 727890. We thank the reviewers for their constructive input that helped improve the paper. 
#+END_acknowledgements


* References                                              :ignore:

#+LaTeX: \bibliographystyle{copernicus}
# #+LaTeX: \bibliography{/home/kdm/Documents/Papers/library,local}{}
# #+LaTeX: \bibliography{local}{}
#+LaTeX: \bibliography{ice_discharge}{}

* Figures
:PROPERTIES:
:clearpage: t
:END:
** Overview                                               :ignore:

#+NAME: fig:overview
#+ATTR_LATEX: :width 0.4\textwidth :placement [!h]
#+CAPTION: Overview showing fast-flowing ice (orange, greater than 100 m yr^{-1}) and the gates for the top eight discharging glaciers (Fig. [[fig:discharge_ts_topfew]]). Gates are shown as black lines in inset images. Each inset is 30 x 30 km and all have the same color scaling, but different than the main map. Insets pair with nearest label and box. On the main map, regions from citet:mouginot_2019_glacier are designated by thicker black lines and large bold labels. Sectors (same source) are delineated with thinner gray lines, and the top discharging glaciers are labeled with smaller font. H = Helheim Gletsjer, KB = (Køge Bugt), KG = Kangerlussuaq Gletsjer, KS = Kangilliup Sermia (Rink Isbræ), N = (Nioghalvfjerdsbræ), P = Petermann Gletsjer, SK = Sermeq Kujalleq (Jakobshavn Isbræ), and Z = Zachariae Isstrøm. Basemap terrain (gray), ocean bathymetry (blues), and ice mask (white) come from BedMachine.
[[./figs/overview.png]]


** Heatmap                                                :ignore:

#+NAME: fig:heatmap
#+ATTR_LATEX: :width \textwidth :placement [!h]
#+CAPTION: Heatmap and table showing ice sheet discharge as a function of gate buffer distance and ice speed cutoff. The colors of the numbers change for readability.
[[./figs/heatmap_all.png]]


** Ice Thickness v. Velocity 2D Histogram: Color = count  :ignore:

#+BEGIN_SRC jupyter-python :session histogram2D :exports none
import matplotlib
import matplotlib as mpl

if "LOADED" not in locals():
    <<load_data>>
    <<millan_2018>>
    <<adjust_thickness>>
    <<adjust_thickness_fit>>
    LOADED=True

plt.close(1)
fig = plt.figure(1, figsize=(8,4)) # w,h
# get_current_fig_manager().window.move(0,0)
fig.clf()
# fig.set_tight_layout(True)

ax1 = fig.add_subplot(121)
thick = th['thick']
thick[thick < 1] = 1
im = ax1.hexbin(vel_baseline['vel'].values, thick,
               gridsize=(40,25),
               norm=mpl.colors.LogNorm(),
               vmin=1, vmax=100,
               xscale='log', yscale='log',
               linewidths=0.01,
               edgecolors='k',  mincnt=1)
ax1.set_ylabel('Thickness [m]')
ax1.set_xlabel('Velocity [m yr$^{-1}$]')
# cb = fig.colorbar(im, ax=ax1, extend='max')
# cb.set_label('Count [#]')

ax2 = fig.add_subplot(122)
im = ax2.hexbin(vel_baseline['vel'].values, th['fit'],
               gridsize=(40,25),
               norm=mpl.colors.LogNorm(),
               vmin=1, vmax=100,
               extent=[2,4,0,3],
               xscale='log', yscale='log',
                linewidths=0.01,
               edgecolors='k',  mincnt=1)
#ax2.set_ylabel('Thickness [m]')
#ax2.set_xlabel('Velocity [m yr$^{-1}$]')
cb = fig.colorbar(im, ax=ax2, extend='max')
cb.set_label('Count [#]')

from adjust_spines import adjust_spines as adj
adj(ax1, ['left','bottom'])
adj(ax2, ['bottom'])

plt.savefig('./figs/h_v_histogram.png', transparent=True, dpi=300)
#+END_SRC

#+NAME: fig:h_v_histogram
#+ATTR_LATEX: :height 0.5\textwidth
#+CAPTION: 2D histogram of velocity and thickness at all gate pixels. Left panel: Unadjusted (BedMachine & citet:millan_2018_vulnerability) thickness. Right panel: Adjusted (as described in the text) thickness.
[[./figs/h_v_histogram.png]]




** Discharge Time Series                                  :ignore:

#+NAME: fig:discharge_ts
#+ATTR_LATEX: :width \textwidth :placement [!h]
#+CAPTION: Bottom panel: Time series of ice discharge from the Greenland ice sheet. Dots represent when observations occurred. Orange stepped line is annual average. Coverage (percentage of total discharge observed at any given time) is shown in top panel, and also by opacity of dot interior and error bars on lower panel. When coverage is < 100 %, total discharge is estimated and shown.
[[./figs/discharge_ts.png]]

** Discharge Time Series: Regions                         :ignore:

#+NAME: fig:discharge_ts_regions
#+ATTR_LATEX: :width \textwidth :placement [!h]
#+CAPTION: Bottom panel: Time series of ice discharge by region. Same graphical properties as Fig. [[fig:discharge_ts]]. Top panel: The region with highest coverage (CE), lowest coverage (NE), and coverage for the region with highest discharge (SE) are shown. Coverage for other regions not shown to reduce clutter.
[[./figs/discharge_ts_regions.png]]

** Discharge Time Series: Top Few                         :ignore:

#+NAME: fig:discharge_ts_topfew
#+ATTR_LATEX: :width \textwidth :placement [!h]
#+CAPTION: Bottom panel: Time series of ice discharge showing top eight (mean of last year) discharging glaciers. Same graphical properties as Fig. [[fig:discharge_ts]]. Only an example high (Kangerlussuaq Gletsjer) and low (Nioghalvfjerdsbræ) coverage shown to reduce clutter.
[[./figs/discharge_ts_topfew.png]]

* Appendix                                                :ignore:
#+LaTeX: \appendix
#+LaTeX: \clearpage

** Errors and Uncertainties

Here we describe our error and uncertainty treatments. We begin with a brief philosophical discussion of common uncertainty treatments, our general approach, and then the influence of various decisions made throughout our analysis, such as gate location and treatments of unknown thicknesses.

# Throughout this work we have tried to use simple rather than complicated methods (e.g. linear interpolation rather than Monte Carlo simulations, or setting bad thickness directly to X m rather than via a convoluted self-correlation) and conservative (low) rather than aggressive (high) estimates (e.g. unknown thicknesses set to 300 \(\pm\) 300 m rather than ~400 \(\pm\) 70 m citep:enderlin_2014_improved).

Traditional and mathematically valid uncertainty treatments divide errors into two classes: systematic (bias) and random. The primary distinction is that systematic errors do not decrease with more samples, and random errors decrease as the number of samples or measurements increases. The question is then which errors are systematic and which are random. A common treatment is to decide that errors within a region are systematic, and among regions are random. This approach has no physical basis - two glaciers a few 100 m apart but in different regions are assumed to have random errors, but two glaciers 1000s of km apart but within the same region are assumed to have systematic errors. It is more likely the case that all glaciers less wide than some width or more deep than some depth have systematic errors even if they are on opposite sides of the ice sheet, if ice thickness is estimated with the same method (i.e. the systematic error is likely caused by the sensor and airplane, not the location of the glacier).

The decision to have \(R\) random samples (where \(R\) is the number of regions, usually ~18 based on citet:zwally_2012_sectors) is also arbitrary. Mathematical treatment of random errors means that even if the error is 50 %, 18 measurements reduces it to only 11.79 %.

#+BEGIN_SRC jupyter-python :results output :exports none
import numpy as np
from uncertainties import unumpy
val = 1 # arbitrary
err = 1 # 100% error # also try with 0.5

x = []
xerr = []
for i in [1,2,10,18,100,176,276,6002]:
    u = np.sum(unumpy.uarray([val]*i, [err]*i))
    x.append(u.n)
    xerr.append(u.s)
    print(i, u.n, u.s, np.round(u.s/u.n*100,2))
#+END_SRC
#+RESULTS:
: 1 1.0 1.0 100.0
: 2 2.0 1.4142135623730951 70.71
: 10 10.0 3.1622776601683795 31.62
: 18 18.0 4.242640687119285 23.57
: 100 100.0 10.0 10.0
: 176 176.0 13.2664991614216 7.54
: 276 276.0 16.61324772583615 6.02
: 6002 6002.0 77.47257579298626 1.29

This reduction is unlikely to be physically meaningful. Our 176 sectors, 276 gates and 6002 pixels means that even if errors were 100 % for each, we could reduce it to 7.5, 6.0, or 1.3 % respectively. We note that the area error introduced by the common EPSG:3413 map projection is -5 % in the north and +8 % in the south. While this error is mentioned in some other works (e.g. citet:joughin_2018_greenland) it is often not explicitly mentioned.

We do not have a solution for the issues brought up here, except to discuss them explicitly and openly so that those, and our own, error treatments are clearly presented and understood to likely contain errors themselves. 

*** Invalid Thickness

# src_jupyter-python[:session]{vel.shape[0]} {{{results(6002)}}}
# src_jupyter-python[:session]{(th['thick']>20).sum()} {{{results(5366)}}}
# src_jupyter-python[:session]{(th['thick']<=20).sum()} {{{results(636)}}}
# src_jupyter-python[:session]{np.round((th['thick']<=20).sum()/(th['thick']>20).sum()*100).astype(np.int)} {{{results(12)}}}

We assume ice thicknesses < 20 m are incorrect where ice speed is > 100 m yr^{-1}. Of 6002 pixels, 5366 have valid thickness, and 636 (12 %) have invalid thickness. However, the speed at the locations of the invalid thicknesses is generally much less (and therefore the assumed thickness is less), and the influence on discharge is less than an average pixel with valid thickness (Table [[tab:thick_adjust]]).

#+NAME: post_clean
#+BEGIN_SRC sh :results verbatim :var r="" :results output
echo "${r}" | sed s/\^:\ \//g | grep -v "^$" | grep -v "^#\ Out.*" | grep -v "#\ output" | grep -v "^#\ text\/.*"
#+END_SRC

#+NAME: tab:thick_adjust
#+BEGIN_SRC jupyter-python :display text/org :exports none :results raw drawer :session thick_adj :noweb yes

<<load_data>>
<<millan_2018>>
<<adjust_thickness>>

df = pd.DataFrame(index=pd.DataFrame(np.random.random(3)).describe().index.values)
df['Good Pixels'] = vel_baseline.loc[~th['bad']].describe()
df['Bad Pixels'] = vel_baseline.loc[th['bad']].describe()

print("#+LATEX_ATTR: :placement [!h]")
print("#+CAPTION: Statistics of pixels with and without valid thickness. Numbers represent speed [m yr^{-1}] except for the \"count\" row.")
print("#+NAME: tab:thick_adjust")
df.round(0).astype(np.int)
#+END_SRC

#+LATEX_ATTR: :placement [!h]
#+CAPTION: Statistics of pixels with and without valid thickness. Numbers represent speed [m yr^{-1}] except for the "count" row.
#+NAME: tab:thick_adjust
|       |   Good Pixels |   Bad Pixels |
|-------+---------------+--------------|
| count |          5366 |          636 |
| mean  |           821 |          266 |
| std   |          1040 |          235 |
| min   |           100 |          101 |
| 25%   |           230 |          129 |
| 50%   |           487 |          171 |
| 75%   |           972 |          281 |
| max   |         10044 |         1423 |
:end:

# NOTE: Manual cleaning performed for above table

# src_jupyter-python[:session thick_adj]{th['gates'].unique().size} {{{results(=276=)}}}
# src_jupyter-python[:session thick_adj]{(th.groupby('gates').mean()['bad'] == 0).sum()} {{{results(=187=)}}}
# src_jupyter-python[:session thick_adj]{(th.groupby('gates').mean()['bad'] == 0).sum()/th['gates'].unique().size} {{{results(=0.677536231884058=)}}}
# src_jupyter-python[:session thick_adj]{(th.groupby('gates').mean()['bad'] > 0).sum()} {{{results(=89=)}}}
# src_jupyter-python[:session thick_adj]{(th.groupby('gates').mean()['bad'] > 0).sum()/th['gates'].unique().size} {{{results(=0.322463768115942=)}}}
# src_jupyter-python[:session thick_adj]{(th.groupby('gates').mean()['bad'] > 0.5).sum()} {{{results(=65=)}}}
# src_jupyter-python[:session thick_adj]{(th.groupby('gates').mean()['bad'] == 1).sum()} {{{results(=61=)}}}
# src_jupyter-python[:session thick_adj]{(th.groupby('gates').mean()['bad'] == 1).sum()/th['gates'].unique().size} {{{results(=0.2210144927536232=)}}}

When aggregating by gate, there are 276 gates. Of these, 187 (68 %) have no bad pixels and 89 (32 %) have some bad pixels, 65 have > 50 % bad pixels, and 61 (22 %) are all bad pixels.

We adjust these thickness using a poor fit (correlation coefficient: 0.3) of the log$_{10}$ of the ice speed to thickness where the relationship is known (thickness > 20 m). We set errors equal to one half the thickness (i.e. \(\sigma_H = \pm 0.5 \, H\)). We also test the sensitivity of this treatment to simpler treatments, and have the following five categories:

+ NoAdj :: No adjustments made. Assume BedMachine thickness are all correct.
+ NoAdj+Millan :: Same as NoAdj, but using citet:millan_2018_vulnerability bed where available.
+ 300 :: If a gate has some valid pixel thicknesses, set the invalid thicknesses to the minimum of the valid thicknesses. If a gate has no valid thickness, set the thickness to 300 m.
+ 400 :: Set all thickness < 50 m to 400 m
+ Fit :: Use the thickness v. speed relationship described above.

Table [[tab:thick_treatments]] shows the estimated baseline discharge to these four treatments:

#+NAME: tab:thick_treatments
#+BEGIN_SRC jupyter-python :exports results :results raw drawer :display text/org :session thick_adj2

<<load_data>>
<<millan_2018>>
<<adjust_thickness>>
<<adjust_thickness_fit>>
<<discharge>>

C = 'Discharge [Gt]'
df = pd.DataFrame(index=['NoMillan', 'NoAdj','300','400','fit'], columns=[C], dtype=np.str)
df.index.name = "Treatment"
D_th2 = D_th.copy(deep=True)
D_th2['NoMillan_err'] = D_th['NoAdj_err']
for r in df.index:
    val = D_th2[r].sum(axis=0).round(0).astype(np.int)
    err = D_th2[r+'_err'].sum(axis=0).round(0).astype(np.int)
    df.loc[r,C] = val.astype(np.str) + ' \(\pm\)' + err.astype(np.str)

df.rename({'fit':'Fit'}, inplace=True) 
    
print("#+LATEX_ATTR: :placement [!h]")
print("#+CAPTION: Effect of different thickness adjustments on baseline discharge")
print("#+NAME: tab:thick_treatments")
df
#+END_SRC

#+RESULTS: tab:thick_treatments
:RESULTS:
#+LATEX_ATTR: :placement [!h]
#+CAPTION: Effect of different thickness adjustments on baseline discharge
#+NAME: tab:thick_treatments
| Treatment   | Discharge [Gt]   |
|-------------+------------------|
| NoMillan    | 472 \(\pm\)49    |
| NoAdj       | 480 \(\pm\)49    |
| 300         | 488 \(\pm\)49    |
| 400         | 495 \(\pm\)51    |
| Fit         | 492 \(\pm\)51    |
:END:

# NOTE: Manual cleaning performed for above table

Finally, Figure [[fig:gate_map]] shows the geospatial locations, concentration, and speed of gates with and without bad pixels.

#+NAME: fig:gate_map
#+ATTR_LATEX: :width \textwidth :placement [!h]
#+CAPTION: Gate locations and thickness quality. Left: locations of all gates. Black dots represent gates with 100 % valid thickness pixels, blue with partial, and red with none. Top right: Percent of bad pixels in each of the 276 gates, arranged by region. Bottom panel: Average speed of gates. Color same as left panel.
[[./figs/gate_map.png]]


*** Missing Velocity
\label{sec:uncertain_vel}

We estimate discharge at all pixel locations for any time when there exists any velocity product. Not every velocity product provides velocity estimates at all locations, and we fill in where there are gaps by linear interpolating velocity at each pixel in time. We calculate coverage, the discharge-weighted percent of observed velocity at any given time (Figure [[fig:coverage_schematic]]), and display coverage as 1) line plots over the time series graphs, 2) opacity of the error bars and 3) opacity of the infilling of time series dots. Linear interpolation and discharge-weighted coverage is illustrated in Figure [[fig:coverage_schematic]], where pixel A has a velocity value at all three times, but pixel B has a filled gap at time \(t_3\). The concentration of valid pixels is 0.5, but the weighted concentration, or coverage, is 9/11 or ~0.82. When displaying these three discharge values, \(t_1\) and \(t_4\) would have opacity of 1 (black), and \(t_3\) would have opacity of 0.82 (dark gray).

# Because velocity uncertainty is << thickness uncertainty (see next section) we do estimate a velocity uncertainty for our gap-filled velocities.

This treatment is applied at the pixel level and then weight-averaged to the gate, sector, region, and ice sheet results.


#+BEGIN_SRC sh :results verbatim
inkscape -z ./figs/gate_weight_schematic.svg -e ./figs/gate_weight_schematic.png
#+END_SRC
#+RESULTS:
: Background RRGGBBAA: ffffff00
: Area 0:0:325.387:185.463 exported to 325 x 185 pixels (96 dpi)
: Bitmap saved as: ./figs/gate_weight_schematic.png


#+NAME: fig:coverage_schematic
#+ATTR_LATEX: :width 0.33\textwidth :placement [!h]
#+CAPTION: Schematic demonstrating coverage. Velocities are filled with linear interpolation in time, and coverage is weighted by discharge. \(t\) columns represent the same two gate pixels (A & B) at three time steps, where \(t_n\) are linearly spaced, but \(t_2\) is not observed anywhere on the ice sheet and therefore not included. Numbers in boxes represents example discharge values. Gray parenthetical number is filled, not sampled, in pixel B at time t\(_3\). Weighted filling computes the coverage as 9/11 = \(0.\overline{81}\), instead of 0.5 (half of the pixels at time t\(_3\) have observations).
[[./figs/gate_weight_schematic.png]]


*** Errors from map projection                          :noexport:
#+LaTeX: \label{sec:uncertain_map}

Our work takes place in a projected coordinate system (EPSG 3413) and therefore errors are introduced between the "true" earth spheroid (which is itself an approximation) and our projected coordinates system. We address these by calculating the projection error due to EPSG 3413 which is approximately +8 % in Northern Greenland and -6 % in Southern Greenland, and multiplying variables by a scaling factor if the variables do not already take this into account. Velocities are "true velocities" and not scaled, but the nominal 200 m gate width is scaled.

** Velocity versus thickness                            :noexport:
:PROPERTIES:
:clearpage: t
:END:

#+NAME: fig:h_v_histogram
#+ATTR_LATEX: :width \textwidth :placement [!h]
#+CAPTION: Thickness versus ice speed histogram. Points limited to discharge pixels.
[[./h_v_histogram.png]]


** Køge Bugt Bed Change between citet:bamber_2013_bed and citet:morlighem_2017_bedmachine
:PROPERTIES:
:clearpage: t
:END:

#+NAME: fig:koge_bugt_bamber
#+ATTR_LATEX: :width \textwidth :placement [!h]
#+CAPTION: Differences between BedMachine citep:morlighem_2017_bedmachine and citet:bamber_2013_bed near Køge Bugt. Panel (a) is baseline ice speed, (b) BedMachine thickness, (c) citet:bamber_2013_bed thickness, and (d) difference computed as BedMachine - Bamber. Curved line is gate used in this work.
[[./figs/koge_bugt.png]]



** Sentinel-1 ice velocity maps
:PROPERTIES:
:CUSTOM_ID: sec:appendix:sentinel
:clearpage: t
:END:

We use ESA Sentinel-1 synthetic aperture radar (SAR) data to derive ice velocity maps covering the Greenland Ice Sheet margin using offset tracking citep:strozzi_2002_glacier assuming surface parallel flow using the digital elevation model from the Greenland Ice Mapping Project (GIMP DEM, NSIDC 0645) by citet:howat_2014_greenland,NSIDC_0645. The operational interferometric post processing (IPP) chain citep:dall_2015_ice,kusk_2018_system, developed at the Technical University of Denmark (DTU) Space and upgraded with offset tracking for ESA’s Climate Change Initiative (CCI) Greenland project, was employed to derive the surface movement. The Sentinel-1 satellites have a repeat cycle of 12 days, and due to their constellation, each track has a six-day repeat cycle. We produce a Greenland wide product that spans two repeat cycles of Sentinel-1 A. The product is a mosaic of all the ice velocity maps based on 12 day pairs produced from all the tracks from Sentinel-1 A and B covering Greenland during those two cycles. The product thus has a total time span of 24 days. Six day pairs are also included in each mosaic from track 90, 112 and 142 covering the ice sheet margin in the south as well as other tracks on an irregular basis in order to increase the spatial resolution. citep:rathmann_2017_highly and citet:vijay_2019_resolving have exploited the high temporal resolution of the product to investigate dynamics of glaciers. The maps are available from 2016-09-13 and onward, are updated regularly, and are freely available from http://promice.dk.


** Software
:PROPERTIES:
:clearpage: t
:END:

This work was performed using only open-source software, primarily =GRASS GIS= citep:neteler_2012_GRASS and =Python= citep:van-rossum_1995_python, in particular the =Jupyter= citep:kluyver_2016_jupyter, =pandas= citep:mckinney_2010_pandas, =numpy= citep:oliphant_2006_numpy, =statsmodel= citep:seabold_2010_statsmodels, =x-array= citep:hoyer_2017_xarray, and =Matplotlib= citep:hunter_2007_matplotlib packages. The entire work was performed in =Emacs= using =Org Mode= citep:schulte_2012_a-multi-language. The =parallel= citep:tange_2011_parallel tool was used to speed up processing. We used =proj4= citep:proj4 to compute the errors in the EPSG 3413 projection. All code used in this work is available in the Supplemental Material.

* Code                                                  :noexport:
:PROPERTIES:
:header-args:sh+: :comments both
:header-args:sh+: :tangle-mode (identity #o744)
:header-args:sh+: :shebang #!/usr/bin/env bash
:END:
** Init

This code, and all code files in this project, are derived products tangled from the ice_discharge.org source file.

#+BEGIN_SRC sh :results verbatim :tangle runme.sh
# trash G # reset

# Create environment if it doesn't exist
if [[ ! -d ./G ]]; then grass -e -c EPSG:3413 ./G; fi

# grass ./G/PERMANENT # manual debug
# export GRASS_OVERWRITE=1

grass ./G/PERMANENT --exec ./import.sh
grass ./G/PERMANENT --exec ./gate_IO_runner.sh
grass ./G/PERMANENT --exec ./vel_eff.sh
grass ./G/PERMANENT --exec ./export.sh
python ./errors.py
python ./raw2discharge.py
grass ./G/PERMANENT --exec ./gate_export.sh
python ./figures.py
#+END_SRC
#+RESULTS:

** Misc Helper
*** Support pretty messages
#+NAME: MSGS_pretty_print
#+BEGIN_SRC sh :results verbatim :tangle no
RED='\033[0;31m'
ORANGE='\033[0;33m'
GREEN='\033[0;32m'
NC='\033[0m' # No Color
MSG_OK() { printf "${GREEN}${1}${NC}\n"; }
MSG_WARN() { printf "${ORANGE}WARNING: ${1}${NC}\n"; }
MSG_ERR() { echo "${RED}ERROR: ${1}${NC}\n" >&2; }
#+END_SRC
#+RESULTS:

*** GRASS config

https://grass.osgeo.org/grass74/manuals/variables.html

#+BEGIN_QUOTE
GRASS_VERBOSE
[all modules]
toggles verbosity level
-1 - complete silence (also errors and warnings are discarded)
0 - only errors and warnings are printed
1 - progress and important messages are printed (percent complete)
2 - all module messages are printed
3 - additional verbose messages are printed
#+END_QUOTE

#+NAME: GRASS_config
#+BEGIN_SRC sh :results verbatim :tangle no
export GRASS_VERBOSE=3
# export GRASS_MESSAGE_FORMAT=silent
export DATADIR=/home/kdm/data

set -x # print commands to STDOUT before running them
#+END_SRC
#+RESULTS:


** Import Data
:PROPERTIES:
:header-args:sh+: :tangle import.sh
:END:

#+BEGIN_SRC sh :results verbatim
<<MSGS_pretty_print>>
<<GRASS_config>>
#+END_SRC
#+RESULTS:

*** Bed and Surface
**** BedMachine v3
+ from [[textcite:Morlighem:2017BedMachine][Morlighem /et al./ (2017)]]
#+BEGIN_SRC sh :results verbatim
MSG_OK "BedMachine"
g.mapset -c BedMachine

for var in $(echo mask surface thickness bed errbed); do
  echo $var
  r.external source=netCDF:${DATADIR}/Morlighem_2017/BedMachineGreenland-2017-09-20.nc:${var} output=${var}
done

r.colors -a map=errbed color=haxby

g.mapset PERMANENT
g.region raster=surface@BedMachine res=200 -a -p
g.region -s
g.mapset BedMachine
g.region -dp

r.colors map=mask color=haxby

r.mapcalc "mask_ice = if(mask == 2, 1, null())"
#+END_SRC
#+RESULTS:
**** Bamber 2013
#+BEGIN_SRC sh :results verbatim
MSG_OK "Bamber 2013"
g.mapset -c Bamber_2013
r.in.gdal input=${DATADIR}/Bamber_2013/IceThickness.tif output=thickness
r.null thickness null=0
#+END_SRC
#+RESULTS:

**** Millan 2018
#+BEGIN_SRC sh :results verbatim
MSG_OK "Millan 2018"
g.mapset -c Millan_2018

x=2760
y=4044
res=150

# ###
# ### OLD
# ###
# FILE=Bathy_SEG_OIB_Millan_et_al_2018.nc
# # from the netcdf metadata, but x/y are swapped:
# n="-2616623"
# w="-109213.637"
# e=$(echo "$w + $x*$res" | bc -l)
# s=$(echo "$n - $y*$res" | bc -l)
# g.region e=$e w=$w s=$s n=$n res=$res -pl
# r.in.gdal -o input=netCDF:${ROOT}/${FILE}:BED output=BED_OLD
# r.region -c map=BED_OLD

###
### NEW
###
FILE=Bathy_SEG_OIB_Millan_et_al_2018_08082018.nc
n="-2616698.800"  # ymax
w="-109288.637" # xmin
e=$(echo "$w + $x*$res" | bc -l)
s=$(echo "$n - $y*$res" | bc -l)
g.region e=$e w=$w s=$s n=$n res=$res -pl
r.in.gdal -o input=netCDF:${DATADIR}/Millan_2018/${FILE}:BED output=BED_NEW
r.region -c map=BED_NEW

r.in.gdal -o input=netCDF:${DATADIR}/Millan_2018/${FILE}:THICKNESSGimp output=THICKNESS
r.region -c map=THICKNESS

g.region -d
r.mapcalc "bed_0 = BED_NEW"
r.null map=bed_0 null=0
r.mapcalc "thickness_0 = THICKNESS"
r.null map=thickness_0 null=0
#+END_SRC

***** Testing Old v. New (Millan)
#+BEGIN_SRC sh :results verbatim :tangle no
d.mon start=wx0
d.erase
d.rast bed@BedMachine
r.colors map=BED color=viridis
d.rast BED_OLD
d.rast BED_NEW

r.mapcalc "diff_old = BED_OLD - bed@BedMachine"
r.mapcalc "diff_new = BED_NEW - bed@BedMachine"
r.colors map=diff_old,diff_new color=differences
d.rast diff_old
d.rast diff_new
d.vect gates_final@gates_50_2500
#+END_SRC
#+RESULTS:

***** Testing Millan v. BedMachine for Ikertivaq
#+BEGIN_SRC sh :results verbatim :tangle no
d.mon start=wx0
d.erase
d.rast bed@BedMachine
d.rast thickness@BedMachine

r.mapcalc "thickness = surface@BedMachine - BED_NEW"
d.rast thickness
d.rast diff_new

d.vect gates_final@gates_50_2500

g.copy vector=gates_final@gates_50_2500,gates
#+END_SRC
#+RESULTS:

**** GIMP 0715
#+BEGIN_SRC sh :results verbatim
MSG_OK "GIMP 0715"
g.mapset -c GIMP.0715
ROOT=${DATADIR}/GIMP/0715

# reset
# g.remove -f type=raster name=$(g.list type=raster mapset=. separator=",")

# read in DEM, DAY, and ERR
# for f in $(ls ${ROOT}/reg/tile_?_?_reg_30m_???.tif); do
#   name=$(basename ${f})
#   r.external input=${f} output=${name}
# done
ls ${ROOT}/reg/tile_?_?_reg_30m_???.tif | parallel --bar r.external input={} output={/.}
ls ${ROOT}/fit/tile_?_?_fit_30m_???.tif | parallel --bar r.external input={} output={/.}

r.patch -s input=$(g.list type=raster pattern=tile_?_?_reg_30m_dem separator=,),$(g.list type=raster pattern=tile_?_?_fit_30m_dem separator=,) output=dem

# no fit day data to patch holes. We'll assign elevation pixels with DEM data but not DAY data to some day, TBD
r.patch -s input=$(g.list type=raster pattern=tile_?_?_reg_30m_day separator=,) output=day

r.patch -s input=$(g.list type=raster pattern=tile_?_?_reg_30m_err separator=,),$(g.list type=raster pattern=tile_?_?_fit_30m_err separator=,) output=err

r.null map=day null=0
#+END_SRC
#+RESULTS:

***** Display
#+BEGIN_SRC sh :results verbatim :tangle no
d.mon start=wx0
d.rast dem
d.rast vel_baseline@MEaSUREs.0478
d.rast vx_baseline@MEaSUREs.0478
d.rast vy_baseline@MEaSUREs.0478
# d.rast VX_2002_06_15@MEaSUREs.0646
for r in $(g.list type=raster pattern=VX_2002* mapset=* -m); do d.rast $r; done
d.vect gate_lines@Enderlin_2014 color=red
d.vect gate_xy@Enderlin_2014
d.rast gates_final@gates_50_2500
d.vect gates_final@gates_50_2500 color=red
#+END_SRC
#+RESULTS:


*** Sectors
+ From citet:mouginot_2019_glacier
***** Import & Clean
#+BEGIN_SRC sh :results verbatim
MSG_OK "Mouginot 2019 sectors"

g.mapset -c Mouginot_2019
v.in.ogr input=${DATADIR}/Mouginot_2019 output=sectors_all
v.extract input=sectors_all where="NAME NOT LIKE '%ICE_CAP%'" output=sectors

db.select table=sectors | head
v.db.addcolumn map=sectors columns="region_name varchar(100)"
db.execute sql="UPDATE sectors SET region_name=SUBREGION1 || \"___\" || NAME"

v.db.addcolumn map=sectors columns="area DOUBLE PRECISION"
v.to.db map=sectors option=area columns=area units=meters

mkdir -p ./tmp/
# db.select table=sectors > ./tmp/Mouginot_2019.txt

v.to.rast input=sectors output=sectors use=cat label_column=region_name
r.mapcalc "mask_GIC = if(sectors)"

# # regions map
v.to.rast input=sectors output=regions_tmp use=cat label_column=SUBREGION1
# which categories exist?
# r.category regions separator=comma | cut -d, -f2 | sort | uniq
# Convert categories to numbers
r.category regions_tmp separator=comma | sed s/NO/1/ | sed s/NE/2/ | sed s/CE/3/ | sed s/SE/4/ | sed s/SW/5/ | sed s/CW/6/ | sed s/NW/7/ > ./tmp/mouginot.cat
r.category regions_tmp separator=comma rules=./tmp/mouginot.cat
# r.category regions_tmp
r.mapcalc "regions = @regions_tmp"

# # region vector 
# r.to.vect input=regions output=regions type=area
# v.db.addcolumn map=regions column="REGION varchar(2)"
# v.what.vect map=regions column=REGION query_map=sectors query_column=SUBREGION1

# # mask
#+END_SRC
#+RESULTS:
***** Test
#+BEGIN_SRC sh :results verbatim :tangle no
grass74 ./G/Mouginot_2019
d.mon start=wx0
d.rast regions
d.rast sectors
d.vect sectors_all fill_color=none color=red
d.vect sectors fill_color=none
#+END_SRC
#+RESULTS:
*** 2D Area Error
+ EPSG:3413 has projection errors of \(\pm\) ~8% in Greenland
+ Method
  + Email: [[mu4e:msgid:m2tvxmd2xr.fsf@gmail.com][Re: {GRASS-user} scale error for each pixel]]
  + Webmail: https://www.mail-archive.com/grass-user@lists.osgeo.org/msg35005.html
#+BEGIN_SRC sh :results verbatim
MSG_OK "2D Area Error"
g.mapset PERMANENT

if [[ "" == $(g.list type=raster pattern=err_2D) ]]; then
    r.mask -r
    g.region -d

    g.region res=1000 -ap # do things faster
    r.mapcalc "x = x()"
    r.mapcalc "y = y()"
    r.latlong input=x output=lat_low
    r.latlong -l input=x output=lon_low

    r.out.xyz input=lon_low,lat_low separator=space > ./tmp/llxy.txt
    PROJSTR=$(g.proj -j)
    echo $PROJSTR

    paste -d" " <(cut -d" " -f1,2 ./tmp/llxy.txt) <(cut -d" " -f3,4 ./tmp/llxy.txt | proj -VS ${PROJSTR} | grep Areal | column -t | sed s/\ \ /,/g | cut -d, -f4) > ./tmp/xy_err.txt

    r.in.xyz input=./tmp/xy_err.txt  output=err_2D_inv separator=space
    r.mapcalc "err_2D = 1/(err_2D_inv^0.5)" # convert area error to linear multiplier error
    g.region -d

    r.latlong input=x output=lat # for exporting at full res
    r.latlong -l input=x output=lon
fi

# sayav done
g.region -d
#+END_SRC
#+RESULTS:

*** Velocity
**** MEaSUREs 
+ See: [[file:~/data/MEaSUREs/README.org][MEaSUREs README]]

+ [X] 0478 :: 2000 -- 2017 annual average
+ [X] 0646 :: Monthly velocity - sparse glacier coverage 1985 through 2016
+ [X] 0481 :: 6-11 day velocity
+ [ ] 0670 :: 1995 -- 2015 average
+ [ ] 0725 :: 2015 & 2016 annual average
+ [X] 0731 :: Monthly  ice sheet velocity 2015 through 2018

***** 0478.002 
+ MEaSUREs Greenland Ice Sheet Velocity Map from InSAR Data, Version 2
+ Winter velocity maps
****** Import
+ First read in the 200 m files
+ Then read in the 500 m files if there were no 200 m files
#+BEGIN_SRC sh :results verbatim
MSG_OK "MEaSURES.0478"
g.mapset -c MEaSUREs.0478

MSG_OK "  200 m..."
r.mask -r
ROOT=${DATADIR}/MEaSUREs/NSIDC-0478.002/
VX=$(find ${ROOT} -name "*mosaic200_*vx*" | head -n1) # DEBUG
for VX in $(find ${ROOT} -name "*mosaic200_*vx*"); do
  VY=${VX/vx/vy}
  EX=${VX/vx/ex}
  EY=${EX/ex/ey}
  DATE=$(dirname ${VX} | rev | cut -d"/" -f1 | rev | sed s/\\./_/g)
  # echo $DATE
  parallel --bar r.external source={1} output={2}_${DATE} ::: ${VX} ${VY} ${EX} ${EY} :::+ VX VY EX EY
  parallel --bar r.null map={}_${DATE} null=0 ::: VX VY EX EY
done
g.region raster=VX_${DATE} -pa

MSG_OK "  500 m..."
VX=$(find ${ROOT} -name "*mosaic500_*vx*" | head -n1) # DEBUG
for VX in $(find ${ROOT} -name "*mosaic500_*vx*"); do
  VY=${VX/vx/vy}
  EX=${VX/vx/ex}
  EY=${EX/ex/ey}
  DATE=$(dirname ${VX} | rev | cut -d"/" -f1 | rev | sed s/\\./_/g)
  echo $DATE

  # Read in all the 500 m velocity data
  parallel --bar r.external source={1} output={2}_${DATE}_500 ::: ${VX} ${VY} ${EX} ${EY} :::+ VX VY EX EY # If the 200 m data exists, will produce an error and continue
  # If the 200 m data does not exist, will resample from 500
  r.mapcalc "VX_${DATE} = VX_${DATE}_500"
  r.mapcalc "VY_${DATE} = VY_${DATE}_500"
  r.mapcalc "EX_${DATE} = EX_${DATE}_500"
  r.mapcalc "EY_${DATE} = EY_${DATE}_500"
  parallel --bar r.null map={}_${DATE} null=0 ::: VX VY EX EY
done
#+END_SRC
#+RESULTS:
****** Baseline: Average of 2015-2017
#+BEGIN_SRC sh :results verbatim
MSG_OK "Baseline"
g.mapset -c MEaSUREs.0478

r.mapcalc << EOF
VX_2015 = if(VX_2015_09_01 == -2*10^9, 0, VX_2015_09_01)
VX_2016 = if(VX_2016_09_01 == -2*10^9, 0, VX_2016_09_01)
VX_2017 = if(VX_2017_09_01 == -2*10^9, 0, VX_2017_09_01)
VY_2015 = if(VY_2015_09_01 == -2*10^9, 0, VY_2015_09_01)
VY_2016 = if(VY_2016_09_01 == -2*10^9, 0, VY_2016_09_01)
VY_2017 = if(VY_2017_09_01 == -2*10^9, 0, VY_2017_09_01)
vx_baseline = (VX_2015 + VX_2016 + VX_2017) / ((VX_2015 != 0) + (VX_2016 != 0) + (VX_2017 != 0))
vy_baseline = (VY_2015 + VY_2016 + VY_2017) / ((VY_2015 != 0) + (VY_2016 != 0) + (VY_2017 != 0))
EX_2015 = if(EX_2015_09_01 == -2*10^9, 0, EX_2015_09_01)
EX_2016 = if(EX_2016_09_01 == -2*10^9, 0, EX_2016_09_01)
EX_2017 = if(EX_2017_09_01 == -2*10^9, 0, EX_2017_09_01)
EY_2015 = if(EY_2015_09_01 == -2*10^9, 0, EY_2015_09_01)
EY_2016 = if(EY_2016_09_01 == -2*10^9, 0, EY_2016_09_01)
EY_2017 = if(EY_2017_09_01 == -2*10^9, 0, EY_2017_09_01)
ex_baseline = (EX_2015 + EX_2016 + EX_2017) / ((EX_2015 != 0) + (EX_2016 != 0) + (EX_2017 != 0))
ey_baseline = (EY_2015 + EY_2016 + EY_2017) / ((EY_2015 != 0) + (EY_2016 != 0) + (EY_2017 != 0))
vel_baseline = sqrt(vx_baseline^2 + vy_baseline^2)
vel_err_baseline = sqrt(ex_baseline^2 + ey_baseline^2)
EOF

# g.remove -f type=raster name=VX_2015,VX_2016,VX_2017,VY_2015,VY_2016,VY_2017
# g.remove -f type=raster name=EX_2015,EX_2016,EX_2017,EY_2015,EY_2016,EY_2017

parallel --bar r.null map={}_baseline setnull=0 ::: vx vy vel ex ey vel_err
r.colors -e map=vel_baseline,vel_err_baseline color=viridis
#+END_SRC
#+RESULTS:

****** Fill in holes
+ There are holes in the velocity data which will create false gates. Fill them in.
+ Clump based on yes/no velocity
  + Largest clump is GIS
  + 2nd largest is ocean
+ Mask by ocean (so velocity w/ holes remains)
+ Fill holes
#+BEGIN_SRC sh :results verbatim
r.mask -r
r.mapcalc "no_vel = if(isnull(vel_baseline), 1, null())"
r.mask no_vel
r.clump input=no_vel output=no_vel_clump --o
ocean_clump=$(r.stats -c -n no_vel_clump sort=desc | head -n1 | cut -d" " -f1)
r.mask -i raster=no_vel_clump maskcats=${ocean_clump} --o
r.fillnulls input=vel_baseline out=vel_baseline_filled method=bilinear
r.mask -r
g.rename raster=vel_baseline_filled,vel_baseline --o
r.colors map=vel_baseline -e color=viridis
#+END_SRC
#+RESULTS:
******* Display
#+BEGIN_SRC sh :results verbatim :tangle no
d.mon start=wx0
d.erase
d.rast vel
d.rast vel_filled
#+END_SRC
#+RESULTS:

***** 0646.002
+ MEaSUREs Greenland Ice Velocity: Selected Glacier Site Velocity Maps from Optical Images, Version 2
+ Monthly velocity maps
****** Generate VRTs
+ One map per month
+ Build GDAL virtual tiles for every month (when data exists)
#+BEGIN_SRC sh :results verbatim
g.mapset -c MEaSUREs.0646

ROOT=${DATADIR}/MEaSUREs/NSIDC-0646.002/
VRTROOT=${DATADIR}/MEaSUREs/NSIDC-0646.002.vrt/
# for year in $(seq 2000 2016); do
for year in $(seq 1985 2016); do
  for month in $(seq -w 1 12); do
    LIST=$(find ${ROOT} -name "*${year}-${month}_vx_v2.tif")
    if [[ ! -z ${LIST} ]]; then
      MSG_OK "Building VRTs for ${year} ${month}"
      if [[ ! -e ${VRTROOT}/${year}_${month}_ey.vrt ]]; then
        parallel --bar gdalbuildvrt ${VRTROOT}/${year}_${month}_{}.vrt $(find ${ROOT} -name "*${year}-${month}_{}_v2.tif") ::: vx vy ex ey
      fi
    fi
  done
done
#+END_SRC
#+RESULTS:

****** Import VRTs
#+BEGIN_SRC sh :results verbatim
MSG_OK "MEaSURES.0646"
g.mapset -c MEaSUREs.0646

r.mask -r
ROOT=${DATADIR}/MEaSUREs/NSIDC-0646.002.vrt
VX=$(find ${ROOT} -name "*vx*" | head -n1) # debug
for VX in $(find ${ROOT} -name "*vx*"); do
    VY=${VX/vx/vy}
    EX=${VX/vx/ex}
    EY=${EX/ex/ey}
    DATE=$(basename $VX | cut -d"_" -f1-2)
    DATE=${DATE}_15
    echo $DATE
    
    parallel --bar r.external source={1} output={2}_${DATE} ::: ${VX} ${VY} ${EX} ${EY} :::+ VX VY EX EY
done
g.region raster=VX_${DATE} -pa
# g.list type=raster mapset=MEaSUREs.0646
#+END_SRC
#+RESULTS:



***** 0481.001
****** Dates
+ Folders are not named using ISO date, so we'll build lightweight VRT files with better names that patch together all the sub-regions for each day.
#+BEGIN_SRC sh :results verbatim :tangle no
f=$(find ${DATADIR}/MEaSUREs/NSIDC-0481.001 -name "*vx*.tif" | head -n1) # DEBUG

# VRTs for any date where there exists any 0481 data
mkdir -p ${DATADIR}/MEaSUREs/NSIDC-0481.001.vrt
for f in $(find ${DATADIR}/MEaSUREs/NSIDC-0481.001 -name "*vx*.tif"); do
    ff=$(basename ${f})
    T0=$(echo ${ff} | cut -d"_" -f3)
    T1=$(echo ${ff} | cut -d"_" -f4)
    SEC0=$(date --date="${T0}" +"%s")
    SEC1=$(date --date="${T1}" +"%s")
    MID=$(echo "(${SEC0}+${SEC1})/2"|bc)
    DATE=$(date --date="@${MID}" +"%Y-%m-%d")
    gdalbuildvrt -vrtnodata -2E9 ${DATADIR}/MEaSUREs/NSIDC-0481.001.vrt/${DATE}_vx.vrt $(find ${DATADIR}/MEaSUREs/NSIDC-0481.001 -name "*${T0}_${T1}_*vx*.tif")
    gdalbuildvrt -vrtnodata -2E9 ${DATADIR}/MEaSUREs/NSIDC-0481.001.vrt/${DATE}_vy.vrt $(find ${DATADIR}/MEaSUREs/NSIDC-0481.001 -name "*${T0}_${T1}_*vy*.tif")
    gdalbuildvrt -vrtnodata -2E9 ${DATADIR}/MEaSUREs/NSIDC-0481.001.vrt/${DATE}_ex.vrt $(find ${DATADIR}/MEaSUREs/NSIDC-0481.001 -name "*${T0}_${T1}_*ex*.tif")
    gdalbuildvrt -vrtnodata -2E9 ${DATADIR}/MEaSUREs/NSIDC-0481.001.vrt/${DATE}_ey.vrt $(find ${DATADIR}/MEaSUREs/NSIDC-0481.001 -name "*${T0}_${T1}_*ey*.tif")
done

# Monthly VRTs including average of all scenes in that month
ROOT=${DATADIR}/MEaSUREs/NSIDC-0481.001.vrt
for year in $(seq 2009 2018); do
  for month in $(seq -w 1 12); do
    LIST=$(find ${ROOT} -name "*${year}-${month}-??_vx.vrt")
    if [[ ! -z ${LIST} ]]; then
      MSG_OK "Building VRT for ${year} ${month}"
      gdalbuildvrt ${ROOT}/${year}-${month}_vx.vrt $(find ${ROOT} -name "*${year}-${month}-??_vx.vrt")
      gdalbuildvrt ${ROOT}/${year}-${month}_vy.vrt $(find ${ROOT} -name "*${year}-${month}-??_vy.vrt")
      gdalbuildvrt ${ROOT}/${year}-${month}_ex.vrt $(find ${ROOT} -name "*${year}-${month}-??_ex.vrt")
      gdalbuildvrt ${ROOT}/${year}-${month}_ey.vrt $(find ${ROOT} -name "*${year}-${month}-??_ey.vrt")
    fi
  done
done


#+END_SRC
#+RESULTS:

***** 0731.001
MEaSUREs Greenland Monthly Ice Sheet Velocity Mosaics from SAR and Landsat, Version 1

****** Import
#+BEGIN_SRC sh :results verbatim
MSG_OK "MEaSURES.0731"
g.mapset -c MEaSUREs.0731
r.mask -r
ROOT=${DATADIR}/MEaSUREs/NSIDC-0731.001/
VX=$(find ${ROOT} -name "*mosaic_*vx*.tif" | head -n1) # DEBUG
for VX in $(find ${ROOT} -name "*mosaic_*vx*.tif"); do
  VY=${VX/vx/vy}
  EX=${VX/vx/ex}
  EY=${EX/ex/ey}

  T0=$(dirname ${VX} | rev | cut -d"/" -f1 | rev|cut -d"_" -f4)
  T1=$(dirname ${VX} | rev | cut -d"/" -f1 | rev|cut -d"_" -f5)
  SEC0=$(date --date="${T0}" +"%s")
  SEC1=$(date --date="${T1}" +"%s")
  MID=$(echo "(${SEC0}+${SEC1})/2"|bc)
  DATE=$(date --date="@${MID}" +"%Y_%m_%d")

  # echo $DATE
  parallel --bar r.external source={1} output={2}_${DATE} ::: ${VX} ${VY} ${EX} ${EY} :::+ VX VY EX EY
  parallel --bar r.null map={}_${DATE} null=0 ::: VX VY EX EY
done
g.region raster=VX_${DATE} -pa
#+END_SRC
#+RESULTS:


**** Sentinel 1
***** Data Intro                                                   :noexport:
#+BEGIN_SRC sh :results verbatim :tangle no
DIR=${DATADIR}/Sentinel1/Sentinel1_IV_maps
(cd ${DIR}; ls *.nc | head)
(cd ${DIR}; ncdump -h $(ls *.nc | head -n1) | grep "float")
#+END_SRC
#+RESULTS:
#+begin_example
IV_20160913_20161006.nc
IV_20160925_20161018.nc
IV_20161007_20161030.nc
IV_20161013_20161105.nc
IV_20161019_20161111.nc
IV_20161031_20161123.nc
IV_20161106_20161129.nc
IV_20161118_20161211.nc
IV_20161124_20161217.nc
IV_20161130_20161223.nc
	float land_ice_surface_easting_velocity(time, y, x) ;
	float land_ice_surface_northing_velocity(time, y, x) ;
	float land_ice_surface_vertical_velocity(time, y, x) ;
	float land_ice_surface_velocity_magnitude(time, y, x) ;
	float land_ice_surface_easting_velocity_std(time, y, x) ;
	float land_ice_surface_northing_velocity_std(time, y, x) ;
	float land_ice_surface_velocity_magnitude_std(time, y, x) ;
#+end_example

***** Import data                                                  :noexport:
+ Read in all the data
+ Convert from [m day-1] to [m year-1]
#+BEGIN_SRC sh :results verbatim
MSG_OK "Sentinel 1"
g.mapset -c Sentinel1
ROOT=${DATADIR}/Sentinel1/Sentinel1_IV_maps

find ${ROOT} -name "*.nc"
# FILE=$(find ${ROOT} -name "*.nc"|head -n1) # testing

FILE=$(find ${ROOT} -name "*.nc" | head -n1) # DEBUG
for FILE in $(find ${ROOT} -name "*.nc"); do
  T=$(ncdump -v time $FILE | tail -n2 | tr -dc '[0-9]')
  DATE=$(date --date="1990-01-01 +${T} days" --iso-8601)
  DATE_STR=$(echo ${DATE} | sed s/-/_/g)
  echo $DATE

  # TT=$(ncdump -v time_bnds $FILE | tail -n2 | head -n1)
  # T0=$(echo ${TT} | cut -d, -f1)
  # T1=$(echo ${TT} | cut -d, -f2 | tr -dc [0-9])
  # D0=$(date --date="1990-01-01 +${T0} days" --iso-8601)
  # D1=$(date --date="1990-01-01 +${T1} days" --iso-8601)

  r.external -o source="NetCDF:${FILE}:land_ice_surface_easting_velocity" output=vx_${DATE_STR}
  r.external -o source="NetCDF:${FILE}:land_ice_surface_northing_velocity" output=vy_${DATE_STR}

  r.external -o source="NetCDF:${FILE}:land_ice_surface_easting_velocity_std" output=ex_${DATE_STR}
  r.external -o source="NetCDF:${FILE}:land_ice_surface_northing_velocity_std" output=ey_${DATE_STR}
done
#+END_SRC
#+RESULTS:

**** Mouginot 2018 (pre-2000 velocities)
+ See citet:mouginot_2018_1972to1990 and citet:mouginot_2018_1991to2000

#+BEGIN_SRC sh :results verbatim
MSG_OK "Mouginot pre 2000"
g.mapset -c Mouginot_pre2000

ROOT=${DATADIR}/Mouginot_2018/D1GW91
find ${ROOT} -name "*.nc"
FILE=$(find ${ROOT} -name "*.nc" | head -n1) # DEBUG
for FILE in $(find ${ROOT} -name "*.nc"); do
  YYYYMMDD=$(echo ${FILE} | cut -d"_" -f4)
  YEAR=$(echo ${YYYYMMDD} | cut -d"-" -f1)
  DATE=${YEAR}_01_01
  echo $DATE
  r.external -o source="NetCDF:${FILE}:VX" output=vx_${DATE}
  r.external -o source="NetCDF:${FILE}:VY" output=vy_${DATE}
done

ROOT=${DATADIR}/Mouginot_2018/D1MM37
find ${ROOT} -name "*.nc"
FILE=$(find ${ROOT} -name "*.nc" | head -n1) # DEBUG
for FILE in $(find ${ROOT} -name "*.nc"); do
  YYYYMMDD=$(echo ${FILE} | cut -d"_" -f4)
  YEAR=$(echo ${YYYYMMDD} | cut -d"-" -f1)
  DATE=${YEAR}_01_01
  echo $DATE
  r.external -o source="NetCDF:${FILE}:VX" output=vx_${DATE}
  r.external -o source="NetCDF:${FILE}:VY" output=vy_${DATE}
done
#+END_SRC
#+RESULTS:

***** Display
#+BEGIN_SRC sh :results verbatim :tangle no
d.mon start=wx0
g.list type=raster pattern=vx_*

d.erase; d.rast vx_1990-07-01
d.erase; d.rast vx_1991-07-01
d.erase; d.rast vx_1992-07-01
d.erase; d.rast vx_1993-07-01
d.erase; d.rast vx_1994-07-01
d.erase; d.rast vx_1995-07-01
d.erase; d.rast vx_1996-07-01
d.erase; d.rast vx_1997-07-01
d.erase; d.rast vx_1998-07-01
d.erase; d.rast vx_1999-07-01
#+END_SRC
#+RESULTS:


*** Glacier Names
+ From [[textcite:Bjork:2015Brief][Bjørk /et al./ (2015)]].
+ Also use citet:mouginot_2019_glacier
**** Bjørk 2015
+ Write out x,y,name. Can use x,y and mean gate location to find closest name for each gate.
#+BEGIN_SRC sh :results verbatim
MSG_OK "Bjørk 2015"
g.mapset -c Bjork_2015

ROOT=${DATADIR}/Bjork_2015/

cat ${ROOT}/GreenlandGlacierNames_GGNv01.csv |  iconv -c -f utf-8 -t ascii | grep GrIS | awk -F';' '{print $3"|"$2"|"$7}' | sed s/,/./g | m.proj -i input=- | sed s/0.00\ //g | v.in.ascii input=- output=names columns="x double precision, y double precision, name varchar(99)"

# db.select table=names | tr '|' ',' > ./tmp/Bjork_2015_names.csv
#+END_SRC
#+RESULTS:
**** Mouginot 2019
#+BEGIN_SRC sh :results verbatim
g.mapset Mouginot_2019
db.select table=sectors | head
# v.out.ascii -c input=sectors output=./tmp/Mouginot_2019_names.csv columns=NAME,SUBREGION1
#+END_SRC
#+RESULTS:


*** Elevation
:PROPERTIES:
:ID:       E36D32FE-DA16-4441-8C0F-9F2506B6143D
:END:
+ h_0 (BedMachine v3) is assumed to be 2007
+ h_{\(\pm\) n} is based on elevation data from Shfaqat Abbas Khan. See [[file:${DATADIR}/Khan_2016/README.org]]
**** dh/dt
#+BEGIN_SRC sh :results verbatim
MSG_OK "dh/dt"

g.mapset -c Khan_2016
r.mask -r

g.region -d
g.region res=2000 -pa

FILE=${DATADIR}/Khan_2016/dhdt_1995-2015_GrIS.txt
head -n1 $FILE

cat << EOF > ./tmp/elev_filter.txt
TITLE     See r.mfilter manual
    MATRIX    3
    1 1 1
    1 1 1
    1 1 1
    DIVISOR   0
    TYPE      P
EOF

Y=1995 # debug
for Y in $(seq 1995 2015); do
  col=$(echo "$Y-1995+3" | bc -l)
  if [[ "" == $(g.list type=raster pattern=dh_${Y}) ]]; then
    # remove comments, leading spaces, and convert spaces to comma, swap lat,lon, then import
    cat ${FILE} | grep -v "^%" | sed s/^\ *//g | sed s/\ \ \*/,/g | cut -d"," -f1,2,${col} | awk -F, '{print $2 "|" $1 "|" $3}' | m.proj -i input=- | r.in.xyz input=- output=dh_${Y}_unfiltered
    r.mfilter -z input=dh_${Y}_unfiltered output=dh_${Y} filter=./tmp/elev_filter.txt repeat=2
    # r.resamp.filter input=dh_${Y}_filtered output=dh_${Y} filter=gauss radius=7
    # r.mfilter input=dh_${Y}_filtered output=dh_${Y} filter=./tmp/elev_filter.txt repeat=2
    r.colors map=dh_${Y} color=difference
  fi
done

# We don't have dh 2016 and 2017 - assume it is the same as 2015
# g.copy raster=dh_2015,dh_2016
# g.copy raster=dh_2015,dh_2017

# for Y in $(seq 1995 2015); do r.info dh_${Y}| grep "Range"; done
#+END_SRC
#+RESULTS:

** Find Gates
:PROPERTIES:
:header-args:sh+: :tangle gate_IO.sh
:END:

*** Algorithm
+ [X] Find all fast-moving ice (>X m yr^{-1})
  S+ Results not very sensitive to velocity limit (10 to 100 m yr^{-1} examined)
+ [X] Find grounding line by finding edge cells where fast-moving ice borders water or ice shelf based (loosely) on BedMachine mask
+ [X] Move grounding line cells inland by X km, again limiting to regions of fast ice.
  + Results not very sensitive to gate position (1 - 5 km range examined)

+ [X] Discard gates if group size \in [1,2]
+ [X] Manually clean a few areas (e.g. land-terminating glaciers, gates due to invalid masks, etc.) by manually selecting invalid regions in Google Earth, then remove gates in these regions

Note that "fast ice" refers to flow velocity, not the sea ice term of "stuck to the land".

INSTRUCTIONS: Set VELOCITY_CUTOFF and BUFFER_DIST to 50 and 2500 respectively and run the code. Then repeat for a range of other velocity cutoffs and buffer distances to get a range of sensitivities.

OR: Tangle via ((org-babel-tangle) the code below (C-c C-v C-t or ) to [[./gate_IO.sh]] and then run this in a GRASS session:
#+BEGIN_SRC sh :results verbatim :tangle gate_IO_runner.sh
<<MSGS_pretty_print>>
<<GRASS_config>>

VELOCITY_CUTOFF=100
BUFFER_DIST=5000
. ./gate_IO.sh
#+END_SRC
#+RESULTS:

Create a new mapset for this specific velocity cutoff and buffer distance

#+BEGIN_SRC sh :results verbatim
g.mapset -c gates_${VELOCITY_CUTOFF}_${BUFFER_DIST}
g.region -d
#+END_SRC

From above:

+ [X] Find grounding line by finding edge cells where fast-moving ice borders water or ice shelf based (loosely) on BedMachine mask

The "loosely" is because the BedMachine mask doesn't always reach into each fjord all the way. I buffer the BedMachine mask by 2 km here so that it extends to the edge of the velocity data.

#+BEGIN_SRC sh :results verbatim
g.copy raster=mask_ice@BedMachine,mask_ice --o
# Grow by 2 km (10 cells @ 200 m/cell)
r.grow input=mask_ice output=mask_ice_grow radius=10 new=1 --o
r.mask mask_ice_grow
#+END_SRC

The fast ice edge is where there is fast-flowing ice overlapping with not-ice.

#+BEGIN_SRC sh :results verbatim
r.mapcalc "fast_ice = if(vel_baseline@MEaSUREs.0478 > ${VELOCITY_CUTOFF}, 1, null())" --o
r.mask -r

# no velocity data, or is flagged as ice shelf or land in BedMachine
r.mapcalc "not_ice = if(isnull(vel_baseline@MEaSUREs.0478) ||| (mask@BedMachine == 0) ||| (mask@BedMachine == 3), 1, null())" --o

r.grow input=not_ice output=not_ice_grow radius=1.5 new=99 --o
r.mapcalc "fast_ice_edge = if(((not_ice_grow == 99) && (fast_ice == 1)), 1, null())" --o
#+END_SRC

The gates are set ${BUFFER_DIST} inland from the fast ice edge. This is done by buffering the fast ice edge (which fills the space between the fast ice edge and buffer extent) and then growing the buffer by 1. This last step defines the gate locations.

However, in order to properly estimate discharge, the gate location is not enough. Ice must flow from outside the gates, through the gates, to inside the gates, and not flow from one gate pixel to another gate pixel (or it would be counted 2x). 

#+BEGIN_SRC sh :results verbatim
r.buffer input=fast_ice_edge output=fast_ice_buffer distances=${BUFFER_DIST} --o
r.grow input=fast_ice_buffer output=fast_ice_buffer_grow radius=1.5 new=99 --o
r.mask -i not_ice --o
r.mapcalc "gates_inside = if(((fast_ice_buffer_grow == 99) && (fast_ice == 1)), 1, null())" --o
r.mask -r

r.grow input=gates_inside output=gates_inside_grow radius=1.1 new=99 --o
r.mask -i not_ice --o
r.mapcalc "gates_maybe = if(((gates_inside_grow == 99) && (fast_ice == 1) && isnull(fast_ice_buffer)), 1, null())" --o
r.mask -r

r.grow input=gates_maybe output=gates_maybe_grow radius=1.1 new=99 --o
r.mask -i not_ice --o
r.mapcalc "gates_outside = if(((gates_maybe_grow == 99) && (fast_ice == 1) && isnull(fast_ice_buffer) && isnull(gates_inside)), 1, null())" --o
r.mask -r

r.mapcalc "gates_IO = 0" --o
r.mapcalc "gates_IO = if(isnull(gates_inside), gates_IO, 1)" --o
r.mapcalc "gates_IO = if(isnull(gates_outside), gates_IO, -1)" --o

r.colors map=gates_inside color=red
r.colors map=gates_maybe color=grey
r.colors map=gates_outside color=blue
r.colors map=gates_IO color=viridis
#+END_SRC
#+RESULTS:

+ For each gate, split into two for the vector components of the velocity, then...
+ If flow is from gate to INSIDE, it is discharged
+ If flow is from gate to GATE, it is ignored
+ If flow is from gate to NOT(GATE || INSIDE) it is ignored
  + If gates are a closed loop, such as the 1700 m flight-line, then
    this scenario would be NEGATIVE discharge, not ignored. This was
    tested with the 1700 m flight-line and compared against both the
    vector calculations and WIC estimates.

#+NAME: tbl_velocity
| var            | value  | meaning           |
|----------------+--------+-------------------|
| vx             | > 0    | east / right      |
| vx             | < 0    | west / left       |
| vy             | > 0    | north / up        |
| vy             | < 0    | south / down      |
|----------------+--------+-------------------|
| GRASS indexing | [0,1]  | cell to the right |
|                | [0,-1] | left              |
|                | [-1,0] | above             |
|                | [1,0]  | below             |

#+BEGIN_SRC sh :results verbatim
# g.mapset -c gates_50_2500

r.mask -r

r.mapcalc "gates_x = 0" --o
r.mapcalc "gates_x = if((gates_maybe == 1) && (vx_baseline@MEaSUREs.0478 > 0), gates_IO[0,1], gates_x)" --o
r.mapcalc "gates_x = if((gates_maybe != 0) && (vx_baseline@MEaSUREs.0478 < 0), gates_IO[0,-1], gates_x)" --o

r.mapcalc "gates_y = 0" --o
r.mapcalc "gates_y = if((gates_maybe != 0) && (vy_baseline@MEaSUREs.0478 > 0), gates_IO[-1,0], gates_y)" --o
r.mapcalc "gates_y = if((gates_maybe != 0) && (vy_baseline@MEaSUREs.0478 < 0), gates_IO[1,0], gates_y)" --o

r.mapcalc "gates_x = if(gates_x == 1, 1, 0)" --o
r.mapcalc "gates_y = if(gates_y == 1, 1, 0)" --o

r.null map=gates_x null=0 # OR r.null map=gates_x setnull=0
r.null map=gates_y null=0 # OR r.null map=gates_y setnull=0
#+END_SRC
*** Clean Gates                                                    :noexport:
**** Subset to where there is known discharge
#+BEGIN_SRC sh :results verbatim
r.mapcalc "gates_xy_clean0 = if((gates_x == 1) || (gates_y == 1), 1, null())" --o
#+END_SRC
#+RESULTS:

**** Remove small areas (clusters <X cells)
#+BEGIN_SRC sh :results verbatim
# Remove clusters of 2 or less. How many hectares in X pixels?
# frink "(200 m)^2 * 2 -> hectares" # ans: 8.0

r.clump -d input=gates_xy_clean0 output=gates_gateID --o
r.reclass.area -d input=gates_gateID output=gates_area value=9 mode=lesser method=reclass --o

r.mapcalc "gates_xy_clean1 = if(isnull(gates_area), gates_xy_clean0, null())" --o
#+END_SRC
#+RESULTS:


**** Limit to Mouginot 2019 mask
+ Actually, limit to approximate Mouginot 2019 mask - its a bit narrow in some places
#+BEGIN_SRC sh :results verbatim
# r.mask mask_GIC@Mouginot_2019 --o
r.grow input=mask_GIC@Mouginot_2019 output=mask_GIC_Mouginot_2019_grow radius=4.5 # three cells
r.mask mask_GIC_Mouginot_2019_grow --o
r.mapcalc "gates_xy_clean2 = gates_xy_clean1" --o
r.mask -r

# r.univar map=gates_xy_clean1
# r.univar map=gates_xy_clean2
#+END_SRC

**** Remove areas from manually-drawn KML mask
+ See [[./dat/remove_manual.kml]]
#+BEGIN_SRC sh :results verbatim
v.import input=./dat/remove_manual.kml output=remove_manual --o
r.mask -i vector=remove_manual --o
r.mapcalc "gates_xy_clean3 = gates_xy_clean2" --o
r.mask -r

r.univar map=gates_xy_clean2
r.univar map=gates_xy_clean3
#+END_SRC
#+RESULTS:

*** Final Gates
#+BEGIN_SRC sh :results verbatim
g.copy "gates_xy_clean3,gates_final" --o
#+end_src
#+RESULTS:

*** Add meta-data to gates
Add:
+ Gate ID
+ Calculate the average x,y of the gate, and then from that ONE point, determine the following. Do this from the average point rather than for each gate pixel because some gates span multiple sectors, or different ends of the gate are nearer different names, etc.
  + Average lon,lat of gate
  + Nearest citet:mouginot_2019_glacier region, sector, and name
  + Nearest citet:bjork_2015_brief name

Do this for both the area vector and the point vector so that we can export
+ KML and GeoPackage with gates and metadata
+ simple CSV w/ gates and metadata.

**** Gate ID
#+BEGIN_SRC sh :results verbatim

# db.droptable -f table=gates_final
# db.droptable -f table=gates_final_pts

# areas (clusters of gate pixels, but diagonals are separate)
r.to.vect input=gates_final output=gates_final type=area --o
v.db.dropcolumn map=gates_final column=label
v.db.dropcolumn map=gates_final column=value
v.db.addcolumn map=gates_final columns="gate INT"
v.what.rast map=gates_final raster=gates_gateID column=gate type=centroid

# # points (each individual gate pixel)
# r.to.vect input=gates_final output=gates_final_pts type=point --o
# v.db.dropcolumn map=gates_final_pts column=label
# v.db.dropcolumn map=gates_final_pts column=value
# v.db.addcolumn map=gates_final_pts columns="gate INT"
# v.what.rast map=gates_final_pts raster=gates_gateID column=gate type=point
#+END_SRC
#+RESULTS:
**** Mean x,y
#+BEGIN_SRC sh :results verbatim
v.db.addcolumn map=gates_final columns="x INT, y INT, mean_x INT, mean_y INT, area INT"
v.to.db map=gates_final option=coor columns=x,y units=meters
v.to.db map=gates_final option=area columns=area units=meters

for G in $(db.select -c sql="select gate from gates_final"|sort -n|uniq); do
  db.execute sql="UPDATE gates_final SET mean_x=(SELECT AVG(x) FROM gates_final WHERE gate == ${G}) where gate == ${G}"
  db.execute sql="UPDATE gates_final SET mean_y=(SELECT AVG(y) FROM gates_final WHERE gate == ${G}) where gate == ${G}"
done

v.out.ascii -c input=gates_final columns=gate,mean_x,mean_y | cut -d"|" -f4- | sort -n|uniq | v.in.ascii input=- output=gates_final_pts skip=1 cat=1 x=2 y=3 --o
v.db.addtable gates_final_pts
v.db.addcolumn map=gates_final_pts columns="gate INT"
v.db.update map=gates_final_pts column=gate query_column=cat

v.db.addcolumn map=gates_final_pts columns="mean_x INT, mean_y INT"
v.to.db map=gates_final_pts option=coor columns=mean_x,mean_y units=meters
#+END_SRC
#+RESULTS:

Here we have:
#+BEGIN_SRC sh :results verbatim :tangle no
db.select table=gates_final|head -n10 # cat|gate|x|y|mean_x|mean_y
db.select table=gates_final_pts|head # cat|gate|mean_x|mean_y
#+END_SRC
#+RESULTS:

**** Mean lon,lat
#+BEGIN_SRC sh :results verbatim
v.what.rast map=gates_final_pts raster=lon@PERMANENT column=lon
v.what.rast map=gates_final_pts raster=lat@PERMANENT column=lat

v.db.addcolumn map=gates_final columns="mean_lon DOUBLE PRECISION, mean_lat DOUBLE PRECISION"
for G in $(db.select -c sql="select gate from gates_final"|sort -n|uniq); do
    db.execute sql="UPDATE gates_final SET mean_lon=(SELECT lon FROM gates_final_pts WHERE gate = ${G}) where gate = ${G}"
    db.execute sql="UPDATE gates_final SET mean_lat=(SELECT lat FROM gates_final_pts WHERE gate = ${G}) where gate = ${G}"
done
#+END_SRC
#+RESULTS:

**** Sector, Region, Names, etc.
+ Sector Number
+ Region Code
+ Nearest Sector or Glacier Name
#+BEGIN_SRC sh :results verbatim
v.db.addcolumn map=gates_final columns="sector INT"
v.db.addcolumn map=gates_final_pts columns="sector INT"
v.distance from=gates_final to=sectors@Mouginot_2019 upload=to_attr column=sector to_column=cat
v.distance from=gates_final_pts to=sectors@Mouginot_2019 upload=to_attr column=sector to_column=cat

v.db.addcolumn map=gates_final columns="region VARCHAR(2)"
v.db.addcolumn map=gates_final_pts columns="region VARCHAR(2)"
v.distance from=gates_final to=sectors@Mouginot_2019 upload=to_attr column=region to_column=SUBREGION1
v.distance from=gates_final_pts to=sectors@Mouginot_2019 upload=to_attr column=region to_column=SUBREGION1

v.db.addcolumn map=gates_final columns="Mouginot_2019 VARCHAR(99)"
v.db.addcolumn map=gates_final_pts columns="Mouginot_2019 VARCHAR(99)"
v.distance from=gates_final to=sectors@Mouginot_2019 upload=to_attr column=Mouginot_2019 to_column=NAME
v.distance from=gates_final_pts to=sectors@Mouginot_2019 upload=to_attr column=Mouginot_2019 to_column=NAME

v.db.addcolumn map=gates_final columns="Bjork_2015 VARCHAR(99)"
v.db.addcolumn map=gates_final_pts columns="Bjork_2015 VARCHAR(99)"
v.distance from=gates_final to=names@Bjork_2015 upload=to_attr column=Bjork_2015 to_column=name
v.distance from=gates_final_pts to=names@Bjork_2015 upload=to_attr column=Bjork_2015 to_column=name


v.db.addcolumn map=gates_final columns="n_pixels INT"
v.db.addcolumn map=gates_final_pts columns="n_pixels INT"
for G in $(db.select -c sql="select gate from gates_final"|sort -n|uniq); do
    db.execute sql="UPDATE gates_final SET n_pixels=(SELECT SUM(area)/(200*200) FROM gates_final WHERE gate = ${G}) where gate = ${G}"
    # now copy that to the average gate location (point) table
    db.execute sql="UPDATE gates_final_pts SET n_pixels = (SELECT n_pixels FROM gates_final WHERE gate = ${G}) WHERE gate = ${G}"
done
#+END_SRC
**** Clean up
#+BEGIN_SRC sh :results verbatim
db.dropcolumn -f table=gates_final column=area
# db.dropcolumn -f table=gates_final column=cat
#+END_SRC
#+RESULTS:

**** Export as metadata CSV
#+BEGIN_SRC sh :results verbatim
mkdir -p out
db.select sql="SELECT gate,mean_x,mean_y,lon,lat,n_pixels,sector,region,Bjork_2015,Mouginot_2019 from gates_final_pts" separator=, | sort -n | uniq  > ./out/gate_meta.csv
#+END_SRC
#+RESULTS:

*** Export Gates to KML                                            :noexport:
#+BEGIN_SRC sh :results verbatim
v.out.ogr input=gates_final output=./tmp/gates_final_${VELOCITY_CUTOFF}_${BUFFER_DIST}.kml format=KML --o
# open ./tmp/gates_final_${VELOCITY_CUTOFF}_${BUFFER_DIST}.kml
#+END_SRC
#+RESULTS:

*** Sensitivity of results to gate distance and cutoff velocity
:PROPERTIES:
:header-args:sh: :tangle no
:END:

Run the gate detection algorithm at a variety of cutoff velocities and buffer distances
#+BEGIN_SRC sh :results verbatim :tangle no
for VELOCITY_CUTOFF in $(echo 10 20 30 40 50 60 70 80 90 100 125 150); do
  for BUFFER_DIST in $(echo 1000 2000 3000 4000 5000 6000 7000 8000 9000); do
      . ./gate_IO.sh
  done
done
#+END_SRC
#+RESULTS:

**** Heatmap
***** Compute
#+BEGIN_SRC sh :results verbatim
rm ./tmp/gate_test.dat
for M in $(g.mapset -l | tr ' ' '\n' | grep gates); do
  g.mapset ${M} --quiet
  r.mask -r --quiet
  g.region -d

  ### To generate this heatmap for a sub-region or single glacier, PRIOR to
  ### this in PERMANENT mapset, zoom in, then "set computational region extent from display",
  ### then save w/ "g.region save=SUBREGION --o", then in the loop zoom in on each mapset with:
  # g.region res=200 align=vel_baseline@MEaSUREs.0478 -pa
  # g.region region=SUBREGION@PERMANENT --o -pa

  r.mapcalc "vel_eff = (if(gates_x == 1, abs(vx_baseline@MEaSUREs.0478), 0) + if(gates_y == 1, abs(vy_baseline@MEaSUREs.0478), 0))" --o
  r.mask gates_final --o
  # frink "(m/yr * m) * m * kg/m^3 * 1E12 -> Gt/yr"

  ### simple D w/ "raw" unadjusted thickness
  r.mapcalc "tmp_D = (vel_eff * 200) * thickness@BedMachine * 917 / pow(10.0, 12)" --o

  ### D w/ adjusted thickness
  # in BedMachine mapset, from the OLS fits.summary()
  # r.mapcalc "thick_fit_adj=if(thickness<20,380*log(vel_baseline@MEaSUREs.0478,10)-380,thickness)"
  # r.mapcalc "tmp_D = (vel_eff * 200) * thick_fit_adj@BedMachine * 917 / pow(10.0, 12)" --o

  r.univar tmp_D | grep sum
  echo ${M} $(r.univar tmp_D | grep sum) >> ./tmp/gate_test.dat
  g.remove -f type=raster name=tmp_D
done
#+END_SRC

#+BEGIN_SRC sh :results verbatim
cat ./tmp/gate_test.dat | sort
#+END_SRC

***** Display

#+BEGIN_SRC jupyter-python :session v_b_compare :exports results :results raw drawer :display text/org
# %matplotlib qt5
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

df0 = pd.read_csv('./tmp/gate_test.dat', sep=" ", names=['param','D'], usecols=((0,2)), index_col=0)
vel = np.sort(np.unique([_.split('_')[1] for _ in df0.index.values.astype(str)]).astype(np.int))
buf = np.unique([_.split('_')[2] for _ in df0.index.values.astype(str)]).astype(np.int)
df = pd.DataFrame(columns=vel, index=buf)
for v in vel:
    for b in buf:
        idx='gates_'+str(v)+'_'+str(b)
        df.loc[b,v] = np.float(df0.loc[idx].values[0])

df = df.apply(pd.to_numeric)
df_int = df.apply(pd.to_numeric).round().astype(np.int)

fig = plt.figure(1, figsize=(8,6)) # w,h
fig.clf()
fig.set_tight_layout(True)
ax = fig.add_subplot(111)

sns.set()

im = sns.heatmap(df,
                 annot=df_int, annot_kws={"size":12},
                 vmin=440, vmax=490,
                 # vmin=470, vmax=510,
                 cbar_kws={'label':'Unadjusted Baseline Discharge [Gt yr$^{-1}$]'},
                           #, 'extend':'min'},
                           # "ticks":[470,480,490,500,510]},
                 fmt='d',
                 ax=ax)

plt.yticks(rotation=0)
ax.text(0.5, -0.1, 'Cutoff Velocity [m yr$^{-1}$]',
        transform=ax.transAxes, horizontalalignment='center')
ax.text(-0.14, 0.5, 'Distance from terminus [m]',
        rotation='vertical',
        transform=ax.transAxes, verticalalignment='center')

       
plt.savefig('./figs/heatmap_all.png', transparent=True, bbox_inches='tight', dpi=300)
plt.savefig('./figs/heatmap_all.pdf', bbox_inches='tight')

df.round()
#+END_SRC

** Effective Velocity
:PROPERTIES:
:header-args:sh+: :tangle vel_eff.sh
:END:

#+BEGIN_SRC sh :results verbatim
<<MSGS_pretty_print>>
<<GRASS_config>>
#+END_SRC
#+RESULTS:


*** NOTDONE All Mapsets
Effective velocity (because gates may only be valid in =x= or =y= direction) was calculated for the sensitivity test above at one time, but for all gate locations (buffer dist and speed cutoff). Now we need to calculate it at all times

#+BEGIN_SRC sh :results verbatim :tangle no
g.mapsets gates_100_5000
r.mask -r

mapset=$(g.mapset -l | tr ' ' '\n' | grep -E gates_??_????| head -n1) # DEBUG
for mapset in $(g.mapset -l | tr ' ' '\n' | grep -E gates_??_????); do
  g.mapset ${mapset} --quiet

  g.region -d
  r.mapcalc "MASK = if((gates_x == 1) | (gates_y == 1), 1, null())" --o
  VX=$(g.list -m type=raster pattern=VX_????_??_?? mapset=* | head -n1) # DEBUG
  for VX in $(g.list -m type=raster pattern=VX_????_??_?? mapset=*); do
    VY=${VX/VX/VY}
    EX=${VX/VX/EX}
    EY=${VX/VX/EY}
    DATE=$(echo $VX | cut -d"_" -f2- | cut -d@ -f1)
    echo $DATE
    r.mapcalc "vel_eff_${DATE} = if(gates_x == 1, if(${VX} == -2*10^9, 0, abs(${VX})), 0) + if(gates_y, if(${VY} == -2*10^9, 0, abs(${VY})), 0)"
    r.mapcalc "err_eff_${DATE} = if(gates_x@gates_50_2500 == 1, if(${EX} == -2*10^9, 0, abs(${EX})), 0) + if(gates_y@gates_50_2500, if(${EY} == -2*10^9, 0, abs(${EY})), 0)"
  done

  VX=$(g.list -m type=raster pattern=vx_????_??_?? mapset=Sentinel1 | head -n1) # DEBUG
  for VX in $(g.list -m type=raster pattern=vx_????_??_?? mapset=Sentinel1); do
    VY=${VX/vx/vy}
    EX=${VX/vx/ex}
    EY=${VX/vx/ey}
    DATE=$(echo $VX | cut -d"_" -f2- | cut -d@ -f1)
    echo $DATE
    r.mapcalc "vel_eff_${DATE} = 365 * (if(gates_x@gates_50_2500 == 1, if(isnull(${VX}), 0, abs(${VX}))) + if(gates_y@gates_50_2500, if(isnull(${VY}), 0, abs(${VY}))))"
    r.mapcalc "err_eff_${DATE} = 365 * (if(gates_x@gates_50_2500 == 1, if(isnull(${EX}), 0, abs(${EX}))) + if(gates_y@gates_50_2500, if(isnull(${EY}), 0, abs(${EY}))))"
  done
done


#+END_SRC
#+RESULTS:

*** Just one velocity cutoff & buffer distance 
#+BEGIN_SRC sh :results verbatim
g.mapsets -l

r.mask -r

MAPSET=gates_100_5000

g.mapset MEaSUREs.0478
g.region -d
r.mapcalc "MASK = if((gates_x@${MAPSET} == 1) | (gates_y@${MAPSET} == 1), 1, null())" --o
VX=$(g.list type=raster pattern=VX_????_??_?? | head -n1) # DEBUG
VX=$(g.list type=raster pattern=VX_????_??_?? | grep 2012_03_03) # DEBUG
for VX in $(g.list type=raster pattern=VX_????_??_??); do
  VY=${VX/VX/VY}
  EX=${VX/VX/EX}
  EY=${VX/VX/EY}
  DATE=$(echo $VX | cut -d"_" -f2-)
  echo $DATE
  # g.region raster=${VX}
  r.mapcalc "vel_eff_${DATE} = if(gates_x@${MAPSET} == 1, if(${VX} == -2*10^9, 0, abs(${VX})), 0) + if(gates_y@${MAPSET}, if(${VY} == -2*10^9, 0, abs(${VY})), 0)"
  r.mapcalc "err_eff_${DATE} = if(gates_x@${MAPSET} == 1, if(${EX} == -2*10^9, 0, abs(${EX})), 0) + if(gates_y@${MAPSET}, if(${EY} == -2*10^9, 0, abs(${EY})), 0)"
done


g.mapset MEaSUREs.0646
g.region -d
r.mapcalc "MASK = if((gates_x@${MAPSET} == 1) | (gates_y@${MAPSET} == 1), 1, null())" --o
VX=$(g.list type=raster pattern=VX_????_??_?? | head -n1) # DEBUG
for VX in $(g.list type=raster pattern=VX_????_??_??); do
  VY=${VX/VX/VY}
  EX=${VX/VX/EX}
  EY=${VX/VX/EY}
  DATE=$(echo $VX | cut -d"_" -f2-)
  echo $DATE
  r.mapcalc "vel_eff_${DATE} = if(gates_x@${MAPSET} == 1, if(isnull(${VX}), 0, abs(${VX}))) + if(gates_y@${MAPSET}, if(isnull(${VY}), 0, abs(${VY})))"
  r.mapcalc "err_eff_${DATE} = if(gates_x@${MAPSET} == 1, if(isnull(${EX}), 0, abs(${EX}))) + if(gates_y@${MAPSET}, if(isnull(${EY}), 0, abs(${EY})))"
done


g.mapset MEaSUREs.0731
g.region -d
r.mapcalc "MASK = if((gates_x@${MAPSET} == 1) | (gates_y@${MAPSET} == 1), 1, null())" --o
VX=$(g.list type=raster pattern=VX_????_??_?? | head -n1) # DEBUG
for VX in $(g.list type=raster pattern=VX_????_??_??); do
  VY=${VX/VX/VY}
  EX=${VX/VX/EX}
  EY=${VX/VX/EY}
  DATE=$(echo $VX | cut -d"_" -f2-)
  echo $DATE
  r.mapcalc "vel_eff_${DATE} = if(gates_x@${MAPSET} == 1, if(isnull(${VX}), 0, abs(${VX}))) + if(gates_y@${MAPSET}, if(isnull(${VY}), 0, abs(${VY})))"
  r.mapcalc "err_eff_${DATE} = if(gates_x@${MAPSET} == 1, if(isnull(${EX}), 0, abs(${EX}))) + if(gates_y@${MAPSET}, if(isnull(${EY}), 0, abs(${EY})))"
done


g.mapset Sentinel1
g.region -d
r.mapcalc "MASK = if((gates_x@${MAPSET} == 1) | (gates_y@${MAPSET} == 1), 1, null())" --o
VX=$(g.list type=raster pattern=vx_????_??_?? | head -n1) # DEBUG
for VX in $(g.list type=raster pattern=vx_????_??_??); do
  VY=${VX/vx/vy}
  EX=${VX/vx/ex}
  EY=${VX/vx/ey}
  DATE=$(echo $VX | cut -d"_" -f2-)
  echo $DATE
  r.mapcalc "vel_eff_${DATE} = 365 * (if(gates_x@${MAPSET} == 1, if(isnull(${VX}), 0, abs(${VX}))) + if(gates_y@${MAPSET}, if(isnull(${VY}), 0, abs(${VY}))))"
  r.mapcalc "err_eff_${DATE} = 365 * (if(gates_x@${MAPSET} == 1, if(isnull(${EX}), 0, abs(${EX}))) + if(gates_y@${MAPSET}, if(isnull(${EY}), 0, abs(${EY}))))"
done

g.mapset Mouginot_pre2000
g.region -d
r.mapcalc "MASK = if((gates_x@${MAPSET} == 1) | (gates_y@${MAPSET} == 1), 1, null())" --o
VX=$(g.list type=raster pattern=vx_????_??_?? | head -n1) # DEBUG
for VX in $(g.list type=raster pattern=vx_????_??_??); do
  VY=${VX/vx/vy}
  DATE=$(echo $VX | cut -d"_" -f2-)
  echo $DATE
  r.mapcalc "vel_eff_${DATE} = (if(gates_x@${MAPSET} == 1, if(isnull(${VX}), 0, abs(${VX}))) + if(gates_y@${MAPSET}, if(isnull(${VY}), 0, abs(${VY}))))"
done
#+END_SRC
#+RESULTS:

** Export all data to CSV
:PROPERTIES:
:header-args:sh+: :tangle export.sh
:END:

#+BEGIN_SRC sh :results verbatim
<<MSGS_pretty_print>>
<<GRASS_config>>
#+END_SRC
#+RESULTS:

#+BEGIN_SRC sh :results output
MSG_OK "Exporting..."
g.mapset PERMANENT
g.region -dp

MAPSET=gates_100_5000

VEL_baseline=vel_baseline@MEaSUREs.0478,vx_baseline@MEaSUREs.0478,vy_baseline@MEaSUREs.0478,vel_err_baseline@MEaSUREs.0478,ex_baseline@MEaSUREs.0478,ey_baseline@MEaSUREs.0478
VEL_0478=$(g.list -m mapset=MEaSUREs.0478 type=raster pattern=vel_eff_????_??_?? separator=,)
ERR_0478=$(g.list -m mapset=MEaSUREs.0478 type=raster pattern=err_eff_????_??_?? separator=,)
VEL_0646=$(g.list -m mapset=MEaSUREs.0646 type=raster pattern=vel_eff_????_??_?? separator=,)
ERR_0646=$(g.list -m mapset=MEaSUREs.0646 type=raster pattern=err_eff_????_??_?? separator=,)
VEL_0731=$(g.list -m mapset=MEaSUREs.0731 type=raster pattern=vel_eff_????_??_?? separator=,)
ERR_0731=$(g.list -m mapset=MEaSUREs.0731 type=raster pattern=err_eff_????_??_?? separator=,)
VEL_SENTINEL=$(g.list -m mapset=Sentinel1 type=raster pattern=vel_eff_????_??_?? separator=,)
ERR_SENTINEL=$(g.list -m mapset=Sentinel1 type=raster pattern=err_eff_????_??_?? separator=,)
VEL_MOUGINOT=$(g.list -m mapset=Mouginot_pre2000 type=raster pattern=vel_eff_????_??_?? separator=,)
THICK=$(g.list -m mapset=Khan_2016 type=raster pattern=dh_???? separator=,)
GIMP_0715=dem@GIMP.0715,day@GIMP.0715 # ,err@GIMP.0715

LIST=lon,lat,err_2D,gates_x@${MAPSET},gates_y@${MAPSET},gates_gateID@${MAPSET},sectors@Mouginot_2019,regions@Mouginot_2019,bed@BedMachine,thickness@BedMachine,surface@BedMachine,thickness@Bamber_2013,bed_0@Millan_2018,thickness_0@Millan_2018,${THICK},${GIMP_0715},${VEL_baseline},${VEL_0478},${VEL_0646},${VEL_0731},${VEL_SENTINEL},${VEL_MOUGINOT},errbed@BedMachine,${ERR_0478},${ERR_0646},${ERR_0731},${ERR_SENTINEL}

r.mask gates_final@${MAPSET} --o

# test
# for v in $(echo $LIST | tr ',' '\n'); do n=$(r.univar $v|grep "^n:"); echo ${v}: ${n}; done

date
MSG_WARN "Exporting: $(echo $LIST|tr ',' '\n' |wc -l) columns"
ulimit -n 2048
time (echo x,y,${LIST}; r.out.xyz input=${LIST} separator=comma) > ./tmp/dat_100_5000.csv
r.mask -r
#+END_SRC
#+RESULTS:
*** Testing
#+BEGIN_SRC sh :results verbatim :tangle no
g.mapset PERMANENT
g.region -dp
MAPSET=gates_100_5000

VEL_baseline=vel_baseline@MEaSUREs.0478,vx_baseline@MEaSUREs.0478,vy_baseline@MEaSUREs.0478,vel_err_baseline@MEaSUREs.0478,ex_baseline@MEaSUREs.0478,ey_baseline@MEaSUREs.0478
VEL_0478=$(g.list -m mapset=MEaSUREs.0478 type=raster pattern=vel_eff_????_??_?? separator=,)
ERR_0478=$(g.list -m mapset=MEaSUREs.0478 type=raster pattern=err_eff_????_??_?? separator=,)
VEL_0646=$(g.list -m mapset=MEaSUREs.0646 type=raster pattern=vel_eff_????_??_?? separator=,)
ERR_0646=$(g.list -m mapset=MEaSUREs.0646 type=raster pattern=err_eff_????_??_?? separator=,)
VEL_SENTINEL=$(g.list -m mapset=Sentinel1 type=raster pattern=vel_eff_????_??_?? separator=,)
ERR_SENTINEL=$(g.list -m mapset=Sentinel1 type=raster pattern=err_eff_????_??_?? separator=,)
VEL_MOUGINOT=$(g.list -m mapset=Mouginot_pre2000 type=raster pattern=vel_eff_????_??_?? separator=,)
THICK=$(g.list -m mapset=Khan_2016 type=raster pattern=dh_???? separator=,)
GIMP_0715=dem@GIMP.0715,day@GIMP.0715 # ,err@GIMP.0715

LIST=lon,lat,err_2D,gates_x@${MAPSET},gates_y@${MAPSET},gates_gateID@${MAPSET},sectors@Mouginot_2019,regions@Mouginot_2019,bed@BedMachine,thickness@BedMachine,surface@BedMachine,thickness@Bamber_2013,bed_0@Millan_2018,thickness_0@Millan_2018,${THICK},${GIMP_0715},${VEL_baseline},${VEL_0478},${VEL_0646},${VEL_SENTINEL},${VEL_MOUGINOT},errbed@BedMachine,${ERR_0478},${ERR_0646},${ERR_SENTINEL}

r.mask gates_final@${MAPSET} --o

# test
# for v in $(echo $LIST | tr ',' '\n'); do n=$(r.univar $v|grep "^n:"); echo ${v}: ${n}; done

date
MSG_WARN "Exporting: $(echo $LIST|tr ',' '\n' |wc -l) columns"
ulimit -n 2048
time (echo x,y,${LIST}; r.out.xyz input=${LIST} separator=comma) > ./tmp/dat_100_5000.csv
r.mask -r

#+END_SRC
#+RESULTS:

** Compute Errors
:PROPERTIES:
:header-args:jupyter-python+: :tangle errors.py
:END:

*** Velocity v Thickness Errors
+ Is velocity uncertainty important relative to thickness uncertainty?
  + ANS: No, proportional velocity uncertainty is an order of magnitude less than thickness uncertainty.
#+BEGIN_SRC jupyter-python :results raw drawer :session discharge :display text/plain :tangle no
import pandas as pd
import numpy as np

df = pd.read_csv("./tmp/dat_100_5000.csv")

thick = df['thickness@BedMachine']
thick[thick < 50] = 50  # IS THIS REASONABLE? IMPORTANT?
thick[thick == 0] = 1
thick_err = np.abs(df['errbed@BedMachine'].values)


vel = df['vel_baseline@MEaSUREs.0478']
vel_err = df['vel_err_baseline@MEaSUREs.0478']

print("Relative thickness error:")
print((thick_err/thick*100).describe())
print("\n\nRelative velocity error:")
print((vel_err/vel*100).describe())

# import matplotlib.pyplot as plt
# plt.clf()
# ax = plt.hexbin((thick_err/thick), (vel_err/vel), mincnt=1, bins='log', xscale='log', yscale='log')
# plt.xlabel('Relative Thickness Error [Log_10]')
# plt.ylabel('Relative Velocity Error [Log_10]')
#+END_SRC

#+RESULTS:
#+begin_example
Relative thickness error:
count    6002.000000
mean       53.964693
std       106.454983
min         1.104972
25%         6.041824
50%        11.801709
75%        29.629630
max       598.000000
Name: thickness@BedMachine, dtype: float64


Relative velocity error:
count    6002.000000
mean        4.259675
std         9.464333
min         0.018398
25%         0.545081
50%         1.294552
75%         4.029445
max       166.474276
dtype: float64
/home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  import sys
/home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
#+end_example


*** Do velocity errors scale w/ velocity?
+ ANS: Not really.
#+BEGIN_SRC jupyter-python :results raw drawer :session discharge :display text/plain :tangle no
import pandas as pd
import numpy as np

# df = pd.read_csv("./tmp/dat_100_5000.csv")
# vel = df['vel_baseline@MEaSUREs.0478']
# vel_err = df['vel_err_baseline@MEaSUREs.0478']


vel = pd.read_csv("./tmp/dat_100_5000.csv", usecols=(lambda c: ('vel_eff' in c)))
vel.rename(columns=lambda c: pd.datetime(int(c[8:12]), int(c[13:15]), int(c[16:18])), inplace=True)
vel.replace(0, np.nan, inplace=True)
# vel = vel.loc[:,vel.columns.year < 2018] # drop 2018
vel.sort_index(axis='columns', inplace=True)

vel_err = pd.read_csv("./tmp/dat_100_5000.csv", usecols=(lambda c: ('err_eff' in c)))
vel_err.rename(columns=lambda c: pd.datetime(int(c[8:12]), int(c[13:15]), int(c[16:18])), inplace=True)
# vel_err = vel_err.loc[:,vel_err.columns.year < 2018] # drop 2018
for c in vel.columns:
    if c not in vel_err.columns:
        vel_err[c] = 0 
vel_err.sort_index(axis='columns', inplace=True) 


import matplotlib.pyplot as plt
plt.clf()
ax = plt.hexbin(vel.values.flatten()+1,
                vel_err.values.flatten()+1,
                mincnt=1,
                bins='log',
                xscale='log', yscale='log',
                gridsize=100)
plt.xlabel("Velocity [m yr$^{-1}$]")
plt.ylabel("Velocity Error [m yr$^{-1}$]")
#+END_SRC

*** Results (GIS)
#+BEGIN_SRC jupyter-python :results raw drawer :session discharge :display text/org :tangle no
from uncertainties import unumpy
import pandas as pd
import numpy as np

df_out = pd.DataFrame(columns=['Val','Unit'])
df_out.index.name = 'Method'

df = pd.read_csv("./tmp/dat_100_5000.csv")

thick = df['thickness@BedMachine']
# thick[thick < 50] = 50  # IS THIS REASONABLE? IMPORTANT?
# vel = df['vel_baseline@MEaSUREs.0478']
vel = np.abs(df['vx_baseline@MEaSUREs.0478'])*df['gates_x@gates_100_5000'] + np.abs(df['vy_baseline@MEaSUREs.0478'])*df['gates_y@gates_100_5000']
D = 200  * thick * vel * 917 / 1E12
df_out.loc['D'] = [np.sum(D), 'Gt']

err_thick = np.abs(df['errbed@BedMachine'].values)
# err_vel = np.abs(df['vel_err_baseline@MEaSUREs.0478'])
err_vel = np.abs(df['ex_baseline@MEaSUREs.0478'])*df['gates_x@gates_100_5000'] + np.abs(df['ey_baseline@MEaSUREs.0478'])*df['gates_y@gates_100_5000']

e_th = 200 * err_thick * vel * 917 / 1E12
df_out.loc['Err (Thickness)'] = [np.sum(e_th), 'Gt']
df_out.loc['Err (Thickness %)'] = [np.sum(e_th)/np.sum(D)*100, '%']

e_vel = 200 * thick * err_vel * 917 / 1E12
df_out.loc['Err (Velocity)'] = [np.sum(e_vel), 'Gt']
df_out.loc['Err (Velocity %)'] = [np.sum(e_vel)/np.sum(D)*100, '%']
df_out.loc['Err (Combined)'] = [np.sum(e_vel+e_th), 'Gt']
df_out.loc['Err (%)'] = [np.sum(e_vel+e_th)/np.sum(D)*100, '%']
# The above assumes everything is systematic/independent/correlated

# If errors are all random and uncorrelated:
t = unumpy.uarray(thick, err_thick)
v = unumpy.uarray(vel, err_vel)
e = np.sum(200 * t * v * 917 / 1E12)
df_out.loc['Random Errors'] = [e, 'Gt']
df_out.loc['Random Errors (%)'] = [e.s/e.n*100, '%']

df_out
#+END_SRC

#+RESULTS:
:RESULTS:
| Method            | Val                 | Unit   |
|-------------------+---------------------+--------|
| D                 | 471.3101008797073   | Gt     |
| Err (Thickness)   | 49.18553057313633   | Gt     |
| Err (Thickness %) | 10.435916922071224  | %      |
| Err (Velocity)    | 3.307065561689056   | Gt     |
| Err (Velocity %)  | 0.7016750872761626  | %      |
| Err (Combined)    | 52.49259613482539   | Gt     |
| Err (%)           | 11.137592009347387  | %      |
| Random Errors     | 471.3+/-1.3         | Gt     |
| Random Errors (%) | 0.27298289176587875 | %      |
:END:

*** Results (Mouginot 2019 Sector)
#+BEGIN_SRC jupyter-python :results raw drawer :session discharge :display text/org
from uncertainties import unumpy
import pandas as pd
import numpy as np

df = pd.read_csv("./tmp/dat_100_5000.csv")

err_sector = pd.DataFrame(columns=['D', 'E', 'E%'])
err_sector.index.name = 'Sector'

sectors = np.unique(df['sectors@Mouginot_2019'].values)
for s in sectors:
    sub = df[df['sectors@Mouginot_2019'] == s]
    thick = sub['thickness@BedMachine']
    vel = sub['vel_baseline@MEaSUREs.0478']
    # vel = np.abs(sub['vx_baseline@MEaSUREs.0478'])*sub['gates_x@gates_100_5000'] + np.abs(sub['vy_baseline@MEaSUREs.0478'])*sub['gates_y@gates_100_5000']
    D = 200  * thick * vel * 917 / 1E12
    err_thick = np.abs(sub['errbed@BedMachine'].values)
    # err_thick[np.where(err_thick < 50)] = 50  # IS THIS REASONABLE? IMPORTANT?
    e_th = 200 * err_thick * vel * 917 / 1E12
    err_sector.loc[s] = [np.sum(D), np.sum(e_th), np.round(np.sum(e_th),10)/np.round(np.sum(D),10)*100]

err_sector.loc['GIS'] = np.sum(err_sector, axis=0)
err_sector.loc['GIS']['E%'] = err_sector.loc['GIS']['E']/err_sector.loc['GIS']['D']*100

err_sector.to_csv('./tmp/err_sector_mouginot.csv')

err_sector.rename(columns = {'D':'D [Gt]',
                             'E':'Error [Gt]',
                             'E%':'Error [%]'}, inplace=True)

err_sector
#+END_SRC

#+RESULTS:
:RESULTS:
| Sector   |        D [Gt] |   Error [Gt] |   Error [%] |
|----------+---------------+--------------+-------------|
| 1        |   1.13296     |   0.119511   |    10.5486  |
| 2        |   0.259001    |   0.109302   |    42.2015  |
| 3        |  13.9562      |   0.900377   |     6.45147 |
| 4        |   1.81572     |   0.268763   |    14.802   |
| 6        |   6.29835     |   0.47056    |     7.47117 |
| 7        |   0.759871    |   0.0538301  |     7.08411 |
| 8        |   0.643772    |   0.0480392  |     7.46214 |
| 9        |   8.93733     |   0.687491   |     7.69236 |
| 10       |   1.9844      |   0.151168   |     7.61786 |
| 14       |   2.93346     |   0.299862   |    10.2221  |
| 15       |   0.450661    |   0.0507318  |    11.2572  |
| 16       |   4.98386     |   0.521496   |    10.4637  |
| 17       |   0.122299    |   0.167712   |   137.133   |
| 19       |   0.117655    |   0.0335031  |    28.4758  |
| 20       |   0.448198    |   0.0389468  |     8.68963 |
| 21       |   0.826369    |   0.0878104  |    10.6261  |
| 22       |   0.805537    |   0.137715   |    17.096   |
| 23       |   1.16484     |   0.115224   |     9.89176 |
| 25       |   0.0965041   |   0.0129959  |    13.4666  |
| 26       |   1.01834     |   0.059183   |     5.81173 |
| 27       |   5.78016     |   0.247551   |     4.28277 |
| 28       |   0.625812    |   0.0235983  |     3.77083 |
| 29       |   1.90508     |   0.120741   |     6.33787 |
| 30       |   1.74007     |   0.140262   |     8.06072 |
| 31       |   0.723047    |   0.0591468  |     8.18022 |
| 32       |   5.25837     |   0.378481   |     7.19768 |
| 33       |   7.24029     |   0.8001     |    11.0507  |
| 34       |   4.43932     |   0.397848   |     8.9619  |
| 35       |   8.90903     |   0.995605   |    11.1752  |
| 36       |   4.35454     |   0.381813   |     8.76817 |
| 37       |   5.9467      |   0.315895   |     5.31211 |
| 38       |   4.80213     |   0.538872   |    11.2215  |
| 41       |   2.36772     |   0.196327   |     8.29182 |
| 42       |   1.01938     |   0.0670365  |     6.57618 |
| 43       |   6.58061     |   0.266743   |     4.05347 |
| 44       |   1.00854     |   0.0598935  |     5.93862 |
| 45       |   0.802593    |   0.0552257  |     6.8809  |
| 47       |   1.55381     |   0.191847   |    12.3469  |
| 48       |   2.40729     |   0.338863   |    14.0765  |
| 49       |   0.765042    |   0.0350007  |     4.575   |
| 50       |   5.69884     |   0.304978   |     5.35159 |
| 53       |   3.06926     |   0.162114   |     5.28187 |
| 55       |   0.129045    |   0.0216342  |    16.7648  |
| 56       |   0.416233    |   0.0805611  |    19.3548  |
| 58       |  12.4341      |   0.652429   |     5.24708 |
| 59       |  13.2393      |   1.28662    |     9.71819 |
| 60       |   8.7061      |   0.619988   |     7.12131 |
| 61       |   0.575287    |   0.0981973  |    17.0693  |
| 62       |   0.690076    |   0.0283109  |     4.10257 |
| 63       |  28.2323      |   2.11523    |     7.49223 |
| 64       |   5.28086     |   0.470461   |     8.90879 |
| 65       |   4.27645     |   0.153325   |     3.58533 |
| 66       |   0.000223683 |   0.0378539  | 16923       |
| 67       |   0.00495062  |   0.629357   | 12712.7     |
| 68       |   3.60124     |   0.558295   |    15.5029  |
| 69       |   1.89908     |   0.182147   |     9.59135 |
| 72       |   1.39015     |   0.114702   |     8.25104 |
| 73       |   0.00306625  |   0.550745   | 17961.5     |
| 74       |   4.70061     |   0.564929   |    12.0182  |
| 75       |   0.112515    |   0.0317329  |    28.2032  |
| 76       |   1.66594     |   0.28158    |    16.9022  |
| 77       |   0.000443706 |   0.060892   | 13723.5     |
| 78       |   1.95408     |   0.132811   |     6.79659 |
| 80       |   8.0312      |   0.668299   |     8.32129 |
| 81       |   7.30356     |   0.624366   |     8.54879 |
| 82       |   1.46381     |   0.0909655  |     6.2143  |
| 83       |   4.32513     |   0.598528   |    13.8384  |
| 84       |   1.57504     |   0.253256   |    16.0794  |
| 85       |   0.0574275   |   0.0119868  |    20.8729  |
| 86       |   7.17275     |   0.75337    |    10.5032  |
| 88       |   0.0540345   |   0.290302   |   537.253   |
| 93       |   2.72698     |   0.220433   |     8.08342 |
| 94       |   1.19425     |   0.349332   |    29.2511  |
| 95       |  10.7311      |   0.294127   |     2.74088 |
| 96       |   5.27952     |   0.905431   |    17.1499  |
| 97       |   0.00490425  |   0.0141609  |   288.746   |
| 98       |   1.46695     |   0.233396   |    15.9102  |
| 100      |   0.248149    |   0.164958   |    66.4752  |
| 102      |  21.8453      |   1.1401     |     5.21898 |
| 103      |   0.362388    |   0.038987   |    10.7584  |
| 104      |   0.593113    |   0.0901735  |    15.2034  |
| 105      |   0.0986737   |   0.0174753  |    17.7102  |
| 106      |  14.603       |   1.02355    |     7.00916 |
| 107      |   4.2366      |   0.354421   |     8.36571 |
| 108      |   0.494752    |   0.166467   |    33.6466  |
| 109      |   0.045015    |   0.0362538  |    80.5373  |
| 110      |   0.373132    |   0.0188092  |     5.0409  |
| 111      |   0.00113236  |   0.11948    | 10551.4     |
| 112      |   6.05528e-05 |   0.00906305 | 14967.2     |
| 113      |   0.0787378   |   0.396151   |   503.127   |
| 114      |   1.64988     |   0.164249   |     9.95517 |
| 115      |   0.977219    |   0.228715   |    23.4047  |
| 116      |   0.000509676 |   0.0818634  | 16061.8     |
| 117      |   4.31234     |   0.4248     |     9.8508  |
| 118      |   0.00135411  |   0.257555   | 19020.3     |
| 120      |   0.431738    |   0.125198   |    28.9985  |
| 121      |   1.54498     |   0.35171    |    22.7647  |
| 122      |   0.00274319  |   0.462101   | 16845.3     |
| 123      |   0.000521908 |   0.0460695  |  8827.13    |
| 124      |   0.00228764  |   0.324143   | 14169.3     |
| 125      |   1.27373     |   0.105741   |     8.30173 |
| 126      |   1.60821     |   0.247405   |    15.3839  |
| 127      |   9.07181     |   0.456823   |     5.03563 |
| 128      |   0.00255097  |   0.403067   | 15800.5     |
| 132      |   0.193615    |   0.150296   |    77.6262  |
| 134      |   1.7019      |   1.01124    |    59.4183  |
| 136      |   0.000397048 |   0.099331   | 25017.4     |
| 138      |   0.0378695   |   0.0237563  |    62.7321  |
| 139      |   0.0983967   |   0.0360802  |    36.6681  |
| 140      |   0.300546    |   0.141292   |    47.0119  |
| 141      |   0.0553563   |   0.0241199  |    43.572   |
| 142      |   0.0668502   |   0.0335327  |    50.161   |
| 144      |   0.000758416 |   0.00862726 |  1137.54    |
| 146      |   0.278089    |   0.0226055  |     8.12888 |
| 147      |   2.19185     |   0.234455   |    10.6967  |
| 148      |   1.27653     |   0.0910306  |     7.13111 |
| 150      |   0.0880221   |   0.0304034  |    34.5406  |
| 151      |   0.500292    |   0.100487   |    20.0857  |
| 152      |   0.146082    |   0.0533827  |    36.5429  |
| 153      |   0.263117    |   0.108511   |    41.2405  |
| 154      |   0.144576    |   0.0701505  |    48.5217  |
| 156      |   0.00348091  |   0.10433    |  2997.21    |
| 157      |   0.0684098   |   0.0342677  |    50.0919  |
| 158      |   0.153157    |   0.0322337  |    21.0462  |
| 164      |   0.0253565   |   0.0459476  |   181.207   |
| 172      |   0.00717637  |   0.0260229  |   362.62    |
| 174      |   0.000181888 |   0.0214308  | 11782.4     |
| 183      |   0.342336    |   0.0593268  |    17.33    |
| 185      |   0.528683    |   0.0885119  |    16.742   |
| 186      |   0.694426    |   0.534415   |    76.9578  |
| 187      |   0.0037427   |   0.0449016  |  1199.71    |
| 188      |   0.00974053  |   0.0670818  |   688.687   |
| 189      |   7.33242     |   0.401675   |     5.47807 |
| 190      |   4.68559     |   1.7709     |    37.7946  |
| 191      |   0.00025485  |   0.0533937  | 20951       |
| 192      |   4.25385     |   0.243481   |     5.72377 |
| 193      |   2.08273     |   0.118326   |     5.68128 |
| 195      |   0.267033    |   0.0566183  |    21.2027  |
| 196      |   0.0144472   |   0.00938492 |    64.96    |
| 197      |   0.700724    |   0.119247   |    17.0177  |
| 199      |   0.458465    |   0.560215   |   122.194   |
| 200      |   0.00278076  |   0.0434554  |  1562.72    |
| 203      |   0.824135    |   0.0919874  |    11.1617  |
| 204      |   0.00338184  |   0.0175625  |   519.317   |
| 207      |   6.69919     |   0.830237   |    12.3931  |
| 208      |   2.84014     |   0.226375   |     7.97057 |
| 209      |   0.834729    |   0.134416   |    16.103   |
| 210      |   0.681866    |   0.130944   |    19.2038  |
| 211      |   2.86473     |   0.287411   |    10.0327  |
| 212      |   5.7736      |   0.430977   |     7.46462 |
| 213      |   4.62881     |   0.489463   |    10.5743  |
| 214      |   8.9976      |   0.919488   |    10.2193  |
| 215      |   0.0391676   |   0.073413   |   187.433   |
| 216      |   0.583004    |   0.0874796  |    15.005   |
| 218      |  51.2607      |   4.69356    |     9.15625 |
| 219      |   0.189514    |   0.0455555  |    24.0381  |
| 222      |   0.0506244   |   0.0257404  |    50.8459  |
| 223      |  11.2588      |   0.472191   |     4.19398 |
| 224      |   0.494333    |   0.256009   |    51.7889  |
| 231      |   0.874542    |   0.110348   |    12.6178  |
| 232      |   0.000323121 |   0.0677302  | 20961.3     |
| 233      |   4.46378     |   0.328824   |     7.3665  |
| 234      |   0.00192408  |   0.00125117 |    65.027   |
| 236      |   0.000371668 |   0.0692465  | 18631.3     |
| 237      |   0.145481    |   0.36019    |   247.585   |
| 239      |   0.0939576   |   0.0281907  |    30.0036  |
| 240      |   0.103579    |   0.024599   |    23.749   |
| 243      |   1.61214     |   0.225554   |    13.9909  |
| 248      |   5.33103     |   0.233721   |     4.38416 |
| 249      |   1.14162     |   0.12377    |    10.8416  |
| 251      |   0.0179124   |   0.00989957 |    55.2667  |
| 252      |   0.00405105  |   0.00457811 |   113.01    |
| 254      |   0.0793327   |   0.0381313  |    48.0651  |
| 255      |   0.900182    |   0.0940261  |    10.4452  |
| 257      |   0.000594161 |   0.0912149  | 15351.9     |
| 259      |   0.000308847 |   0.0471869  | 15278.4     |
| GIS      | 472.086       |  49.393      |    10.4627  |
:END:

*** Results (Mouginot 2019 Region)
#+BEGIN_SRC jupyter-python :results raw drawer :session discharge :display text/org
from uncertainties import unumpy
import pandas as pd
import numpy as np

df = pd.read_csv("./tmp/dat_100_5000.csv")

err_sector = pd.DataFrame(columns=['D','E', 'E%'])
err_sector.index.name = 'Sector'

sectors = np.unique(df['regions@Mouginot_2019'].values)
for s in sectors:
   sub = df[df['regions@Mouginot_2019'] == s]
   thick = sub['thickness@BedMachine']
   vel = np.abs(sub['vx_baseline@MEaSUREs.0478'])*sub['gates_x@gates_100_5000'] + np.abs(sub['vy_baseline@MEaSUREs.0478'])*sub['gates_y@gates_100_5000']
   D = 200  * thick * vel * 917 / 1E12
   err_thick = np.abs(sub['errbed@BedMachine'].values)
   # err_thick[np.where(err_thick < 50)] = 50  # IS THIS REASONABLE? IMPORTANT?
   e_th = 200 * err_thick * vel * 917 / 1E12
   err_sector.loc[s] = [np.sum(D), np.sum(e_th), np.round(np.sum(e_th),10)/np.round(np.sum(D),10)*100]

err_sector.loc['GIS'] = np.sum(err_sector, axis=0)
err_sector.loc['GIS']['E%'] = err_sector.loc['GIS']['E']/err_sector.loc['GIS']['D']*100

err_sector.to_csv('./tmp/err_region_mouginot.csv')

err_sector.rename(columns = {'D':'D [Gt]', 
                         'E':'Error [Gt]',
                         'E%':'Error [%]'}, inplace=True)

err_sector
#+END_SRC

#+RESULTS:
:results:
# Out[19]:
# text/org
| Sector   |   D [Gt] |   Error [Gt] |   Error [%] |
|----------+----------+--------------+-------------|
| 1        |  24.1364 |      1.80701 |     7.48665 |
| 2        |  27.0622 |      2.30435 |     8.51501 |
| 3        |  66.6954 |      6.47462 |     9.70775 |
| 4        | 135.024  |     17.6865  |    13.0988  |
| 5        |  14.4604 |      2.95814 |    20.4568  |
| 6        |  88.5004 |      7.62999 |     8.62142 |
| 7        | 115.431  |     10.3249  |     8.9446  |
| GIS      | 471.31   |     49.1855  |    10.4359  |
:end:

*** Results (Gate)                                                 :noexport:
#+BEGIN_SRC jupyter-python :results raw drawer :session discharge :display text/org
from uncertainties import unumpy
import pandas as pd
import numpy as np

df = pd.read_csv("./tmp/dat_100_5000.csv")

err_gate = pd.DataFrame(columns=['D','E', 'E%'])
err_gate.index.name = 'Gate'

gates = np.unique(df['gates_gateID@gates_100_5000'].values)
for g in gates:
    sub = df[df['gates_gateID@gates_100_5000'] == g]
    thick = sub['thickness@BedMachine']
    vel = np.abs(sub['vx_baseline@MEaSUREs.0478'])*sub['gates_x@gates_100_5000'] + np.abs(sub['vy_baseline@MEaSUREs.0478'])*sub['gates_y@gates_100_5000']
    D = 200  * thick * vel * 917 / 1E12
    err_thick = np.abs(sub['errbed@BedMachine'].values)
    # err_thick[np.where(err_thick < 50)] = 50  # IS THIS REASONABLE? IMPORTANT?
    e_th = 200 * err_thick * vel * 917 / 1E12
    err_gate.loc[g] = [np.sum(D), np.sum(e_th), np.sum(e_th)/np.sum(D)*100]

err_gate.loc['GIS'] = np.sum(err_gate, axis=0)
err_gate.loc['GIS']['E%'] = err_gate.loc['GIS']['E']/err_gate.loc['GIS']['D']*100

gate_meta = pd.read_csv("./out/gate_meta.csv")
err_gate['name'] = ''
for g in err_gate.index.values:
    if (g == 'GIS'): continue
    if (sum(gate_meta.gate == g) == 0): continue
    err_gate.loc[g,'name'] = gate_meta[gate_meta.gate == g].Mouginot_2019.values[0]

err_gate.to_csv('./tmp/err_gate.csv')
err_gate.rename(columns = {'D':'D [Gt]', 
                           'E':'Error [Gt]',
                           'E%':'Error [%]'}, inplace=True),

err_gate
#+END_SRC

#+RESULTS:
:results:
# Out[38]:
# text/org
| Gate   |        D [Gt] |   Error [Gt] |   Error [%] | name                             |
|--------+---------------+--------------+-------------+----------------------------------|
| 1      |   0.000242779 |   0.030608   | 12607.4     | JUNGERSEN_HENSON_NARAVANA        |
| 3      |   0.00010155  |   0.0153045  | 15071       | JUNGERSEN_HENSON_NARAVANA        |
| 4      |   0.123399    |   0.0207551  |    16.8195  | MARIE_SOPHIE_GLETSCHER           |
| 5      |   0.570673    |   0.0769139  |    13.4778  | HAGEN_BRAE                       |
| 6      |   0.0308782   |   0.0581098  |   188.191   | BRIKKERNE_GLETSCHER              |
| 7      |   0.430717    |   0.08547    |    19.8437  | ACADEMY                          |
| 8      |   0.0445647   |   0.0359111  |    80.5819  | HARDER_GLETSCHER                 |
| 10     |   2.56798     |   0.10477    |     4.07984 | RYDER_GLETSCHER                  |
| 11     |   1.56763     |   0.15128    |     9.65019 | OSTENFELD_GLETSCHER              |
| 12     |   0.306089    |   0.0243717  |     7.96228 | STEENSBY_GLETSCHER               |
| 17     |   0.0463436   |   0.0407937  |    88.0245  | OSTENFELD_GLETSCHER              |
| 19     |   0.0449537   |   0.0489306  |   108.847   | OSTENFELD_GLETSCHER              |
| 22     |   0.208659    |   0.0173091  |     8.2954  | RYDER_GLETSCHER                  |
| 23     |   0.277659    |   0.0390611  |    14.068   | RYDER_GLETSCHER                  |
| 25     |   0.0256916   |   0.0819198  |   318.858   | PETERMANN_GLETSCHER_N            |
| 26     |   0.489938    |   0.169616   |    34.6199  | PETERMANN_GLETSCHER_N            |
| 27     |  11.3084      |   0.48615    |     4.299   | PETERMANN_GLETSCHER              |
| 28     |   5.81144     |   0.308143   |     5.30235 | HUMBOLDT_GLETSCHER               |
| 29     |   0.0965288   |   0.00284121 |     2.94338 | HUMBOLDT_GLETSCHER               |
| 30     |  12.2803      |   0.662151   |     5.39196 | NIOGHALVFJERDSFJORDEN            |
| 31     |   0.184463    |   0.00874956 |     4.74326 | HUMBOLDT_GLETSCHER               |
| 32     |  14.2076      |   1.37361    |     9.6681  | ZACHARIAE_ISSTROM                |
| 33     |   7.399e-05   |   0.00760604 | 10279.8     | DIEBITSCH                        |
| 34     |   0.000111118 |   0.0141656  | 12748.2     | DIEBITSCH                        |
| 35     |   0.00730055  |   0.0264451  |   362.235   | MORRIS_JESUP                     |
| 36     |   0.0273209   |   0.0508885  |   186.262   | BOWDOIN                          |
| 37     |   0.75569     |   0.115062   |    15.226   | MELVILLE_GLETSCHER               |
| 38     |   0.106608    |   0.0189026  |    17.7309  | SHARP                            |
| 39     |   0.484742    |   0.0523378  |    10.797   | FARQUHAR_GLETSCHER               |
| 40     |   2.59612     |   0.360908   |    13.9018  | TRACY_GLETSCHER                  |
| 41     |   1.61629     |   0.202593   |    12.5344  | HEILPRIN_GLETSCHER               |
| 48     |   0.887973    |   0.0415969  |     4.68448 | HARALD_MOLTKE_BRAE               |
| 49     |   0.0115047   |   0.00239869 |    20.8496  | SAVISSUAQ_WW                     |
| 50     |   0.0957547   |   0.0231554  |    24.1819  | SAVISSUAQ_WW                     |
| 51     |   0.391447    |   0.0693461  |    17.7153  | SAVISSUAQ_WWWW                   |
| 52     |   0.0844815   |   0.0232816  |    27.5582  | SAVISSUAQ_W                      |
| 53     |   0.0264578   |   0.00951808 |    35.9746  | SAVISSUAQ_W                      |
| 54     |   0.555191    |   0.0914293  |    16.4681  | SAVISSUAQ                        |
| 55     |   0.263227    |   0.0557872  |    21.1936  | YNGVAR_NIELSEN_BRAE_W            |
| 56     |   0.68171     |   0.113537   |    16.6548  | YNGVAR_NIELSEN_BRAE              |
| 57     |   0.835121    |   0.0533348  |     6.38648 | CARLOS                           |
| 59     |   0.00382726  |   0.00519325 |   135.691   | CARLOS                           |
| 60     |   0.0144374   |   0.00938091 |    64.9765  | HELLAND                          |
| 61     |   2.37829     |   0.241055   |    10.1357  | GADE-MORELL                      |
| 62     |   0.0433492   |   0.025323   |    58.4162  | GADE-MORELL                      |
| 63     |   0.00955888  |   0.00246878 |    25.8271  | DOCKER_SMITH_GLETSCHER_W         |
| 64     |   1.41693     |   0.0933727  |     6.58978 | DOCKER_SMITH_GLETSCHER_W         |
| 65     |   1.61348     |   0.108635   |     6.73294 | DOCKER_SMITH_GLETSCHER           |
| 66     |   0.257332    |   0.019505   |     7.57969 | DOCKER_SMITH_GLETSCHER           |
| 68     |   1.08707     |   0.064738   |     5.95526 | RINK_GLETSCHER                   |
| 69     |   3.48082     |   0.177199   |     5.09073 | ISSUUARSUIT_SERMIA               |
| 71     |   1.0023      |   0.0280765  |     2.8012  | ISSUUARSUIT_SERMIA               |
| 72     |   1.05992     |   0.0392134  |     3.69965 | ISSUUARSUIT_SERMIA               |
| 73     |   1.09225     |   0.0727899  |     6.66423 | NONAME_NORTH_OSCAR               |
| 74     |   8.37833     |   0.428752   |     5.1174  | KONG_OSCAR_GLETSCHER             |
| 78     |   0.0425836   |   0.0024955  |     5.86023 | KONG_OSCAR_GLETSCHER             |
| 79     |   0.0623227   |   0.0026047  |     4.17937 | NORDENSKIOLD_GLESCHER_NW         |
| 80     |   7.13103     |   0.41994    |     5.88891 | NORDENSKIOLD_GLESCHER_NW         |
| 89     |   0.486803    |   0.042913   |     8.81527 | NANSEN_GLETSCHER                 |
| 90     |   0.072116    |   0.0139877  |    19.3961  | SVERDRUP_GLETSCHER               |
| 91     |   5.36468     |   0.440891   |     8.21841 | SVERDRUP_GLETSCHER               |
| 92     |   1.2727      |   0.156588   |    12.3036  | STEENSTRUP-DIETRICHSON           |
| 93     |   1.06929     |   0.100174   |     9.36822 | STEENSTRUP-DIETRICHSON           |
| 94     |   3.29302     |   0.370044   |    11.2372  | STEENSTRUP-DIETRICHSON           |
| 95     |   6.58815     |   0.771847   |    11.7157  | KJER_GLETSCHER                   |
| 96     |   0.234172    |   0.0336786  |    14.382   | HAYES_GLETSCHER_N_NN             |
| 98     |   0.154598    |   0.0161649  |    10.456   | HAYES_GLETSCHER_N_NN             |
| 99     |   0.583881    |   0.0318647  |     5.45739 | HAYES_GLETSCHER_N_NN             |
| 101    |   0.764497    |   0.0387761  |     5.07211 | HAYES_GLETSCHER_N_NN             |
| 102    |   7.80946     |   0.861176   |    11.0273  | HAYES_GLETSCHER_N_NN             |
| 103    |   3.27963     |   0.300765   |     9.1707  | HAYES_GLETSCHER_M_SS             |
| 111    |   0.550768    |   0.0386663  |     7.02044 | HAYES_GLETSCHER_M_SS             |
| 112    |   0.143022    |   0.0130102  |     9.09666 | HAYES_GLETSCHER_M_SS             |
| 113    |   5.67861     |   0.621164   |    10.9387  | ALISON_GLETSCHER                 |
| 114    |   0.111175    |   0.0403181  |    36.2655  | WALTERSHAUSEN                    |
| 115    |   1.50209     |   0.163485   |    10.8839  | ALISON_GLETSCHER                 |
| 116    |   0.0328206   |   0.0206552  |    62.9336  | ADOLF_HOEL                       |
| 117    |   5.21698     |   0.374311   |     7.17486 | ILLULLIP_SERMIA                  |
| 118    |   0.456206    |   0.0409648  |     8.97946 | CORNELL_GLETSCHER                |
| 119    |   0.238665    |   0.0132572  |     5.55473 | CORNELL_GLETSCHER                |
| 120    |   0.302417    |   0.142394   |    47.0854  | GERARD_DE_GEER                   |
| 122    |   0.978693    |   0.0841493  |     8.59813 | USSING_BRAEER_N                  |
| 123    |   0.967159    |   0.0744336  |     7.69611 | USSING_BRAEER_N                  |
| 124    |   0.0609109   |   0.0262416  |    43.0819  | JAETTEGLETSCHER                  |
| 125    |   1.7352      |   0.103243   |     5.9499  | USSING_BRAEER                    |
| 126    |   0.147957    |   0.0206948  |    13.9871  | USSING_BRAEER                    |
| 127    |   0.722277    |   0.0267532  |     3.70401 | QEQERTARSUUP_SERMIA              |
| 128    |   0.0664664   |   0.0332622  |    50.0436  | NORDENSKIOLD_NE                  |
| 129    |   6.21848     |   0.256494   |     4.1247  | KAKIVFAAT_SERMIAT                |
| 130    |   0.04207     |   0.00444826 |    10.5735  | KAKIVFAAT_SERMIAT                |
| 131    |   0.000504298 |   0.00572415 |  1135.07    | HISINGER_GLETSCHER               |
| 132    |   1.14576     |   0.0662401  |     5.78134 | NUNATAKASSAAP_SERMIA             |
| 133    |   0.606772    |   0.0762735  |    12.5704  | UPERNAVIK_ISSTROM_N              |
| 134    |   7.49194     |   0.770461   |    10.2839  | UPERNAVIK_ISSTROM_N              |
| 135    |   6.64584     |   0.826002   |    12.4289  | UPERNAVIK_ISSTROM_C              |
| 136    |   2.76336     |   0.221      |     7.99752 | UPERNAVIK_ISSTROM_S              |
| 137    |   0.111059    |   0.0151142  |    13.6092  | UPERNAVIK_ISSTROM_SS             |
| 138    |   0.898969    |   0.100764   |    11.2088  | F_GRAAE                          |
| 139    |   0.00332712  |   0.0173174  |   520.492   | CHARCOT                          |
| 140    |   9.50652     |   0.681882   |     7.17278 | DAUGAARD-JENSEN                  |
| 141    |   1.27855     |   0.124804   |     9.76137 | INNGIA_ISBRAE                    |
| 142    |   1.31589     |   0.137993   |    10.4867  | UMIAMMAKKU_ISBRAE                |
| 143    |  12.9161      |   0.860943   |     6.66564 | RINK_ISBRAE                      |
| 144    |   0.0284199   |   0.0102236  |    35.9735  | EIELSON_HARE_FJORD-ROLIGE        |
| 145    |   1.73059     |   0.252957   |    14.6168  | KANGERLUSSUUP_SERMERSUA          |
| 146    |   0.582089    |   0.0938946  |    16.1306  | EIELSON_HARE_FJORD-ROLIGE        |
| 147    |   0.157873    |   0.0763918  |    48.388   | BREDEGLETSJER                    |
| 149    |   0.0666042   |   0.0138273  |    20.7604  | KANGERLUARSUUP_SERMIA            |
| 150    |   0.00317286  |   0.0895822  |  2823.39    | GEIKIE3                          |
| 153    |   0.263847    |   0.108678   |    41.1899  | SYDBR                            |
| 154    |   0.668659    |   0.0350659  |     5.24422 | SERMEQ_SILARLEQ                  |
| 155    |   1.9771      |   0.337389   |    17.0648  | VESTFJORD                        |
| 156    |   2.56442     |   0.243065   |     9.47835 | SERMEQ_SILARLEQ                  |
| 157    |   2.59729     |   0.155423   |     5.98405 | SERMEQ_SILARLEQ                  |
| 158    |   1.59047     |   0.250703   |    15.7628  | MAGGA_DAN_GLETSCHER              |
| 159    |   0.751903    |   0.0531029  |     7.06247 | CW_NONAME2                       |
| 160    |   0.218979    |   0.0734166  |    33.5267  | GEIKIE_UNNAMED_VESTFORD_S        |
| 161    |   0.00739597  |   0.00607013 |    82.0734  | GEIKIE_UNNAMED_VESTFORD_S        |
| 162    |   0.0558706   |   0.0361039  |    64.6205  | GEIKIE_UNNAMED_VESTFORD_S        |
| 163    |   0.523165    |   0.0359505  |     6.87173 | LILLE_GLETSCHER                  |
| 164    |   0.181546    |   0.016093   |     8.86444 | LILLE_GLETSCHER                  |
| 165    |   0.351213    |   0.0176866  |     5.03587 | STORE_GLETSCHER                  |
| 166    |   0.0507093   |   0.0148463  |    29.2773  | DENDRITGLETSCHER                 |
| 167    |   0.0987294   |   0.0167473  |    16.9629  | DENDRITGLETSCHER                 |
| 168    |   8.37552     |   0.669684   |     7.99573 | STORE_GLETSCHER                  |
| 169    |   0.00209469  |   0.00135255 |    64.5706  | GEIKIE7                          |
| 170    |   1.97443     |   0.158261   |     8.01552 | SERMEQ_AVANNARLEQ2               |
| 172    |   7.45363     |   0.632841   |     8.49037 | SERMEQ_KUJALLEQ                  |
| 173    |   0.055432    |   0.0271794  |    49.032   | GEIKIE6                          |
| 178    |   0.999155    |   0.0487067  |     4.87479 | KANGILERNGATA_SERMIA             |
| 179    |   1.16795     |   0.0962233  |     8.23863 | EQIP_SERMIA                      |
| 182    |   0.177637    |   0.0655021  |    36.8741  | BORGGRAVEN                       |
| 183    |   0.0128347   |   0.00732208 |    57.0492  | SERMEQ_AVANNARLEQ                |
| 184    |   0.0350119   |   0.0167663  |    47.8875  | SERMEQ_AVANNARLEQ                |
| 185    |   0.266693    |   0.0638888  |    23.9559  | KRONBORG                         |
| 186    |  45.9601      |   4.27592    |     9.30355 | JAKOBSHAVN_ISBRAE                |
| 187    |   0.0983969   |   0.0358237  |    36.4073  | ROSENBORG                        |
| 188    |   0.0209153   |   0.0054144  |    25.8873  | ROSENBORG                        |
| 189    |   0.121413    |   0.0036937  |     3.04227 | KONG_CHRISTIAN                   |
| 190    |   0.0180087   |   0.00992784 |    55.1281  | STYRTE                           |
| 191    |   3.19292     |   0.387732   |    12.1435  | KONG_CHRISTIAN                   |
| 192    |  25.1468      |   1.2993     |     5.16687 | KANGERLUSSUAQ                    |
| 193    |   0.00365628  |   0.00413155 |   112.999   | COURTAULD                        |
| 194    |   0.797613    |   0.0319491  |     4.00558 | UNNAMED_KANGER_W                 |
| 195    |   0.149501    |   0.0421936  |    28.2229  | SORGENFRI                        |
| 196    |   0.00499414  |   0.0143874  |   288.086   | UNNAMED_KANGER_E                 |
| 197    |   0.170237    |   0.0401522  |    23.586   | SAQQARLIUP_ALANGORLIUP           |
| 199    |   0.199083    |   0.133361   |    66.9875  | FREDERIKSBORG_GLETSCHER          |
| 202    |   0.00214098  |   0.00716085 |   334.465   | UNNAMED_DECEPTION_N              |
| 204    |   0.00230142  |   0.00453713 |   197.144   | UNNAMED_DECEPTION_N              |
| 205    |   0.0720322   |   0.00738748 |    10.2558  | UNNAMED_DECEPTION_N              |
| 206    |   0.00615882  |   0.00802063 |   130.23    | UNNAMED_DECEPTION_N              |
| 207    |   1.29726     |   0.0759682  |     5.85604 | POLARIC-DECEPTION_O_N            |
| 208    |   0.0343321   |   0.0058739  |    17.1091  | POLARIC-DECEPTION_O_N            |
| 209    |   0.0639358   |   0.0539933  |    84.4493  | POLARIC-DECEPTION_O_N            |
| 210    |   0.001269    |   0.193065   | 15213.9     | UNNAMED_POLARIC_S                |
| 211    |   0.00109478  |   0.153219   | 13995.4     | UNNAMED_POLARIC_S                |
| 212    |   7.55157e-05 |   0.00353036 |  4675       | UNNAMED_POLARIC_S                |
| 213    |   9.04978     |   0.246587   |     2.72478 | UNNAMED_DECEPTION_O_CN_CS        |
| 214    |   4.85333     |   0.212367   |     4.37569 | UNNAMED_UUNARTIT_ISLANDS         |
| 215    |   0.788414    |   0.0391102  |     4.96062 | KRUUSE_FJORD                     |
| 216    |   0.165063    |   0.0466749  |    28.2771  | KRUUSE_FJORD                     |
| 217    |   0.00325578  |   0.00513041 |   157.579   | KRUUSE_FJORD                     |
| 218    |   0.000585518 |   0.0508575  |  8685.9     | LAUBE_GLETSCHER                  |
| 219    |   0.000245903 |   0.040796   | 16590.2     | UNNAMED_LAUBE_S                  |
| 223    |   0.000650151 |   0.104006   | 15997.2     | UNNAMED_LAUBE_S                  |
| 225    |   0.00182813  |   0.313983   | 17175.1     | UNNAMED_LAUBE_S                  |
| 226    |   4.65263     |   0.795364   |    17.0949  | KIV_STEENSTRUP_NODRE_BRAE        |
| 227    |   0.0003607   |   0.0671429  | 18614.6     | KIV_STEENSTRUP_SONDRE_BRAE       |
| 228    |   0.000300914 |   0.0459726  | 15277.6     | NIGERTULUUP_KATTILERTARPIA       |
| 229    |   0.000639405 |   0.0983331  | 15378.8     | GLACIERDEFRANCE                  |
| 230    |   1.23429     |   0.348222   |    28.2123  | MIDGARDGLETSCHER                 |
| 231    |   6.44917e-05 |   0.0076296  | 11830.4     | MIDGARDGLETSCHER                 |
| 232    |   2.74239     |   0.221404   |     8.07341 | FENRISGLETSCHER                  |
| 233    |  28.307       |   2.12532    |     7.50812 | HELHEIMGLETSCHER                 |
| 235    |   0.00388564  |   0.0469587  |  1208.52    | APUSEERAJIK                      |
| 238    |   0.463106    |   0.567007   |   122.436   | HEIM_GLETSCHER                   |
| 239    |   0.00271756  |   0.0443609  |  1632.38    | BRCKNER_GLETSCHER                |
| 240    |   0.734125    |   0.553775   |    75.4333  | NONAME_IKERTIVAQ_N               |
| 241    |   7.8092      |   0.765558   |     9.80329 | IKERTIVAQ_NN                     |
| 242    |  10.4887      |   0.949496   |     9.05253 | IKERTIVAQ_M                      |
| 245    |   0.00992406  |   0.0688171  |   693.437   | NONAME_IKERTIVAQ_S               |
| 246    |   0.00300063  |   0.159131   |  5303.26    | SE_NONAME1                       |
| 247    |   3.90797     |   0.571938   |    14.6352  | KOGE_BUGT_N                      |
| 248    |  16.999       |   1.22285    |     7.19366 | KOGE_BUGT_C                      |
| 250    |   0.0593096   |   0.0675393  |   113.876   | SE_NONAME1                       |
| 253    |   0.00166484  |   0.0362639  |  2178.22    | SE_NONAME1                       |
| 255    |   0.286414    |   0.012137   |     4.23755 | KOGE_BUGT_S                      |
| 260    |   6.27373     |   0.338655   |     5.39799 | KOGE_BUGT_S                      |
| 261    |   0.405083    |   0.0310792  |     7.6723  | KOGE_BUGT_S                      |
| 263    |   0.000516315 |   0.0950138  | 18402.3     | KOGE_BUGT_SS                     |
| 266    |   0.000943541 |   0.165326   | 17521.9     | KOGE_BUGT_SS                     |
| 267    |   0.00166026  |   0.11882    |  7156.68    | KOGE_BUGT_SS                     |
| 268    |   0.00375699  |   0.264724   |  7046.18    | UMIIVIK_FJORD                    |
| 269    |   0.00702439  |   0.0733914  |  1044.81    | UMIIVIK_FJORD                    |
| 271    |   2.5632      |   0.260519   |    10.1638  | NARSAP_SERMIA                    |
| 272    |   0.0150389   |   0.192821   |  1282.15    | UMIIVIK_FJORD                    |
| 273    |   4.4488      |   1.04037    |    23.3855  | UMIIVIK_FJORD                    |
| 274    |   0.153855    |   0.0978981  |    63.6302  | UMIIVIK_FJORD                    |
| 278    |   0.498188    |   0.0555729  |    11.155   | AKULLERSUUP-QAMANAARSUUP         |
| 279    |   0.000274782 |   0.0575628  | 20948.5     | APUSEERSERPIA                    |
| 280    |   4.21919     |   0.586934   |    13.9111  | GRAULV                           |
| 282    |   0.269382    |   0.0362142  |    13.4434  | GRAULV                           |
| 283    |   5.06237     |   0.526477   |    10.3998  | KANGIATA_NUNAATA_SERMIA          |
| 284    |   4.58791     |   0.268957   |     5.86229 | GYLDENLOVE                       |
| 285    |   0.150007    |   0.277036   |   184.682   | GYLDENLOVE_S                     |
| 286    |   0.00034279  |   0.0719289  | 20983.4     | GYLDENLOVE_SS                    |
| 287    |   0.000454846 |   0.0715212  | 15724.3     | GYLDENLOVE_S                     |
| 289    |   3.29127e-05 |   0.00523086 | 15893.1     | SE_NONAME2                       |
| 290    |   7.61328e-05 |   0.0116113  | 15251.4     | SE_NONAME2                       |
| 291    |   0.000866503 |   0.154398   | 17818.6     | SE_NONAME2                       |
| 292    |   0.00051318  |   0.0943305  | 18381.6     | SE_NONAME2                       |
| 293    |   0.000532712 |   0.066162   | 12419.8     | FIMBULGETLSCHER                  |
| 294    |   0.00212486  |   0.354715   | 16693.6     | FIMBULGETLSCHER                  |
| 295    |   7.2802e-05  |   0.00889059 | 12212       | SE_NONAME2                       |
| 296    |   0.00094828  |   0.185566   | 19568.7     | SE_NONAME2                       |
| 297    |   3.85746     |   0.301947   |     7.8276  | AP_BERNSTOFF_GLETSCHER           |
| 298    |   1.14415     |   0.0667781  |     5.83647 | AP_BERNSTOFF_GLETSCHER           |
| 301    |   0.000255828 |   0.0436295  | 17054.2     | SE_NONAME2                       |
| 304    |   0.000252462 |   0.0424767  | 16825       | SE_NONAME2                       |
| 306    |   0.127045    |   0.0316944  |    24.9473  | MAELKEVEJEN                      |
| 310    |   0.927038    |   0.215917   |    23.2911  | MAELKEVEJEN                      |
| 311    |   0.00277807  |   0.012022   |   432.748   | SKINFAXE                         |
| 313    |   1.55004     |   0.224873   |    14.5075  | SKINFAXE                         |
| 314    |   1.85274     |   0.106221   |     5.73321 | RIMFAXE                          |
| 315    |   1.40774     |   0.116474   |     8.27383 | HEIMDAL_GLETSCHER                |
| 316    |   0.0490824   |   0.00838582 |    17.0852  | TINGMIARMIUT_FJORD               |
| 317    |   8.04856     |   0.664796   |     8.25982 | TINGMIARMIUT_FJORD               |
| 318    |   0.0767972   |   0.0146471  |    19.0725  | MOGENS_HEINESEN_N                |
| 319    |   2.08105     |   0.28327    |    13.6118  | MOGENS_HEINESEN_N                |
| 320    |   0.87322     |   0.109203   |    12.5058  | MOGENS_HEINESEN_C                |
| 321    |   4.72381     |   0.168764   |     3.57263 | MOGENS_HEINESEN_S                |
| 322    |   0.000286124 |   0.052938   | 18501.8     | MOGENS_HEINESEN_SS_SSS           |
| 323    |   0.128705    |   0.0322384  |    25.0483  | AVANNARLEQ-NIGERLIKASIK          |
| 325    |   0.450794    |   0.0641945  |    14.2403  | MOGENS_HEINESEN_SS_SSS           |
| 326    |   0.000160574 |   0.0448814  | 27950.7     | MOGENS_HEINESEN_SS_SSS           |
| 327    |   0.0113678   |   0.0059108  |    51.9958  | AVANNARLEQ-NIGERLIKASIK          |
| 328    |   4.37535     |   0.367043   |     8.38889 | PUISORTOQ_N                      |
| 329    |   1.80255     |   0.172765   |     9.58448 | UKAASORSUAQ                      |
| 330    |   0.823701    |   0.125904   |    15.2851  | PUISORTOQ_S                      |
| 331    |   0.00571661  |   0.00689541 |   120.621   | PUISORTOQ_S                      |
| 332    |   0.374217    |   0.0651766  |    17.4168  | NAPASORSUAQ_N                    |
| 335    |   0.349705    |   0.0723664  |    20.6936  | NAPASORSUAQ_N                    |
| 337    |   7.64832e-05 |   0.0160352  | 20965.6     | NAPASORSUAQ_C_S                  |
| 338    |   0.000108454 |   0.0172894  | 15941.7     | NAPASORSUAQ_C_S                  |
| 339    |   6.43811e-05 |   0.00945635 | 14688.1     | NAPASORSUAQ_C_S                  |
| 340    |   4.71132     |   0.390471   |     8.28792 | ANORITUUP_KANGERLUA              |
| 341    |   0.487766    |   0.0422666  |     8.66534 | SERMILIGAARSSUK_BRAE             |
| 342    |   6.48975e-05 |   0.00752451 | 11594.5     | ANORITUUP_KANGERLUA              |
| 343    |   0.356953    |   0.100031   |    28.0236  | ANORITUUP_KANGERLUA              |
| 345    |   0.00136043  |   0.259424   | 19069.3     | UNNAMED_ANORITUUP_KANGERLUA_S    |
| 346    |   0.000125657 |   0.0235513  | 18742.6     | UNNAMED_ANORITUUP_KANGERLUA_SS   |
| 347    |   0.000157755 |   0.0272631  | 17281.9     | UNNAMED_ANORITUUP_KANGERLUA_SS   |
| 348    |   0.000215261 |   0.028888   | 13420       | UNNAMED_ANORITUUP_KANGERLUA_SS   |
| 349    |   0.686977    |   0.0736333  |    10.7184  | QAJUUTTAP_SERMIA                 |
| 350    |   0.965747    |   0.226052   |    23.407   | UNNAMED_HERLUF_TROLLE_N          |
| 351    |   0.127861    |   0.0347248  |    27.1583  | EQALORUTSIT_KILLIIT_SERMIAT      |
| 352    |   1.87522     |   1.18394    |    63.136   | EQALORUTSIT_KILLIIT_SERMIAT      |
| 354    |   0.888318    |   0.15148    |    17.0525  | KIATTUUT-QOOQQUP                 |
| 355    |   1.64062     |   0.163468   |     9.96379 | UNNAMED_HERLUF_TROLLE_S          |
| 357    |   0.0751754   |   0.324192   |   431.247   | UNNAMED_KANGERLULUK              |
| 358    |   0.000406285 |   0.101505   | 24983.6     | NAAJAT_SERMIAT                   |
| 359    |   0.193933    |   0.151796   |    78.2728  | SERMILIK                         |
| 360    |   5.21983e-05 |   0.00781245 | 14966.9     | HERLUF_TROLLE-KANGERLULUK-DANELL |
| 361    |   0.000292309 |   0.038651   | 13222.6     | UNNAMED_KANGERLULUK              |
| 362    |   0.000101103 |   0.0131135  | 12970.4     | UNNAMED_KANGERLULUK              |
| 364    |   0.440709    |   0.128063   |    29.0585  | UNNAMED_DANELL_FJORD             |
| 365    |   0.131109    |   0.00674577 |     5.14515 | UNNAMED_SOUTH_DANELL_FJORD       |
| 366    |   0.122957    |   0.144721   |   117.701   | SW_NONAME1                       |
| 368    |   0.0105897   |   0.0205935  |   194.466   | SW_NONAME1                       |
| 369    |   0.000111339 |   0.0174474  | 15670.6     | SOUTHERN_TIP                     |
| 370    |   0.000162942 |   0.0297611  | 18264.9     | SOUTHERN_TIP                     |
| 371    |   4.75215e-05 |   0.00141861 |  2985.19    | SOUTHERN_TIP                     |
| 372    |   6.6866e-05  |   0.00864522 | 12929.2     | SOUTHERN_TIP                     |
| 373    |   0.000116932 |   0.0148475  | 12697.5     | SOUTHERN_TIP                     |
| 374    |   0.000311045 |   0.0340177  | 10936.6     | SOUTHERN_TIP                     |
| GIS    | 471.31        |  49.1855     |    10.4359  |                                  |
:end:


** Raw data to discharge product
:PROPERTIES:
:header-args:jupyter-python+: :tangle raw2discharge.py :session raw2discharge
:END:
*** Load data

+ What columns are in the file?
+ Don't show all the "vel_eff_YYYY_MM_DD" and "err_eff_YYYY_MM_DD" columns.
#+BEGIN_SRC sh :results verbatim :tangle no
head -n1 ./tmp/dat_100_5000.csv | tr ',' '\n' | grep -v "vel_eff_*" | grep -v "err_eff_*" | grep -v "dh_*" | sort | uniq | tr '\n' '\t'
echo "also: dh_YYYY@elev, vel_eff_YYYY_MM_DD@various, etc."
#+END_SRC
#+RESULTS:
: bed_0@Millan_2018	bed@BedMachine	day@GIMP.0715	dem@GIMP.0715	err_2D	errbed@BedMachine	ex_baseline@MEaSUREs.0478	ey_baseline@MEaSUREs.0478	gates_gateID@gates_100_5000	gates_x@gates_100_5000	gates_y@gates_100_5000	lat	lon	regions@Mouginot_2019	sectors@Mouginot_2019	surface@BedMachine	thickness_0@Millan_2018	thickness@Bamber_2013	thickness@BedMachine	vel_baseline@MEaSUREs.0478	vel_err_baseline@MEaSUREs.0478	vx_baseline@MEaSUREs.0478	vy_baseline@MEaSUREs.0478	x	y	also: dh_YYYY@elev, vel_eff_YYYY_MM_DD@various, etc.

#+NAME: load_data
#+BEGIN_SRC jupyter-python :exports none :results raw drawer
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt

# pd.options.display.notebook_repr_html = False

###
### Load metadata
### 
meta_cols = ["x", "y", "err_2D", 
             "regions@Mouginot_2019", "sectors@Mouginot_2019", "gates_gateID@gates_100_5000"]
meta = pd.read_csv("./tmp/dat_100_5000.csv", usecols=meta_cols)
# rename columns
meta.rename(inplace=True, columns={'regions@Mouginot_2019':'regions', 
                                   'sectors@Mouginot_2019':'sectors',
                                   'gates_gateID@gates_100_5000':'gates'})
regions = {1:'NO', 2:'NE', 3:'CE', 4:'SE', 5:'SW', 6:'CW', 7:'NW'}
meta['regions'] = meta['regions'].map(regions.get) # Convert sector numbers to meaningful names
meta['ones'] = 1

R = pd.read_csv('./out/gate_meta.csv')
meta['name'] = ''
for g in meta['gates'].unique(): meta.loc[meta['gates'] == g, 'name'] = R[R['gate'] == g]['Mouginot_2019'].values

###
### Load BASELINE velocity
###
vel_baseline = pd.read_csv("./tmp/dat_100_5000.csv", usecols=['vel_baseline@MEaSUREs.0478'])
vel_baseline.rename(inplace=True, columns={'vel_baseline@MEaSUREs.0478':'vel'})

###
### Load all velocity
###
vel = pd.read_csv("./tmp/dat_100_5000.csv", usecols=(lambda c: ('vel_eff' in c)))
vel.rename(columns=lambda c: pd.datetime(int(c[8:12]), int(c[13:15]), int(c[16:18])), inplace=True)
# vel.drop(columns=pd.datetime(1999, 7, 1), inplace=True) # bad year?
vel.replace(0, np.nan, inplace=True)
# vel = vel.loc[:,vel.columns.year < 2018] # drop 2018
vel.sort_index(axis='columns', inplace=True)

####################
# Filter Velocity: Rolling Windows
##################
def filter_bad_v(v):
    WINDOW=30
    SIGMA=2
    vel_rolling = v.T.rolling(window=WINDOW, center=True, min_periods=1).mean().T
    vel_residual = v - vel_rolling
    vel_std = vel_residual.T.rolling(window=WINDOW, center=True, min_periods=1).std().T
    vel_outlier = (v > vel_rolling+SIGMA*vel_std) | ( v < vel_rolling-SIGMA*vel_std)
    v[vel_outlier] = np.nan
    return v
####################
vel = filter_bad_v(vel)
vel = filter_bad_v(vel)
vel = filter_bad_v(vel)

fill = vel.copy() / vel # 1 where data, nan where not

vel = vel.interpolate(method='time', axis=1, limit_area='inside')
vel.fillna(method='ffill', axis=1, inplace=True)
vel.fillna(method='bfill', axis=1, inplace=True)

# vel[meta.name == TESTNAME].T.sort_index().head()
# fill[meta.name == TESTNAME].T.sort_index().head()

# vel.sum(axis='rows').resample('1D').mean().interpolate(method='time', limit_area='inside').resample('A').mean()/1E6


###
### Load all velocity ERROR
###
err = pd.read_csv("./tmp/dat_100_5000.csv", usecols=(lambda c: ('err_eff' in c)))
err.rename(columns=lambda c: pd.datetime(int(c[8:12]), int(c[13:15]), int(c[16:18])), inplace=True)
err.replace(0, np.nan, inplace=True)
err.sort_index(axis='columns', inplace=True)
# err.interpolate(method='time', limit_area='inside', axis=1 inplace=True)
err.fillna(method='ffill', axis=1, inplace=True)
err.fillna(method='backfill', axis=1, inplace=True)

for c in err.columns[err.columns.duplicated()]:
    err.drop(columns=c, inplace=True)

# make sure we have error (even if 0) for each velocity, and no err w/o vel
for c in vel.columns:
    if c not in err.columns:
        err[c] = np.nan

for c in err.columns:
    if c not in vel.columns:
        err.drop(columns=c, inplace=True)
    
err.sort_index(axis='columns', inplace=True)

# tmp = np.array([c if c not in err.columns else None for c in vel.columns]); print(tmp[tmp != None])


###
### Thickness
###
th = pd.read_csv("./tmp/dat_100_5000.csv", usecols=["thickness@BedMachine",
                                           "surface@BedMachine",
                                           "bed@BedMachine",
                                           "errbed@BedMachine",
                                           "bed_0@Millan_2018",
                                           "thickness_0@Millan_2018",
                                           "gates_gateID@gates_100_5000"])
th.rename(inplace=True, columns={'thickness@BedMachine':'thick',
                                 'bed_0@Millan_2018':'bed@Millan_2018',
                                 'thickness_0@Millan_2018':'thickness@Millan_2018',
                                 'errbed@BedMachine': 'err',
                                 'gates_gateID@gates_100_5000':'gates'})
th_GIMP = pd.read_csv("./tmp/dat_100_5000.csv", usecols=(lambda c: ('@GIMP.0715' in c)))
th_GIMP['day'] = [dt.datetime(2000,1,1) + dt.timedelta(days=np.int(_)) for _ in th_GIMP['day@GIMP.0715']]
for _ in th_GIMP.columns: th[_] = th_GIMP[_]
del(th_GIMP)


###
### dh/dt
###
dhdt = pd.read_csv("./tmp/dat_100_5000.csv", usecols=(lambda c: ('dh' in c)))
mv = {}
for c in dhdt.columns: mv[c] = np.int(c.split('@')[0].split('_')[1])
dhdt.rename(inplace=True, columns=mv)

# assume linear trend average of adjacent 3 years for missing years
# dhdt[2018] = dhdt.loc[:,2015:2017].mean(axis='columns')
# dhdt[2019] = dhdt.loc[:,2015:2017].mean(axis='columns')
dhdt[2018] = 0
dhdt[2019] = 0
for y in np.arange(1985,1994+1): dhdt[y] = dhdt.loc[:,1995:1997].mean(axis='columns')
dhdt.sort_index(axis='columns', inplace=True)

# what is the unadjusted discharge using BedMachine thickness?
D = (vel).apply(lambda c: c * (200 * th['thick'] * meta['err_2D'].values), axis=0) * 917 / 1E12
D.sum(axis='rows').resample('1D').mean().interpolate(method='time', limit_area='inside').resample('A').mean()
#+END_SRC

#+RESULTS: load_data
#+begin_example
1985-12-31    366.513535
1986-12-31    375.755071
1987-12-31    375.464187
1988-12-31    371.502576
1989-12-31    374.419049
1990-12-31    370.962210
1991-12-31    368.800622
1992-12-31    371.283426
1993-12-31    373.966629
1994-12-31    372.115812
1995-12-31    366.201837
1996-12-31    372.471276
1997-12-31    378.800381
1998-12-31    381.143531
1999-12-31    391.551121
2000-12-31    397.503762
2001-12-31    398.169010
2002-12-31    414.841587
2003-12-31    423.172457
2004-12-31    437.415797
2005-12-31    453.088396
2006-12-31    444.779904
2007-12-31    439.746026
2008-12-31    442.429674
2009-12-31    449.653892
2010-12-31    456.536357
2011-12-31    460.792451
2012-12-31    461.384041
2013-12-31    468.030631
2014-12-31    472.025586
2015-12-31    472.851399
2016-12-31    468.849381
2017-12-31    474.629449
2018-12-31    466.229784
2019-12-31    468.359631
Freq: A-DEC, dtype: float64
#+end_example

*** Adjust thickness 
**** Millan 2018
What are the changes in discharge in Millan 2018 region?

#+NAME: millan_2018
#+BEGIN_SRC jupyter-python :exports none :results raw drawer
th['thick_max'] = np.maximum(th['thick'].values, th['thickness@Millan_2018'].values)

D_tmp = pd.DataFrame(index=th.index, columns=['BedMachine','Millan','Max'])

D_tmp['BedMachine'] = vel_baseline.apply(lambda c: c * th['thick'].values, axis=0) * 200 * 917 / 1E12
D_tmp['Millan'] = vel_baseline.apply(lambda c: c * th['thickness@Millan_2018'].values, axis=0) * 200 * 917 / 1E12
D_tmp['Max'] = vel_baseline.apply(lambda c: c * th['thick_max'].values, axis=0) * 200 * 917 / 1E12

D_tmp.index.name = "Discharge [Gt]"
D_tmp[D_tmp['Millan'] != 0].sum(axis=0)
#+END_SRC

#+RESULTS: millan_2018
: BedMachine    101.664973
: Millan        110.146378
: Max           117.419139
: dtype: float64


Decision: Use Millan (2018) where it exists. Then adjust as per BedMachine for thickness <= 20 m.

**** Adjust "bad" thickness
Here we perform a few thickness adjustments:

First, patch in Millan (2018) where it exists.

+ 300 :: All ice <= 20 m thick is assumed bad and set to the minimum
         "good" thickness in a gate if good exists, or 300 m if it
         does not exist
+ 400 :: All ice <= 50 m thick is set to 400 m thick
+ fit :: All ice <= 20 m thick is fit to the log10(thickness) v.
         log10(velocity) relationship, even though it is not a good
         fit.

For testing, gate clumps 9 (all bad) and 546 (some bad)

#+CALL: load_data[:results none]()

#+NAME: adjust_thickness
#+BEGIN_SRC jupyter-python :exports none :results raw drawer :display text/org

# Patch in Millan (2018) where that data set has thickness > 0.
th.loc[th['thickness@Millan_2018'] > 0, 'thick'] = th.loc[th['thickness@Millan_2018'] > 0, 'thickness@Millan_2018']

th['bad'] = th['thick'] <= 20

th['thick_adj_300'] = th['thick']
th['thick_adj_300_err'] = th['err']
for g in th[th['bad']]['gates'].unique(): # only work on gates with some (or all) bad thickness
    if all(th[th['gates'] == g]['bad']): # If all bad, set to 300
        th.loc[th['gates'] == g, 'thick_adj_300'] = 300
        th.loc[th['gates'] == g, 'thick_adj_300_err'] = 300/2.

    elif any(th[th['gates'] == g]['bad']): # If any bad, set to minimum of good.
        th.loc[(th['gates'] == g) & (th['bad']), 'thick_adj_300'] = \
        (th.loc[(th['gates'] == g) & (~th['bad']), 'thick']).min()
        th.loc[(th['gates'] == g) & (th['bad']), 'thick_adj_300_err'] = 300/2.

# aggressive: Anything <= 50 gets 400 m thickness
th['thick_adj_400'] = [400 if T <= 50 else T for T in th['thick']]
th['thick_adj_400_err'] = [400/2. if T[0] <= 50 else T[1] for T in zip(th['thick'],th['err'])]
#+END_SRC

#+RESULTS: adjust_thickness

#+BEGIN_SRC jupyter-python :exports none :results raw drawer :display text/org :session discharge
<<load_data>>
<<millan_2018>>
<<adjust_thickness>>

print(th.describe())

D0 = (vel).apply(lambda c: c * (200 * th['thick'] * meta['err_2D'].values), axis=0) * 917 / 1E12
D1 = (vel).apply(lambda c: c * (200 * th['thick_adj_300'] * meta['err_2D'].values), axis=0) * 917 / 1E12
D0 = D0.sum(axis='rows').resample('1D').mean().interpolate(method='time', limit_area='inside',).resample('A').mean()
D1 = D1.sum(axis='rows').resample('1D').mean().interpolate(method='time', limit_area='inside').resample('A').mean()
pd.concat([D0,D1, D1-D0], axis='columns', keys=['BedMachine','300','diff'])
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
             gates  bed@BedMachine        thick  surface@BedMachine  bed@Millan_2018  \
count  6002.000000     6002.000000  6002.000000         6002.000000      6002.000000   
mean    168.306398       42.703099   404.833499          441.314562        90.434342   
std     103.096612      395.702530   260.601610          248.583263       231.987699   
min       1.000000    -1129.000000     0.000000           61.000000      -946.148190   
25%      80.000000     -223.750000   210.687670          257.000000         0.000000   
50%     163.000000       -2.000000   409.649870          399.000000         0.000000   
75%     246.000000      328.000000   584.000000          562.750000         0.000000   
max     374.000000     1465.000000  1477.000000         1587.000000      1444.466800   

       thickness@Millan_2018          err  dem@GIMP.0715  day@GIMP.0715    thick_max  \
count            6002.000000  6002.000000    6002.000000    6002.000000  6002.000000   
mean               96.546331    67.189270     461.711131    4770.782073   410.314916   
std               205.603627    58.707289     256.669219    1324.627790   262.792015   
min               -53.307507    10.000000      72.231743       0.000000     0.000000   
25%                 0.000000    27.000000     269.828997    4549.000000   216.351658   
50%                 0.000000    48.000000     408.804415    5224.000000   416.000000   
75%                 2.206833    86.000000     598.706865    5550.000000   588.000000   
max              1295.148200   305.000000    1616.262000    5782.000000  1477.000000   

       thick_adj_300  thick_adj_300_err  thick_adj_400  thick_adj_400_err  
count    6002.000000        6002.000000    6002.000000        6002.000000  
mean      432.288209          67.202266     457.119268          75.745751  
std       230.638613          55.374865     210.502317          66.432581  
min        21.000000          10.000000      50.429825          10.000000  
25%       300.000000          27.000000     347.000000          28.000000  
50%       409.649870          49.000000     409.649870          51.000000  
75%       584.000000          90.000000     584.000000          95.000000  
max      1477.000000         305.000000    1477.000000         305.000000  
#+end_example
|                     |   BedMachine |     300 |    diff |
|---------------------+--------------+---------+---------|
| 1985-12-31 00:00:00 |      374.375 | 382.212 | 7.83762 |
| 1986-12-31 00:00:00 |      383.616 | 391.454 | 7.83762 |
| 1987-12-31 00:00:00 |      383.325 | 391.163 | 7.83762 |
| 1988-12-31 00:00:00 |      379.364 | 387.201 | 7.83762 |
| 1989-12-31 00:00:00 |      382.28  | 390.118 | 7.83762 |
| 1990-12-31 00:00:00 |      378.823 | 386.661 | 7.83762 |
| 1991-12-31 00:00:00 |      376.662 | 384.578 | 7.91649 |
| 1992-12-31 00:00:00 |      378.871 | 386.748 | 7.87661 |
| 1993-12-31 00:00:00 |      381.563 | 389.228 | 7.66483 |
| 1994-12-31 00:00:00 |      380.443 | 388.131 | 7.68873 |
| 1995-12-31 00:00:00 |      375.421 | 383.299 | 7.87824 |
| 1996-12-31 00:00:00 |      381.479 | 389.671 | 8.19185 |
| 1997-12-31 00:00:00 |      386.988 | 394.988 | 8.0001  |
| 1998-12-31 00:00:00 |      389.026 | 396.698 | 7.67141 |
| 1999-12-31 00:00:00 |      399.215 | 406.817 | 7.60238 |
| 2000-12-31 00:00:00 |      406.172 | 413.258 | 7.08638 |
| 2001-12-31 00:00:00 |      406.827 | 414.011 | 7.18404 |
| 2002-12-31 00:00:00 |      423.918 | 431.258 | 7.34085 |
| 2003-12-31 00:00:00 |      432.797 | 440.326 | 7.52853 |
| 2004-12-31 00:00:00 |      445.853 | 453.637 | 7.78348 |
| 2005-12-31 00:00:00 |      461.521 | 469.362 | 7.84053 |
| 2006-12-31 00:00:00 |      452.762 | 460.468 | 7.70625 |
| 2007-12-31 00:00:00 |      447.352 | 454.935 | 7.58251 |
| 2008-12-31 00:00:00 |      450.1   | 457.769 | 7.66886 |
| 2009-12-31 00:00:00 |      457.835 | 465.763 | 7.92724 |
| 2010-12-31 00:00:00 |      464.367 | 472.247 | 7.88048 |
| 2011-12-31 00:00:00 |      468.892 | 476.691 | 7.79915 |
| 2012-12-31 00:00:00 |      469.052 | 476.754 | 7.70207 |
| 2013-12-31 00:00:00 |      476.133 | 483.941 | 7.80758 |
| 2014-12-31 00:00:00 |      480.493 | 488.013 | 7.52002 |
| 2015-12-31 00:00:00 |      481.436 | 488.866 | 7.42998 |
| 2016-12-31 00:00:00 |      477.094 | 484.309 | 7.21526 |
| 2017-12-31 00:00:00 |      483.651 | 490.622 | 6.97033 |
| 2018-12-31 00:00:00 |      475.28  | 482.348 | 7.06867 |
| 2019-12-31 00:00:00 |      478.09  | 485.357 | 7.26744 |
:END:

**** Adjust thickness w thickness v. velocity fit.

https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9
https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html

#+NAME: adjust_thickness_fit
#+BEGIN_SRC jupyter-python :exports none :results raw drawer :display text/plain
CUTOFF = 20
df = vel_baseline.join(th['thick'])
max_vel = df.loc[df['thick'] <= CUTOFF, 'vel'].max() # limit fit to velocities where data is missing
# df = df[(df['thick'] > CUTOFF) & (df['vel'] <= max_vel)]
df = df[df['thick'] > CUTOFF]
# df = df[df['vel'] <= max_vel]

import statsmodels.api as sm
y = (df['thick'])
X = np.log10(df['vel'])
X = sm.add_constant(X)
model = sm.OLS(y, X)
fits = model.fit()
# print(fits.summary())
predictions = fits.predict(X)

from statsmodels.sandbox.regression.predstd import wls_prediction_std
XX = np.linspace(X['vel'].min(), X['vel'].max(), 50)
XX = sm.add_constant(XX)
yy = fits.predict(XX)
sdev, lower, upper = wls_prediction_std(fits, exog=XX, alpha=0.05)

fig = plt.figure(1, figsize=(4,4)) # w,h
# get_current_fig_manager().window.move(0,0)
fig.clf()
fig.set_tight_layout(True)

ax = fig.add_subplot(111)
im = ax.scatter(X['vel'], y, alpha=0.1, color='k')
xl, yl = ax.get_xlim(), ax.get_ylim()
ax.set_ylabel('Thickness [m]')
ax.set_xlabel('Velocity [m yr$^{-1}$]')
ax.plot(XX[:,1], yy, 'r--')
ax.fill_between(XX[:,1], lower, upper, color='#888888', alpha=0.4)
ax.fill_between(XX[:,1], lower, upper, color='#888888', alpha=0.1)
# ax.set_xlim(50,xl[1])
ax.set_ylim(0,yl[1])
plt.savefig('./tmp/vel_thick_fit.png', transparent=True, bbox_inches='tight', dpi=300)
plt.savefig('./tmp/vel_thick_fit.pdf', transparent=True, bbox_inches='tight', dpi=300)
            
th['fit'] = th['thick']
vel_where_thick_bad = vel_baseline.loc[th['bad'] == True, 'vel']
th.loc[th['bad'] == True, 'fit'] = fits.predict(sm.add_constant(np.log10(vel_where_thick_bad)))
# set err to thickness where fit
th['fit_err'] = th['err']
th.loc[th['bad'] == True, 'fit_err'] = th.loc[th['bad'] == True, 'fit'] /2.


fits.summary()


D0 = (vel).apply(lambda c: c * (200 * th['thick_adj_300'] * meta['err_2D'].values), axis=0) * 917 / 1E12
D1 = (vel).apply(lambda c: c * (200 * th['fit'] * meta['err_2D'].values), axis=0) * 917 / 1E12
D0 = D0.sum(axis='rows').resample('1D').mean().interpolate(method='time', limit_area='inside').resample('A').mean()
D1 = D1.sum(axis='rows').resample('1D').mean().interpolate(method='time', limit_area='inside').resample('A').mean()
pd.concat([D0,D1, D1-D0], axis='columns', keys=['300','fit','diff'])

#+END_SRC

#+RESULTS: adjust_thickness_fit
:RESULTS:
: /home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   warnings.warn("This figure includes Axes that are not compatible "
: /home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   warnings.warn("This figure includes Axes that are not compatible "
#+begin_example
                   300         fit      diff
1985-12-31  382.212214  385.190263  2.978049
1986-12-31  391.453750  394.431799  2.978049
1987-12-31  391.162866  394.140915  2.978049
1988-12-31  387.201255  390.179304  2.978049
1989-12-31  390.117728  393.095777  2.978049
1990-12-31  386.660889  389.638937  2.978049
1991-12-31  384.578171  387.610159  3.031988
1992-12-31  386.747774  389.762390  3.014616
1993-12-31  389.228259  392.104511  2.876252
1994-12-31  388.131312  390.976866  2.845554
1995-12-31  383.299107  386.186263  2.887155
1996-12-31  389.670965  392.879998  3.209033
1997-12-31  394.988013  398.517121  3.529108
1998-12-31  396.697697  400.358936  3.661238
1999-12-31  406.817321  410.546617  3.729296
2000-12-31  413.258337  416.605503  3.347166
2001-12-31  414.010682  417.330430  3.319749
2002-12-31  431.258369  434.605964  3.347595
2003-12-31  440.325960  443.764954  3.438994
2004-12-31  453.636674  457.168805  3.532130
2005-12-31  469.361686  472.892685  3.530998
2006-12-31  460.468037  463.983593  3.515556
2007-12-31  454.934508  458.438836  3.504328
2008-12-31  457.768744  461.312991  3.544246
2009-12-31  465.762541  469.323988  3.561447
2010-12-31  472.247421  475.833743  3.586321
2011-12-31  476.690945  480.336766  3.645821
2012-12-31  476.753950  480.444280  3.690330
2013-12-31  483.940671  487.630615  3.689944
2014-12-31  488.012985  491.670156  3.657171
2015-12-31  488.865766  492.392053  3.526288
2016-12-31  484.309188  487.775400  3.466212
2017-12-31  490.621586  493.891863  3.270276
2018-12-31  482.348355  485.628145  3.279790
2019-12-31  485.357292  488.780664  3.423371
#+end_example
: /home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   warnings.warn("This figure includes Axes that are not compatible "
: <Figure size 288x288 with 1 Axes>
:END:

**** Table of thickness adjustments
:PROPERTIES:
:ID:       62f5d28c-c704-422d-9e9b-c0771d5b86ee
:END:

#+BEGIN_SRC jupyter-python :exports none :results raw drawer :display text/org
th[['thick','thick_adj_300','thick_adj_400','fit']].describe()
#+END_SRC

#+RESULTS:
|       |   thick | thick_adj_300 | thick_adj_400 |     fit |
|-------+---------+---------------+---------------+---------|
| count |    6002 |          6002 |          6002 |    6002 |
| mean  | 404.833 |       432.288 |       457.119 | 439.779 |
| std   | 260.602 |       230.639 |       210.502 | 225.811 |
| min   |       0 |            21 |       50.4298 |      21 |
| 25%   | 210.688 |           300 |           347 | 288.665 |
| 50%   |  409.65 |        409.65 |        409.65 | 420.584 |
| 75%   |     584 |           584 |           584 | 584.605 |
| max   |    1477 |          1477 |          1477 |    1477 |



**** Baseline discharge values for various thickness adjustments
Here we calculate:
+ D_baseline_th_noadj :: Discharge with no thickness adjustment
+ D_baseline_th_300 :: The baseline discharge
+ D_baseline_th_400 :: The discharge assuming the aggressive thickness adjustment
+ D_baseline_th_fit :: The discharge assuming the fitted thickness adjustment
+ D_baseline :: The baseline discharge - picked from our favorite of the above. TBD

#+NAME: discharge_th
#+BEGIN_SRC jupyter-python :exports none :results raw drawer
D_th = pd.DataFrame(index=th.index,
                    columns=['NoAdj','NoAdj_err','300','300_err','400','400_err','fit','fit_err'])

# + D_baseline_th_noadj :: Discharge with no thickness adjustment
D_th['NoAdj'] = vel_baseline.apply(lambda c: c * (th['thick'].values * 200 * meta['err_2D'].values), axis=0) * 917 / 1E12

# should match HeatMap
D_th['NoMillan'] = vel_baseline.apply(lambda c: c * ((th['surface@BedMachine']-th['bed@BedMachine']).values * 200), axis=0) * 917 / 1E12

# D_baseline_th_noadj_err ::
D_th['NoAdj_err'] = vel_baseline.apply(lambda c: c * (th['err'].values * 200 * meta['err_2D'].values), axis=0) * 917 / 1E12

D_th['300'] = vel_baseline.apply(lambda c: c * (th['thick_adj_300'].values * 200 * meta['err_2D'].values), axis=0) * 917 / 1E12
D_th['300_err'] = vel_baseline.apply(lambda c: c * (th['thick_adj_300_err'].values  * 200 * meta['err_2D'].values), axis=0) * 917 / 1E12

D_th['400'] = vel_baseline.apply(lambda c: c * (th['thick_adj_400'].values * 200 * meta['err_2D'].values), axis=0) * 917 / 1E12
D_th['400_err'] = vel_baseline.apply(lambda c: c * (th['thick_adj_400_err'].values * 200 * meta['err_2D'].values), axis=0) * 917 / 1E12

D_th['fit'] = vel_baseline.apply(lambda c: c * (th['fit'].values * 200 * meta['err_2D'].values), axis=0) * 917 / 1E12
D_th['fit_err'] = vel_baseline.apply(lambda c: c * (th['fit_err'].values* 200 * meta['err_2D'].values), axis=0) * 917 / 1E12

D_th.sum(axis=0)
#+END_SRC

#+RESULTS: discharge_th
: NoAdj        480.004070
: NoAdj_err     49.160007
: 300          488.156346
: 300_err       48.940213
: 400          494.564033
: 400_err       51.337163
: fit          492.210294
: fit_err       50.529233
: NoMillan     472.086164
: dtype: float64

**** Map of where thickness adjustments occur

#+BEGIN_SRC jupyter-python :session gate_map :exports none :results raw drawer :tangle no
import matplotlib.pyplot as plt
import numpy as np

from grass_session import Session
from grass.pygrass.modules.shortcuts import general as g
from grass.pygrass.modules.shortcuts import raster as r
from grass.pygrass.modules.shortcuts import vector as v
from grass.pygrass.modules.shortcuts import display as d
from grass.script import array as garray

from grass.pygrass.modules.shortcuts import vector as v
import shapefile

if 'th' not in locals():
    <<load_data>>
    <<millan_2018>>
    <<adjust_thickness>>
    <<adjust_thickness_fit>>
    # <<discharge>>

# th['bad'] = th['thick'] <= 20
th['all_bad'] = False
for g in th[th['bad']]['gates'].unique():
    if all(th[th['gates'] == g]['bad']):
        th.loc[th['gates'] == g, 'all_bad'] = True
        
tmp = meta.copy()
tmp['vel'] = vel_baseline
gates = pd.DataFrame(index=th['gates'].unique())
id = 'gates' #_gateID@gates_100_5000'
gates['x'] = tmp.groupby(id).mean()['x']
gates['y'] = tmp.groupby(id).mean()['y']
gates['all_bad'] = th.groupby('gates').mean()['all_bad']
gates['bad'] = th.groupby('gates').mean()['bad']
gates['regions'] = tmp.groupby(id).first()['regions']
gates['vel_all_bad'] = tmp[th['all_bad'] == 1].groupby(id).mean()['vel']
gates['vel_part_bad'] = tmp[th['bad']>0].groupby(id).mean()['vel']
gates['vel_gd'] = tmp[th['bad'] == 0].groupby(id).mean()['vel']

fig = plt.figure(1, figsize=(8,6)) # w,h
fig.clf()
fig.set_tight_layout(True)

from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

import matplotlib.gridspec as gridspec
gs = gridspec.GridSpec(2, 2) #h, w
ax1 = plt.subplot(gs[:,0])
ax2 = plt.subplot(gs[0,1])
ax3 = plt.subplot(gs[1,1])

SIZE=4

# Write out the vector from GRASS to disk
if 'r_regions' not in locals():
    with Session(gisdb="./", location="G", mapset="Mouginot_2019", create_opts=""):
        # r.to.vect input=regions output=regions type=area
        # v.out.ogr -c input=tmp output=tmp.shp format=ESRI_Shapefile --o
        r_regions = shapefile.Reader("./tmp/SHP/tmp.shp")

sf = r_regions

for shape in list(sf.iterShapes()):
   x_lon = np.zeros(len(shape.points))
   y_lat = np.zeros(len(shape.points))
   for ip in range(len(shape.points)):
       x_lon[ip] = shape.points[ip][0]
       y_lat[ip] = shape.points[ip][1]

   dist = np.sqrt(x_lon**2 + y_lat**2)
   dist_d = np.abs(np.diff(dist))
   bad = np.where(dist_d > 5000)[0]
   x_lon[bad] = np.nan; y_lat[bad] = np.nan
   ax1.plot(x_lon, y_lat, 'k', color='lightgray', zorder=-999)
   ax1.text(-130000,-2750000, 'SW')
   ax1.text(-130000,-2125000, 'CW')
   ax1.text(-130000,-1750000, 'NW')
   ax1.text(-135000,-1200000, 'NO')
   ax1.text(260000,-1.35E6, 'NE')
   ax1.text(370000,-2.1E6, 'CE')
   ax1.text(200000,-2.5E6, 'SE')

ax1.scatter(gates['x'],gates['y'], marker='o', color='k', s=SIZE, label="0 % Invalid")
ax1.scatter(gates.loc[gates['all_bad'] == True, 'x'],
            gates.loc[gates['all_bad'] == True, 'y'],
            marker='v', color='r', s=SIZE, label="100 % Invalid")
ax1.scatter(gates.loc[(gates['bad'] > 0) & (gates['bad'] < 1), 'x'], # partial bad
            gates.loc[(gates['bad'] > 0) & (gates['bad'] < 1), 'y'],
            marker='s', color='b', s=SIZE, label="Partial")
ax1.set_yticklabels([])
ax1.set_xticklabels([])
ax1.legend(fontsize=10, markerscale=5)

MS=4

gs = gates.sort_values(by='regions')
gs.index = np.arange(gs['bad'].size)
ax2.plot(gs.loc[gs['bad'] == 1, 'bad']*100, '.', color='r', marker='v', markersize=MS, markerfacecolor='None')
ax2.plot(gs.loc[gs['bad']%1 != 0, 'bad']*100, '.', color='b', marker='s', markersize=MS, markerfacecolor='None')
ax2.plot(gs.loc[gs['bad'] == 0, 'bad']*100, '.', color='k')
ax2.set_ylabel('Invalid thickness\npixels in gate [%]')
ax2.set_xlabel('Gate [#]')

ax3.plot(gs['vel_gd'], '.', color='k', label='Gate pixels all good')
ax3.plot(gs['vel_part_bad'], '.', color='b', label='Gate pixels partial bad', marker='s', markersize=MS, markerfacecolor='None')
ax3.plot(gs['vel_all_bad'], '.', color='r', label='Gate pixels all bad', marker='v', markersize=MS, markerfacecolor='None')
ax3.set_ylabel('Gate average speed [m yr$^{-1}$]')
ax3.set_xlabel('Gate [#]')
ax3.set_yscale('log')
# ax3.legend()

# print region label on ax4. Shade ax4 and ax5 every 2nd region
for n,R in enumerate(gs['regions'].unique()):
    ax2.text(np.mean(gs[gs['regions'] == R].index), 50, R, horizontalalignment='center', rotation=90)
    for a in [ax2,ax3]:
        if n % 2 == 0:
            a.axvspan(gs[gs['regions'] == R].index[0],
                      gs[gs['regions'] == R].index[-1],
                      alpha=0.1, color='k')
             
print("Gates: ", gates.shape[0])
print("Gates w/ any bad pixels: ", gates[gates['bad'] > 0].shape[0])
print("Gates w/ >50% bad pixels: ", gates[gates['bad'] > 0.5].shape[0])
print("Gates w/ all bad pixels: ", gates[gates['bad'] == 1].shape[0])
print("Gates w/ all bad pixels: ", gates[gates['all_bad'] == True].shape[0])
    
plt.savefig('./figs/gate_map.png', transparent=True, bbox_inches='tight', dpi=300)
# plt.savefig('./figs/gate_map.pdf', transparent=True, bbox_inches='tight', dpi=300)

print(tmp[th['bad'] == 0]['vel'].describe())
print(tmp[th['bad'] > 0]['vel'].describe())
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
Gates:  276
Gates w/ any bad pixels:  89
Gates w/ >50% bad pixels:  65
Gates w/ all bad pixels:  61
Gates w/ all bad pixels:  61
/home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  warnings.warn("This figure includes Axes that are not compatible "
/home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  warnings.warn("This figure includes Axes that are not compatible "
count     5366.000000
mean       821.290794
std       1040.254586
min        100.116739
25%        229.867897
50%        487.465244
75%        972.494886
max      10044.146554
Name: vel, dtype: float64
count     636.000000
mean      265.870956
std       235.284065
min       100.521579
25%       128.675465
50%       171.290958
75%       281.274382
max      1423.016866
Name: vel, dtype: float64
/home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  warnings.warn("This figure includes Axes that are not compatible "
#+end_example
[[file:./.ob-jupyter/ee14e5e3da592eb54875dadec3aeeca04d77c5b7.png]]
:END:

**** Temporal changes in thickness
+ For each pixel, make daily time series of thickness changes from citet:khan_2016_geodetic
+ Then set 0 to GIMP.0715 date for each pixel
+ Then add them together
#+BEGIN_SRC jupyter-python :exports none :results raw drawer
dhdt_ts = dhdt.copy(deep=True)
# dhdt_ts[2016] = dhdt_ts[2015] # assume annual dh/dt continues at fixed rate
# dhdt_ts[2017] = dhdt_ts[2015]
# dhdt_ts[2018] = dhdt_ts[2015]
# dhdt_ts[2016] = 0
# dhdt_ts[2017] = 0
# dhdt_ts[2018] = 0
dhdt_ts[dhdt.columns.max()+1] = 0
dhdt_ts = dhdt_ts.reindex(sorted(dhdt_ts.columns), axis='columns')
dhdt_ts = dhdt_ts.cumsum(axis='columns')
dhdt_ts.columns = dhdt_ts.columns.astype(np.str).astype(np.datetime64)
dhdt_ts = dhdt_ts.T.resample('1D').interpolate().T
# from above: daily accumulated change at each discharge pixel for SEC time series

# expand from SEC 1st time to velocity 1st time, assuming no changes in SEC
dhdt_ts[dhdt_ts.columns.min()-1] = dhdt_ts[dhdt_ts.columns.min()]
dhdt_ts[vel.columns.min()] = dhdt_ts[dhdt_ts.columns.min()]
dhdt_ts = dhdt_ts.T.resample('1D').interpolate().T

th_ts = pd.DataFrame().reindex_like(dhdt_ts)
for idx in th.index:
    t0_pix = th.loc[idx]
    # adjusted thickness at t0_pix['day']
    t0_pix_th = t0_pix['fit'] + (th.loc[idx]['dem@GIMP.0715'] - th.loc[idx]['surface@BedMachine'])

    # now using that thickness, put it in a time Series
    dhdt_pix = dhdt_ts.loc[idx]
    dhdt_pix = dhdt_pix - dhdt_pix.loc[t0_pix['day']]
    th_ts.loc[idx] = t0_pix_th + dhdt_pix

th_ts = th_ts[vel.columns]

# Re-adjust th_ts where thickness < 20 m.
bad = th_ts.min(axis=1) < 20 # Pixels with bad thickness somewhere in the time series
for px in bad[bad == True].index:
    th_ts.loc[px] = fits.predict(np.log10([1,vel_baseline.iloc[px]]))
#+END_SRC

#+RESULTS:

*** Discharge

And more importantly and long-term, we calculate the following time series discharge products, using our preferred method (fill w/ 300 m):
+ D :: Discharge at gate scale
+ D_err :: The discharge error at gate scale
+ D_fill :: The fill percentage for each gate at each point in time
+ D_sector :: Same, but at Mouginot 2019 sector scale
+ D_sector_err ::
+ D_sector_fill :: 
+ D_region :: Same, but at Mouginot 2019 region scale
+ D_region_err ::
+ D_region_fill ::
+ D_all :: Same, but all GIS
+ D_all_err ::
+ D_all_fill ::

#+BEGIN_SRC jupyter-python :display text/plain :session test :tangle no
import pandas as pd
import numpy as np
filled_D = pd.DataFrame(index=['A','B'], columns=['t1','t3','t4'], data=[[8,9,7],[4,2,1]])
fill = filled_D/filled_D
fill.loc['B','t3'] = np.nan

no_filled_D = filled_D * fill
# filled_weighted_D = filled_D / filled_D.sum()
no_filled_weighted_D = no_filled_D / filled_D.sum()

r = ((filled_D*fill)/filled_D.sum()).sum()
r.round(2)                        
#+END_SRC

#+RESULTS:
: t1    1.00
: t3    0.82
: t4    1.00
: dtype: float64

#+NAME: discharge
#+BEGIN_SRC jupyter-python :exports none :results raw drawer
# D :: Discharge at pixel scale
# D_err :: The discharge error at pixel scale
# D_fill :: The fill percentage for each pixel at each point in time
D = (vel*th_ts).apply(lambda c: c * (200 * meta['err_2D'].values), axis=0) * 917 / 1E12
# Don't adjust thickness over time
# D = (vel).apply(lambda c: c * (200 * meta['err_2D'].values * th['thick'].values), axis=0) * 917 / 1E12

D_err = vel.apply(lambda c: c * (th['fit_err'] * 200 * meta['err_2D'].values), axis=0) * 917 / 1E12

[DD,DD_err] = [_.copy() for _ in [D,D_err]]
DD[['gates','sectors','regions','ones','name']] = meta[['gates','sectors','regions','ones','name']]
DD_err[['gates','sectors','regions','ones','name']] = meta[['gates','sectors','regions','ones','name']]


# D_gate :: Same, but at the gate scale
# D_gate_err ::
# D_gate_fill ::
D_gates = DD.groupby('gates').sum().drop(['ones','sectors'], axis=1)
D_gates_err = DD_err.groupby('gates').sum().drop(['ones','sectors'], axis=1)
D_gates_fill_weight = pd.DataFrame().reindex_like(D_gates)
for g in D_gates.index:
    g_idx = (DD['gates'] == g)
    D_gates_fill_weight.loc[g] = ((D[g_idx]*fill[g_idx])/D[g_idx].sum()).sum()

D_gates.columns = D_gates.columns.astype(np.str).astype(np.datetime64)
D_gates_err.columns = D_gates_err.columns.astype(np.str).astype(np.datetime64)
D_gates_fill_weight.columns = D_gates_fill_weight.columns.astype(np.str).astype(np.datetime64)
D_gates_fill_weight.clip(lower=0, upper=1, inplace=True)



# D_sector :: Same, but at Mouginot sector scale
# D_sector_err ::
# D_sector_fill ::
D_sectors = DD.groupby('name').sum().drop(['ones','sectors','gates'], axis=1)
D_sectors_err = DD_err.groupby('name').sum().drop(['ones','sectors','gates'], axis=1)
D_sectors_fill_weight = pd.DataFrame().reindex_like(D_sectors)
for s in D_sectors.index:
    s_idx = (DD['name'] == s)
    D_sectors_fill_weight.loc[s] = ((D[s_idx]*fill[s_idx])/D[s_idx].sum()).sum()

D_sectors.columns = D_sectors.columns.astype(np.str).astype(np.datetime64)
D_sectors_err.columns = D_sectors_err.columns.astype(np.str).astype(np.datetime64)
D_sectors_fill_weight.columns = D_sectors_fill_weight.columns.astype(np.str).astype(np.datetime64)
D_sectors_fill_weight.clip(lower=0, upper=1, inplace=True)


# D_region :: Same, but at Mouginot region scale
# D_region_err ::
# D_region_fill ::
D_regions = DD.groupby('regions').sum().drop(['ones','sectors','gates'], axis=1)
D_regions_err = DD_err.groupby('regions').sum().drop(['ones','sectors','gates'], axis=1)
D_regions_fill_weight = pd.DataFrame().reindex_like(D_regions)
for r in D_regions.index:
    r_idx = DD['regions'] == r
    D_regions_fill_weight.loc[r] = ((D[r_idx]*fill[r_idx])/D[r_idx].sum()).sum()
    
    # # or, broken apart into simple steps.
    # # Whether any given pixel is filled (1) or not (0).
    # r_fill = fill[DD['regions'] == r].fillna(value=0)
    # # Discharge for each pixel in this region, using filling
    # r_filled_D = DD[DD['regions'] == r].drop(['sectors','regions','ones'], axis=1)
    # # weighted filling for this region
    # r_fill_weight = ((r_filled_D*r_fill)/r_filled_D.sum()).sum()
    # D_regions_fill_weight.loc[r] = r_fill_weight
    
D_regions.columns = D_regions.columns.astype(np.str).astype(np.datetime64)
D_regions_err.columns = D_regions_err.columns.astype(np.str).astype(np.datetime64)
D_regions_fill_weight.columns = D_regions_fill_weight.columns.astype(np.str).astype(np.datetime64)
D_regions_fill_weight.clip(lower=0, upper=1, inplace=True)


# D_all :: Same, but all GIS
# D_all_err ::
# D_all_fill ::
D_all = DD.drop(['regions','sectors','ones','name','gates'], axis=1).sum()
D_all_err = DD_err.drop(['regions','sectors','ones','name','gates'], axis=1).sum()
D_all_fill_weight = pd.Series().reindex_like(D_all)
for c in D.columns:
    D_all_fill_weight.loc[c] = (fill[c] * (D[c] / D[c].sum())).sum()
   
#+END_SRC

#+RESULTS: discharge
: NoAdj        480.004070
: NoAdj_err     49.160007
: 300          488.156346
: 300_err       48.940213
: 400          494.564033
: 400_err       51.337163
: fit          492.210294
: fit_err       50.529233
: NoMillan     472.086164
: dtype: float64

*** SAVE & RESTORE STATE

#+BEGIN_SRC jupyter-python :results raw drawer :tangle no
%store D
%store D_err
%store fill
%store D_gates
%store D_gates_err
%store D_gates_fill_weight
%store D_sectors
%store D_sectors_err
%store D_sectors_fill_weight
%store D_regions
%store D_regions_err
%store D_regions_fill_weight
%store D_all
%store D_all_err
%store D_all_fill_weight
%store meta
#+END_SRC

#+RESULTS:
:results:
# Out[796]:
# output
Stored 'D' (DataFrame)
Stored 'D_err' (DataFrame)
Stored 'fill' (DataFrame)
Stored 'D_gates' (DataFrame)
Stored 'D_gates_err' (DataFrame)
Stored 'D_gates_fill_weight' (DataFrame)
Stored 'D_sectors' (DataFrame)
Stored 'D_sectors_err' (DataFrame)
Stored 'D_sectors_fill_weight' (DataFrame)
Stored 'D_regions' (DataFrame)
Stored 'D_regions_err' (DataFrame)
Stored 'D_regions_fill_weight' (DataFrame)
Stored 'D_all' (Series)
Stored 'D_all_err' (Series)
Stored 'D_all_fill_weight' (Series)
Stored 'meta' (DataFrame)

:end:

#+BEGIN_SRC jupyter-python :results raw drawer :tangle no
%store -r

D = D.T['2000':].T
D_err = D_err.T['2000':].T
fill = fill.T['2000':].T
D_gates = D_gates.T['2000':].T
D_gates_err = D_gates_err.T['2000':].T
D_gates_fill_weight = D_gates_fill_weight.T['2000':].T
D_sectors = D_sectors.T['2000':].T
D_sectors_err = D_sectors_err.T['2000':].T
D_sectors_fill_weight = D_sectors_fill_weight.T['2000':].T
D_regions = D_regions.T['2000':].T
D_regions_err = D_regions_err.T['2000':].T
D_regions_fill_weight = D_regions_fill_weight.T['2000':].T
D_all = D_all.T['2000':].T
D_all_err = D_all_err.T['2000':].T
D_all_fill_weight = D_all_fill_weight.T['2000':].T
#+END_SRC

#+RESULTS:
:results:
# Out[278]:
:end:


*** Export Data
**** Crop time series
#+BEGIN_SRC jupyter-python :exports both :results raw drawer

STARTDATE='1986'
D_all = D_all.T[STARTDATE:].T
D_all_err = D_all_err.T[STARTDATE:].T
D_all_fill_weight = D_all_fill_weight.T[STARTDATE:].T
D_gates = D_gates.T[STARTDATE:].T
D_gates_err = D_gates_err.T[STARTDATE:].T
D_gates_fill_weight = D_gates_fill_weight.T[STARTDATE:].T
D_sectors = D_sectors.T[STARTDATE:].T
D_sectors_err = D_sectors_err.T[STARTDATE:].T
D_sectors_fill_weight = D_sectors_fill_weight.T[STARTDATE:].T
D_regions = D_regions.T[STARTDATE:].T
D_regions_err = D_regions_err.T[STARTDATE:].T
D_regions_fill_weight = D_regions_fill_weight.T[STARTDATE:].T
D_all = D_all.T[STARTDATE:].T
D_all_err = D_all_err.T[STARTDATE:].T
D_all_fill_weight = D_all_fill_weight.T[STARTDATE:].T

#+END_SRC

#+RESULTS:

**** README

#+BEGIN_SRC org :tangle ./out/README.txt :mkdirp ./out
README for "Greenland Ice Sheet solid ice discharge from 1986 through 2018"

Paper Citation: TODO

Original Paper: doi:10.5194/essd-11-769-2019

Data Citation: TODO

Original Data Citations: doi:10.22008/promice/data/ice_discharge

Source: https://github.com/mankoff/ice_discharge

,* Usage instructions:

When using any of the following data, you are required to cite the paper and the data set.

,* Data Descriptions

Data sets released as part of this work include:
+ Discharge data
+ Gates
+ Surface Elevation Change
+ Code

Each are described briefly below.

,** Discharge Data

This data set is made up of the following files

| Filename            | Description                                           |
|---------------------+-------------------------------------------------------|
| GIS_D.csv           | Greenland Ice Sheet cumulative discharge by timestamp |
| GIS_err.csv         | Errors for GIS_D.csv                                  |
| GIS_coverage.csv    | Coverage for GIS_D.csv                                |
| region_D.csv        | Regional discharge                                    |
| region_err.csv      | Errors for region_D.csv                               |
| region_coverage.csv | Coverage for region_D.csv                             |
| sector_D.csv        | Sector discharge                                      |
| sector_err.csv      | Errors for sector_D.csv                               |
| sector_coverage.csv | Coverage for sector_D.csv                             |
| gate_D.csv          | Gate discharge                                        |
| gate_err.csv        | Errors for gate_D.csv                                 |
| gate_coverage.csv   | Coverage for gate_D.csv                               |
|---------------------+-------------------------------------------------------|
| gate_meta.csv       | Metadata for each gate                                |


D and err data have units [Gt yr-1].
Coverage is in range [0, 1]

,** Gates

| Filename   | Description                                   |
|------------+-----------------------------------------------|
| gates.kml  | KML file of gate location and metadata        |
| gates.gpkg | GeoPackage file of gate location and metadata |

,** Surface Elevation Change

The "surface_elevation_change" file set contains the surface elevation change data used in this work (DOI 10.22008/promice/data/DTU/surface_elevation_change/v1.0.0)

,** Code

The "code" file set (DOI 10.22008/promice/data/ice_discharge/code/v0.0.1) contains the digital workbook that produced the data, the ESSD document text and figures, this README, and everything else associated with this work.
#+END_SRC

**** Gates
***** D, err, coverage
#+BEGIN_SRC jupyter-python :exports both :results raw drawer
D_gatesT = D_gates.T
D_gates_errT = D_gates_err.T
D_gates_fill_weightT = D_gates_fill_weight.T

D_gatesT.index.name = "Date"
D_gates_errT.index.name = "Date"
D_gates_fill_weightT.index.name = "Date"

D_gatesT.to_csv('./out/gate_D.csv')
D_gates_errT.to_csv('./out/gate_err.csv')
D_gates_fill_weightT.to_csv('./out/gate_coverage.csv')
#+END_SRC

#+RESULTS:
:results:
# Out[118]:
:end:

**** Sectors
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
# meta_sector = pd.DataFrame(index=meta.groupby('sectors').first().index)
# meta_sector['mean x'] = meta.groupby('sectors').mean()['x'].round().astype(np.int)
# meta_sector['mean y'] = meta.groupby('sectors').mean()['y'].round().astype(np.int)
# meta_sector['n gates'] = meta.groupby('sectors').count()['gates'].round().astype(np.int)
# meta_sector['region'] = meta.groupby('sectors').first()['regions']

D_sectorsT = D_sectors.T
D_sectors_errT = D_sectors_err.T
D_sectors_fill_weightT = D_sectors_fill_weight.T

D_sectorsT.index.name = "Date"
D_sectors_errT.index.name = "Date"
D_sectors_fill_weightT.index.name = "Date"

# meta_sector.to_csv('./out/sector_meta.csv')
D_sectorsT.to_csv('./out/sector_D.csv')
D_sectors_errT.to_csv('./out/sector_err.csv')
D_sectors_fill_weightT.to_csv('./out/sector_coverage.csv')

# meta_sector.head(10)
#+END_SRC

**** Regions
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
# meta_region = pd.DataFrame(index=meta.groupby('regions').first().index)
# meta_region['n gates'] = meta.groupby('regions').count()['gates'].round().astype(np.int)

D_regionsT = D_regions.T
D_regions_errT = D_regions_err.T
D_regions_fill_weightT = D_regions_fill_weight.T
D_regionsT.index.name = "Date"
D_regions_errT.index.name = "Date"
D_regions_fill_weightT.index.name = "Date"

# meta_region.to_csv('./out/region_meta.csv')
D_regionsT.to_csv('./out/region_D.csv')
D_regions_errT.to_csv('./out/region_err.csv')
D_regions_fill_weightT.to_csv('./out/region_coverage.csv')

# meta_region.head(10)
#+END_SRC


**** GIS
#+BEGIN_SRC jupyter-python :exports both :results raw drawer
D_all.index.name = "Date"
D_all_err.index.name = "Date"
D_all_fill_weight.index.name = "Date"

D_all.to_csv('./out/GIS_D.csv', float_format='%.5f', header=["Discharge [Gt yr-1]"])
D_all_err.to_csv('./out/GIS_err.csv', float_format='%.5f', header=["Discharge Error [Gt yr-1]"])
D_all_fill_weight.to_csv('./out/GIS_coverage.csv', float_format='%.5f', header=["Coverage [unit]"])
#+END_SRC

#+RESULTS:
:results:
# Out[114]:
:end:

**** Gates
#+BEGIN_SRC sh :results verbatim :tangle gate_export.sh
g.mapset gates_100_5000

v.out.ogr input=gates_final output=./out/gates.kml format=KML --o
v.out.ogr input=gates_final output=./out/gates.gpkg format=GPKG --o
#+END_SRC
#+RESULTS:

**** Elevation change

Done manually. See DOI

**** TODO Code

Make sure this Org file is tidy enough...

**** TODO Distribute
#+BEGIN_SRC sh :results verbatim :tangle no
(cd out; zip -e /media/kdm/promicedata/ice_discharge/gates/gates.zip gates*)
(cd out; zip -e /media/kdm/promicedata/ice_discharge/d/D.zip D*csv)
cp ./out/README.txt /media/kdm/promicedata/ice_discharge/

zip -e /media/kdm/promicedata/ice_discharge/code/mankoff_et_al.zip ice_discharge.org

cp ${DATADIR}/Khan_2016/dhdt_1995-2015_GrIS.txt /media/kdm/promicedata/ice_discharge/surface_elevation_change

#+END_SRC
#+RESULTS:


** Figures
:PROPERTIES:
:header-args:jupyter-python+: :tangle figures.py
:header-args:jupyter-python+: :session sob_figures
:END:
*** Overview
:PROPERTIES:
:header-args:jupyter-python+: :tangle no
:END:
**** Top contributors
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
import pandas as pd

df = pd.read_csv('./out/sector_D.csv', index_col=0, parse_dates=True)
df = df.resample('1D').mean().\
     interpolate(method='time').\
     resample('A').mean()

top8 = df.iloc[-1,:].sort_values(ascending=False).head(9)
# names = {}
# for m in mouginot_names: names[m] = ""
# names["JAKOBSHAVN_ISBRAE"] = "Sermeq Kujalleq (Jakobshavn Isbræ)"
# names["KOGE_BUGT_C"] = "(Køge Bugt C)"
# names["ZACHARIAE_ISSTROM"] = "Zachariae Isstrøm"
# names["RINK_ISBRAE"] = 'Kangilliup Sermia (Rink Isbræ)',
# names["NIOGHALVFJERDSFJORDEN"] = "(Nioghalvfjerdsfjorden)"

meta = pd.read_csv("./out/gate_meta.csv")

out = pd.DataFrame(index=top8.index, columns=["x","y"])
for name in out.index:
    out.loc[name]["x"] = meta[meta["Mouginot_2019"] == name]["mean_x"].values[0]
    out.loc[name]["y"] = meta[meta["Mouginot_2019"] == name]["mean_y"].values[0]

out.to_csv('./tmp/overview_D.csv')
out
#+END_SRC

#+RESULTS:
|                       |       x |            y |
|-----------------------+---------+--------------|
| JAKOBSHAVN_ISBRAE     | -181992 |  -2.2678e+06 |
| HELHEIMGLETSCHER      |  303757 | -2.57624e+06 |
| KANGERLUSSUAQ         |  484506 | -2.28798e+06 |
| KOGE_BUGT_C           |  182821 |  -2.7211e+06 |
| ZACHARIAE_ISSTROM     |  487875 | -1.10175e+06 |
| NIOGHALVFJERDSFJORDEN |  439187 |  -1.0736e+06 |
| RINK_ISBRAE           | -224240 | -1.97637e+06 |
| IKERTIVAQ_M           |  227847 | -2.67781e+06 |
| PETERMANN_GLETSCHER   | -261888 |      -996839 |

**** Prep data
#+BEGIN_SRC sh :results verbatim
grass -c ./G/fig_overview
g.region -d
r.mask -r


###
### vel
###
r.mask mask_GIC@Mouginot_2019 --o
r.mapcalc "vel = vel_baseline@MEaSUREs.0478" --o
r.mask -r

r.mapcalc "vel2 = if(vel < 100, null(), vel)" --o
# r.mapcalc "vel2 = if(vel2 > max(vel2)*0.5, max(vel2)*0.5, vel2)" --o
# r.colors -g map=vel2 color=oranges

cat << EOF | r.colors map=vel2 rules=-
0 255:255:255
99 255:255:255
100 255:235:215
500 255:128:0
15000 255:128:0
nv 255:255:255
default 255:255:255
EOF

g.copy vector=sectors@Mouginot_2019,sectors --o
g.copy vector=regions@Mouginot_2019,regions --o
g.copy vector=gates_final@gates_100_5000,gates --o


### get baseline discharge at each gate
v.in.ascii input=./tmp/overview_D.csv output=overview_D separator=, skip=1 x=2 y=3 --o


###
### Ocean Basemap
###
r.mask -i mask@BedMachine
g.region res=1000
r.relief input=bed@BedMachine output=ocean_relief zscale=30  --o
# r.colors map=bed@BedMachine color=srtm_plus
r.colors -ne map=bed@BedMachine color=water
r.shade shade=ocean_relief color=bed@BedMachine output=ocean_shade --o
g.region -d


r.mask -r 
g.region res=2500
r.relief input=bed@BedMachine output=land_relief zscale=30  --o
r.mapcalc "land_relief = land_relief" --o
# r.colors map=bed@BedMachine color=srtm_plus
# r.shade shade=land_relief color=bed@BedMachine output=land_shade --o
g.region -d




g.region -d
r.mask -r
g.copy mask_ice@BedMachine,ice --o
# r.mapcalc "ice = if(isnull(ice), 2, 1)" --o
cat << EOF | r.colors map=ice rules=-
# 2 0:0:0
1 255:255:255
EOF



r.mask -r
d.mon start=wx0
d.erase
d.rast land_relief
d.rast ocean_shade
# d.rast ocean_relief
# d.rast land_shade
d.rast ice
d.rast vel2


## now patch them all to there R,G,B channels so we can produce a single ps.map raster.

g.region res=1000
r.mapcalc --o <<EOF
ocean.r = if(isnull(ocean_shade), null(), r#ocean_shade)
ocean.g = if(isnull(ocean_shade), null(), g#ocean_shade)
ocean.b = if(isnull(ocean_shade), null(), b#ocean_shade)
land.r = if(isnull(land_relief), null(), r#land_relief)
land.g = if(isnull(land_relief), null(), g#land_relief)
land.b = if(isnull(land_relief), null(), b#land_relief)
ice.r = if(isnull(ice), null(), r#ice)
ice.g = if(isnull(ice), null(), g#ice)
ice.b = if(isnull(ice), null(), b#ice)
vel.r = if(isnull(vel2), null(), r#vel2)
vel.g = if(isnull(vel2), null(), g#vel2)
vel.b = if(isnull(vel2), null(), b#vel2)
EOF

r.patch -s input=vel.r,ice.r,ocean.r,land.r output=rr --o
r.patch -s input=vel.g,ice.g,ocean.g,land.g output=gg --o
r.patch -s input=vel.b,ice.b,ocean.b,land.b output=bb --o

r.colors map=rr,gg,bb color=grey

d.erase
d.rgb -n red=rr green=gg blue=bb


g.region res=1000
cat << EOF | ps.map input=- output=./figs/psmap/BASE.ps --o
border n
rgb rr gg bb
EOF
g.region -d
# convert ${CONVERTOPTS} -transparent white ./figs/psmap/vel.ps ./figs/psmap/vel.png



# gates at each of the top 7 dischargers
# for row in $(db.select -c table=overview_D | tr ' ' '_' | tac); do
g.region -d
r.mapcalc "vel3 = vel2" --o

# r.colors map=vel3 color=oranges
cat << EOF | r.colors map=vel3 rules=-
0 255:255:255
99 255:255:255
100 255:235:215
10000 255:128:0
20000 255:128:0
nv 255:255:255
default 255:255:255
EOF



mkdir -p ./figs/psmap
id=$(db.select -c sql='select cat from overview_D'|head -n1) # DEBUG
for id in $(db.select -c sql='select cat from overview_D'); do
  xy=$(db.select -c sql="select int_1,int_2 from overview_D where cat == '${id}'")
  x=$(echo ${xy}|cut -d"|" -f1)
  y=$(echo ${xy}|cut -d"|" -f2)
  g.region w=$x s=$y e=$(( $x + 1 )) n=$(( $y + 1))
  g.region w=w-15000 s=s-15000 e=e+15000 n=n+15000 res=200
  g.region save=${id} --o

  cat << EOF | ps.map input=- output=./figs/psmap/${id}.ps --o
border n
raster vel3
vareas gates
  color black
  end
EOF
  convert -trim ./figs/psmap/${id}.{ps,png}
done


# overview map w/ boxes
# db.select -c sql='select cat from overview_D'
# 1 through 8

g.region -d res=1000
cat << EOF | ps.map input=- output=./figs/psmap/BASE_boxes.ps --o
border n
rgb rr gg bb
region 1
  color black
  width 1
  end  
region 2
  color black
  width 1
  end  
region 3
  color black
  width 1
  end  
region 4
  color black
  width 1
  end  
region 5
  color black
  width 1
  end  
region 6
  color black
  width 1
  end  
region 7
  color black
  width 1
  end  
region 8
  color black
  width 1
  end  
EOF
g.region -d
 

#+END_SRC
#+RESULTS:
**** ps.map overview
+ create named regions with
  + d.mon start=wx0
  + d.rast vel
  + d.rast gates color=red
  + zoom, then "set computational region extent from display"
  + then "g.region save=<name>"

#+BEGIN_SRC sh :results verbatim
g.mapset fig_overview
g.list type=region mapset=.
# g.remove -f type=region pattern=*
#+END_SRC
#+RESULTS:


#+BEGIN_SRC PSMAP :tangle overview.psmap
border n

rgb rr gg bb

# raster vel2
#+END_SRC

**** ps.map inset

#+BEGIN_SRC PSMAP :tangle overview_inset.psmap
border n

raster vel3

vareas gates
  color black
  end

end
#+END_SRC

**** Run psmap
#+BEGIN_SRC sh :results verbatim
g.region -d res=5000 -pa
ps.map input=overview.psmap output=overview.ps --o; o overview.ps
# ps.map -e input=overview.psmap output=overview.eps --o; o overview.eps

region=$(g.list type=region mapset=.|head -n1) # DEBUG
for region in $(g.list type=region mapset=.); do
  g.region ${region}
  g.region res=100
  ps.map input=overview_inset.psmap output=${region}.ps --o; 
  # o ${region}.ps
  convert -trim -transparent white ${region}.ps ./figs/psmap/${region}.png
  rm ${region}.ps
  # o ./figs/psmap/${region}.png
done
#+END_SRC
#+RESULTS:
**** Configure in Inkscape

Manually align and adjust in Inkscape, add text, boxes, etc. then...

#+BEGIN_SRC sh :results verbatim
inkscape -z ./figs/overview.svg -e ./figs/overview.png
#+END_SRC
#+RESULTS:

*** Discharge Time Series - GIS
#+NAME: fig:discharge_ts
#+BEGIN_SRC jupyter-python :results raw drawer :display text/org
import matplotlib.gridspec as gridspec
import numpy as np
import pandas as pd
from adjust_spines import adjust_spines as adj
import matplotlib.pyplot as plt
import datetime as dt

plt.close(1)

fig = plt.figure(1, figsize=(9,5)) # w,h
fig.clf()
grid = plt.GridSpec(2, 1, height_ratios=[1,5], hspace=0.1) # h, w

ax_D = fig.add_subplot(grid[1,:])
ax_coverage = fig.add_subplot(grid[0,:], sharex=ax_D)

adj(ax_D, ['left','bottom'])
adj(ax_coverage, ['left'])
ax_coverage.minorticks_off()
ax_coverage.tick_params(length=0, which='both', axis='x')

D = pd.read_csv("./out/GIS_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/GIS_err.csv", index_col=0, parse_dates=True)
coverage = pd.read_csv("./out/GIS_coverage.csv", index_col=0, parse_dates=True)

MS=4
ax_D.errorbar(err.index, D.values, fmt='o', mfc='none', mec='k', ms=MS)
for i in np.arange(D.values.size):
    ax_D.errorbar(D.index[i], D.values[i],
                  yerr=err.values[i], ecolor='grey',
                  alpha=coverage.values[i],
                  marker='o', ms=MS, mfc='k', mec='k')

# Take annual average from daily interpolated rather than the existing samples.
D_day_year = D.resample('1D').mean().interpolate(method='time',limit_area='inside',axis=0).resample('A').mean()
err_day_year = err.resample('1D').mean().interpolate(method='time',limit_area='inside',axis=0).resample('A').mean()

Z=99
D_day_year.plot(drawstyle='steps', linewidth=3, ax=ax_D, alpha=0.75, color='orange', zorder=Z)

ax_D.legend("", framealpha=0)
ax_D.set_xlabel('Time [Years]')
ax_D.set_ylabel('Discharge [Gt yr$^{-1}$]')

import matplotlib.dates as mdates
ax_D.xaxis.set_major_locator(mdates.YearLocator())
ax_D.minorticks_off()
ax_D.xaxis.set_tick_params(rotation=66)
ax_D.set_xlim(np.array(ax_D.get_xlim())+[365,-365])

###
### Coverage
###

ax_coverage.plot(coverage.index, coverage.values*100,
                  color='k',
                  alpha=0.25,
                  # linewidth=3,
                  drawstyle='steps')
ax_coverage.set_ylim(-1,101)
ax_coverage.set_yticks([0,100])
ax_coverage.set_ylabel('Coverage [%]')    

plt.savefig('./figs/discharge_ts.png', transparent=True, bbox_inches='tight', dpi=300)
# plt.savefig('./figs/discharge_ts.pdf', box_inches='tight', dpi=300)

disp = pd.DataFrame(data = {'D':D_day_year.values.flatten(), 'err':err_day_year.values.flatten()},
                    index = D_day_year.index.year)
disp.index.name = 'Year'
disp
#+END_SRC

#+RESULTS: fig:discharge_ts
| Year |       D |     err |
|------+---------+---------|
| 1986 | 437.168 | 43.3904 |
| 1987 | 433.425 | 42.8831 |
| 1988 | 428.625 | 42.6298 |
| 1989 | 430.938 | 42.8525 |
| 1990 | 426.959 | 42.5263 |
| 1991 | 424.506 | 42.3046 |
| 1992 | 426.478 | 42.5487 |
| 1993 | 428.188 |  42.623 |
| 1994 |  426.24 |  42.904 |
| 1995 | 420.541 | 42.4104 |
| 1996 | 427.194 | 43.3595 |
| 1997 | 432.996 | 44.0224 |
| 1998 | 434.877 |  43.943 |
| 1999 | 445.702 | 44.8418 |
| 2000 | 451.501 | 44.7395 |
| 2001 | 451.201 |  44.504 |
| 2002 | 467.996 | 45.9138 |
| 2003 | 475.938 | 46.7059 |
| 2004 | 488.322 | 48.1511 |
| 2005 | 502.878 | 48.9798 |
| 2006 | 491.059 | 48.1582 |
| 2007 | 483.344 | 47.6454 |
| 2008 | 484.462 | 48.1241 |
| 2009 | 490.686 | 48.9171 |
| 2010 | 495.196 | 49.5888 |
| 2011 | 497.124 | 49.9296 |
| 2012 | 494.389 | 50.0723 |
| 2013 | 500.333 | 50.4275 |
| 2014 | 501.838 | 50.6399 |
| 2015 | 501.168 | 50.3744 |
| 2016 | 496.311 | 49.9853 |
| 2017 | 503.459 | 50.2416 |
| 2018 | 495.607 | 49.2709 |
| 2019 | 499.627 | 50.3993 |


**** Filtered v. unfiltered
| Year | filtered | unfiltered | diff [Gt] |    diff [%] |
|------+----------+------------+-----------+-------------|
| 1986 |  437.954 |    467.434 |     29.48 |  -6.7313005 |
| 1987 |  434.214 |    457.426 |    23.212 |  -5.3457512 |
| 1988 |  429.421 |    448.697 |    19.276 |  -4.4888350 |
| 1989 |   431.74 |    460.618 |    28.878 |  -6.6887479 |
| 1990 |  427.765 |    457.693 |    29.928 |  -6.9963648 |
| 1991 |  425.336 |     431.99 |     6.654 |  -1.5644103 |
| 1992 |  427.346 |    430.197 |     2.851 | -0.66714091 |
| 1993 |  429.094 |    433.901 |     4.807 |  -1.1202674 |
| 1994 |  427.136 |    436.287 |     9.151 |  -2.1424090 |
| 1995 |  421.375 |    421.163 |    -0.212 | 0.050311480 |
| 1996 |  427.977 |    423.235 |    -4.742 |   1.1080035 |
| 1997 |  433.854 |    429.534 |     -4.32 |  0.99572667 |
| 1998 |  435.777 |    437.337 |      1.56 | -0.35798126 |
| 1999 |  446.558 |    451.861 |     5.303 |  -1.1875277 |
| 2000 |  452.377 |    461.442 |     9.065 |  -2.0038596 |
| 2001 |  452.084 |    457.837 |     5.753 |  -1.2725511 |
| 2002 |  468.846 |    475.337 |     6.491 |  -1.3844631 |
| 2003 |   476.82 |    497.393 |    20.573 |  -4.3146261 |
| 2004 |  489.334 |    499.104 |      9.77 |  -1.9965913 |
| 2005 |  504.017 |    513.761 |     9.744 |  -1.9332681 |
| 2006 |  492.232 |    497.707 |     5.475 |  -1.1122804 |
| 2007 |  484.564 |    495.699 |    11.135 |  -2.2979421 |
| 2008 |  485.528 |        493 |     7.472 |  -1.5389432 |
| 2009 |  492.206 |    496.611 |     4.405 | -0.89495049 |
| 2010 |  496.914 |     503.48 |     6.566 |  -1.3213554 |
| 2011 |  498.946 |    518.767 |    19.821 |  -3.9725742 |
| 2012 |  496.274 |    502.503 |     6.229 |  -1.2551534 |
| 2013 |  502.511 |    518.619 |    16.108 |  -3.2055020 |
| 2014 |  503.128 |    508.178 |      5.05 |  -1.0037207 |
| 2015 |  501.615 |    506.752 |     5.137 |  -1.0240922 |
| 2016 |  494.365 |    500.796 |     6.431 |  -1.3008607 |
| 2017 |  499.647 |    502.387 |      2.74 | -0.54838716 |
#+TBLFM: $4=$3-$2::$5=100-($3/$2)*100

*** Discharge Time Series - Regions
#+NAME: fig:discharge_ts_regions
#+BEGIN_SRC jupyter-python :results raw drawer :display text/org
import matplotlib.gridspec as gridspec
import numpy as np
import pandas as pd
from adjust_spines import adjust_spines as adj
import datetime as dt

plt.close(1)

fig = plt.figure(1, figsize=(9,7)) # w,h
fig.clf()
# fig.set_tight_layout(True)
grid = plt.GridSpec(2, 1, height_ratios=[1,6], hspace=0.1) # h, w

ax_D = fig.add_subplot(grid[1,:])
ax_coverage = fig.add_subplot(grid[0,:], sharex=ax_D)

from adjust_spines import adjust_spines as adj
adj(ax_D, ['left','bottom'])
adj(ax_coverage, ['left'])
ax_coverage.minorticks_off()
ax_coverage.tick_params(length=0, which='both', axis='x')


D = pd.read_csv("./out/region_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/region_err.csv", index_col=0, parse_dates=True)
coverage = pd.read_csv("./out/region_coverage.csv", index_col=0, parse_dates=True)

### Take annual average from daily interpolated rather than the existing samples.
D_day_year = D.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()
err_day_year=err.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

MS=4
Z=99
for r in D.columns:
    e = ax_D.errorbar(D[r].index, D[r].values, fmt='o', mfc='none', ms=MS)
    C = e.lines[0].get_color()
    D_day_year[r].plot(drawstyle='steps', linewidth=2, ax=ax_D,
                       color=C,
                       # color='orange'
                       alpha=0.75, zorder=Z)
    for i in np.arange(D.index.size):
        ax_D.errorbar(D.iloc[i].name, D.iloc[i][r],
                      yerr=err.iloc[i][r], ecolor='gray',
                      marker='o', ms=MS,
                      # mfc='k', mec='k',
                      mfc=C, mec=C,
                      alpha=coverage.iloc[i][r])

    tx = D_day_year.iloc[-1].name + dt.timedelta(days=50)
    ty = D_day_year.iloc[-1][r]
    if r in ['CE', 'SW']: ty=ty-4
    if r == 'NE': ty=ty+4
    if r == 'NO': ty=ty-2
    ax_D.text(tx, ty, r, verticalalignment='center')

    if r in ['CE','NE','SE']:
        ax_coverage.plot(coverage[r]*100,
                         drawstyle='steps',
                         # alpha=0.5,
                         color=C)
        # print((D_coverage.loc[:,r]*100).describe(), '\n')

ax_coverage.set_ylabel('Coverage [%]')
ax_coverage.set_ylim([0,100])
    
import matplotlib.dates as mdates
ax_D.xaxis.set_major_locator(mdates.YearLocator())

# plt.legend()
ax_D.legend("", framealpha=0)
ax_D.set_xlabel('Time [Years]')
ax_D.set_ylabel('Discharge [Gt yr$^{-1}$]')
# ax_D.set_yscale('log')

ax_D.xaxis.set_tick_params(rotation=66)
ax_D.set_xlim(np.array(ax_D.get_xlim())+[365,-365])

plt.savefig('./figs/discharge_ts_regions.png', transparent=True, bbox_inches='tight', dpi=300)
# plt.savefig('./figs/discharge_ts_regions.pdf', transparent=True, bbox_inches='tight', dpi=300)

Err_pct = (err_day_year.values/D_day_year.values*100).round().astype(np.int).astype(np.str)
tbl = (D_day_year.round().astype(np.int).astype(np.str) + ' ('+Err_pct+')')
tbl.index = tbl.index.year.astype(np.str)
tbl.columns = [_ + ' (Err %)' for _ in tbl.columns]
tbl
#+END_SRC

#+RESULTS: fig:discharge_ts_regions
:RESULTS:
|   Date | CE (Err %)   | CW (Err %)   | NE (Err %)   | NO (Err %)   | NW (Err %)   | SE (Err %)   | SW (Err %)   |
|--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------|
|   1986 | 65 (10)      | 69 (9)       | 21 (8)       | 24 (7)       | 89 (8)       | 148 (12)     | 19 (13)      |
|   1987 | 65 (10)      | 67 (9)       | 21 (8)       | 24 (7)       | 88 (8)       | 148 (12)     | 19 (13)      |
|   1988 | 65 (10)      | 66 (8)       | 22 (8)       | 23 (7)       | 88 (8)       | 146 (12)     | 19 (13)      |
|   1989 | 67 (10)      | 68 (8)       | 22 (8)       | 24 (7)       | 88 (8)       | 143 (12)     | 19 (13)      |
|   1990 | 67 (10)      | 66 (8)       | 22 (8)       | 22 (7)       | 88 (8)       | 143 (12)     | 19 (13)      |
|   1991 | 69 (10)      | 64 (8)       | 22 (8)       | 21 (7)       | 88 (8)       | 141 (12)     | 19 (13)      |
|   1992 | 70 (10)      | 63 (8)       | 22 (8)       | 22 (7)       | 91 (8)       | 140 (12)     | 19 (13)      |
|   1993 | 69 (10)      | 64 (8)       | 22 (8)       | 22 (7)       | 93 (8)       | 139 (12)     | 19 (13)      |
|   1994 | 66 (10)      | 65 (8)       | 22 (8)       | 23 (9)       | 92 (8)       | 140 (12)     | 18 (13)      |
|   1995 | 64 (10)      | 65 (8)       | 22 (8)       | 23 (8)       | 90 (8)       | 139 (12)     | 18 (13)      |
|   1996 | 65 (10)      | 66 (8)       | 22 (8)       | 22 (7)       | 89 (8)       | 143 (13)     | 19 (13)      |
|   1997 | 67 (10)      | 66 (8)       | 23 (8)       | 22 (7)       | 90 (8)       | 145 (13)     | 20 (13)      |
|   1998 | 68 (10)      | 71 (8)       | 23 (8)       | 23 (7)       | 89 (8)       | 142 (13)     | 19 (13)      |
|   1999 | 68 (10)      | 76 (8)       | 23 (8)       | 25 (7)       | 90 (8)       | 145 (12)     | 19 (14)      |
|   2000 | 67 (10)      | 80 (8)       | 24 (8)       | 23 (7)       | 91 (8)       | 146 (12)     | 21 (14)      |
|   2001 | 68 (10)      | 82 (8)       | 26 (8)       | 23 (7)       | 91 (8)       | 141 (12)     | 20 (14)      |
|   2002 | 69 (10)      | 82 (8)       | 29 (8)       | 25 (7)       | 92 (8)       | 150 (12)     | 21 (14)      |
|   2003 | 70 (10)      | 85 (8)       | 26 (8)       | 23 (7)       | 93 (8)       | 158 (11)     | 21 (14)      |
|   2004 | 74 (10)      | 84 (8)       | 24 (8)       | 24 (7)       | 96 (9)       | 166 (11)     | 21 (15)      |
|   2005 | 85 (9)       | 84 (8)       | 25 (8)       | 25 (7)       | 97 (9)       | 166 (11)     | 21 (15)      |
|   2006 | 84 (9)       | 87 (8)       | 24 (8)       | 23 (7)       | 97 (9)       | 155 (12)     | 21 (15)      |
|   2007 | 79 (9)       | 87 (9)       | 25 (8)       | 24 (8)       | 97 (9)       | 150 (12)     | 20 (15)      |
|   2008 | 78 (10)      | 87 (9)       | 25 (8)       | 24 (8)       | 99 (9)       | 152 (12)     | 19 (15)      |
|   2009 | 77 (10)      | 90 (9)       | 25 (8)       | 23 (7)       | 101 (9)      | 156 (12)     | 20 (15)      |
|   2010 | 76 (10)      | 90 (9)       | 25 (8)       | 24 (7)       | 103 (9)      | 157 (12)     | 20 (16)      |
|   2011 | 78 (10)      | 89 (9)       | 25 (8)       | 25 (7)       | 107 (9)      | 153 (12)     | 21 (15)      |
|   2012 | 76 (10)      | 94 (9)       | 26 (8)       | 24 (7)       | 105 (9)      | 150 (12)     | 20 (16)      |
|   2013 | 76 (10)      | 94 (9)       | 26 (8)       | 24 (7)       | 108 (9)      | 152 (12)     | 20 (14)      |
|   2014 | 75 (10)      | 93 (9)       | 27 (8)       | 26 (7)       | 112 (9)      | 150 (12)     | 20 (15)      |
|   2015 | 74 (10)      | 93 (9)       | 28 (8)       | 25 (7)       | 110 (9)      | 152 (12)     | 20 (15)      |
|   2016 | 72 (10)      | 92 (9)       | 28 (8)       | 25 (7)       | 113 (9)      | 146 (12)     | 19 (15)      |
|   2017 | 75 (9)       | 83 (9)       | 29 (8)       | 26 (7)       | 116 (9)      | 154 (12)     | 19 (15)      |
|   2018 | 76 (9)       | 77 (9)       | 30 (8)       | 27 (7)       | 115 (9)      | 151 (12)     | 19 (15)      |
|   2019 | 76 (10)      | 75 (9)       | 30 (8)       | 28 (8)       | 114 (9)      | 157 (12)     | 20 (15)      |:END:

*** Region & Catchment Contributions
:PROPERTIES:
:header-args:jupyter-python+: :tangle no
:END:

**** 2000 to 2017 mean
:PROPERTIES:
:ID:       eca400fe-cffc-4e6b-8fc1-694945093adb
:END:
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
D = pd.read_csv("./out/GIS_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/GIS_err.csv", index_col=0, parse_dates=True)

D = D[D.index.year >= 2000]\
    .resample('1D',axis='rows')\
    .mean()\
    .interpolate(method='time',limit_area='inside')\
    .mean()

err = err[err.index.year >= 2000]\
      .resample('1D',axis='rows')\
      .mean()\
      .interpolate(method='time',limit_area='inside')\
      .mean()

print(D.values)
print(err.values)
#+END_SRC

#+RESULTS:
: [488.44884769]
: [48.60211048]




**** Discharge time series
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
D = pd.read_csv("./out/GIS_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/GIS_err.csv", index_col=0, parse_dates=True)

D = D.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()
err = err.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

df = pd.DataFrame(index=D.index)
df['D'] = D
df['err'] = err

print("Min: ", df.loc[df['D'].argmin()])
print("Max: ", df.loc[df['D'].argmax()])
df
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
Min:  D      420.540625
err     42.410435
Name: 1995-12-31 00:00:00, dtype: float64
Max:  D      504.999339
err     50.511728
Name: 2018-12-31 00:00:00, dtype: float64
/home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'
will be corrected to return the positional minimum in the future.
Use 'series.values.argmin' to get the position of the minimum now.
  # This is added back by InteractiveShellApp.init_path()
/home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'
will be corrected to return the positional maximum in the future.
Use 'series.values.argmax' to get the position of the maximum now.
  if sys.path[0] == '':
#+end_example
| Date                |       D |     err |
|---------------------+---------+---------|
| 1986-12-31 00:00:00 | 437.168 | 43.3904 |
| 1987-12-31 00:00:00 | 433.425 | 42.8831 |
| 1988-12-31 00:00:00 | 428.625 | 42.6298 |
| 1989-12-31 00:00:00 | 430.938 | 42.8525 |
| 1990-12-31 00:00:00 | 426.959 | 42.5263 |
| 1991-12-31 00:00:00 | 424.506 | 42.3046 |
| 1992-12-31 00:00:00 | 426.478 | 42.5487 |
| 1993-12-31 00:00:00 | 428.188 | 42.623  |
| 1994-12-31 00:00:00 | 426.24  | 42.904  |
| 1995-12-31 00:00:00 | 420.541 | 42.4104 |
| 1996-12-31 00:00:00 | 427.194 | 43.3595 |
| 1997-12-31 00:00:00 | 432.996 | 44.0224 |
| 1998-12-31 00:00:00 | 434.877 | 43.943  |
| 1999-12-31 00:00:00 | 445.702 | 44.8418 |
| 2000-12-31 00:00:00 | 451.501 | 44.7395 |
| 2001-12-31 00:00:00 | 451.201 | 44.504  |
| 2002-12-31 00:00:00 | 467.996 | 45.9138 |
| 2003-12-31 00:00:00 | 475.938 | 46.7059 |
| 2004-12-31 00:00:00 | 488.322 | 48.1511 |
| 2005-12-31 00:00:00 | 502.878 | 48.9798 |
| 2006-12-31 00:00:00 | 491.059 | 48.1582 |
| 2007-12-31 00:00:00 | 483.344 | 47.6454 |
| 2008-12-31 00:00:00 | 484.462 | 48.1241 |
| 2009-12-31 00:00:00 | 490.687 | 48.9173 |
| 2010-12-31 00:00:00 | 495.197 | 49.5894 |
| 2011-12-31 00:00:00 | 497.126 | 49.9305 |
| 2012-12-31 00:00:00 | 494.392 | 50.0738 |
| 2013-12-31 00:00:00 | 500.327 | 50.429  |
| 2014-12-31 00:00:00 | 501.804 | 50.6444 |
| 2015-12-31 00:00:00 | 501.309 | 50.3954 |
| 2016-12-31 00:00:00 | 496.269 | 49.9868 |
| 2017-12-31 00:00:00 | 503.878 | 50.2857 |
| 2018-12-31 00:00:00 | 504.999 | 50.5117 |
:END:


**** Contribution from SE
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
D = pd.read_csv("./out/region_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/region_err.csv", index_col=0, parse_dates=True)

D = D.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()
err = err.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

print("D SE described:\n", D['SE'].describe())
print("\nD SE err percent:", (err['SE']/D['SE']*100).describe())

df = pd.DataFrame(index=D.index)
df['SE'] = D['SE']
df['SEerr'] = err['SE']
df
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
D SE described:
 count     33.000000
mean     148.775833
std        7.111016
min      138.758834
25%      142.972414
50%      148.249142
75%      152.878415
max      166.262388
Name: SE, dtype: float64

D SE err percent: count    33.000000
mean     11.914561
std       0.347710
min      11.367979
25%      11.683121
50%      11.848489
75%      12.088603
max      12.717230
Name: SE, dtype: float64
#+end_example
| Date                |      SE |   SEerr |
|---------------------+---------+---------|
| 1986-12-31 00:00:00 | 148.249 | 17.1599 |
| 1987-12-31 00:00:00 | 147.832 | 17.154  |
| 1988-12-31 00:00:00 | 145.674 | 17.0682 |
| 1989-12-31 00:00:00 | 143.292 | 16.978  |
| 1990-12-31 00:00:00 | 142.863 | 16.9946 |
| 1991-12-31 00:00:00 | 141.104 | 16.924  |
| 1992-12-31 00:00:00 | 139.889 | 16.9106 |
| 1993-12-31 00:00:00 | 139.108 | 16.9568 |
| 1994-12-31 00:00:00 | 139.642 | 17.0754 |
| 1995-12-31 00:00:00 | 138.759 | 17.0589 |
| 1996-12-31 00:00:00 | 142.972 | 18.0841 |
| 1997-12-31 00:00:00 | 145.35  | 18.4844 |
| 1998-12-31 00:00:00 | 141.826 | 17.9258 |
| 1999-12-31 00:00:00 | 145.235 | 18.0454 |
| 2000-12-31 00:00:00 | 146.081 | 17.4915 |
| 2001-12-31 00:00:00 | 140.549 | 16.8309 |
| 2002-12-31 00:00:00 | 150.129 | 17.4641 |
| 2003-12-31 00:00:00 | 157.618 | 18.0291 |
| 2004-12-31 00:00:00 | 165.713 | 18.9262 |
| 2005-12-31 00:00:00 | 166.262 | 18.9007 |
| 2006-12-31 00:00:00 | 155.219 | 18.0591 |
| 2007-12-31 00:00:00 | 150.38  | 17.5777 |
| 2008-12-31 00:00:00 | 151.96  | 17.8184 |
| 2009-12-31 00:00:00 | 155.669 | 18.187  |
| 2010-12-31 00:00:00 | 157.03  | 18.4568 |
| 2011-12-31 00:00:00 | 152.878 | 18.2838 |
| 2012-12-31 00:00:00 | 149.551 | 18.1449 |
| 2013-12-31 00:00:00 | 152.467 | 18.2772 |
| 2014-12-31 00:00:00 | 149.941 | 17.9267 |
| 2015-12-31 00:00:00 | 151.965 | 17.8598 |
| 2016-12-31 00:00:00 | 146.365 | 17.3233 |
| 2017-12-31 00:00:00 | 154.374 | 17.9661 |
| 2018-12-31 00:00:00 | 153.655 | 18.0683 |
:END:

**** Contribution from SE relative to the total
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
D = pd.read_csv("./out/region_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/region_err.csv", index_col=0, parse_dates=True)

D = D.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()
err = err.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

D_SE = D['SE']
# D_rest = D.drop(columns='SE').sum(axis='columns')
D_rest = D.sum(axis='columns')

print((D_SE/D_rest*100).describe())
print("")
print(D_SE/D_rest*100)
#+END_SRC

#+RESULTS:
#+begin_example
count    33.000000
mean     32.141980
std       1.343697
min      29.492955
25%      31.112378
50%      32.487698
75%      33.239571
max      34.107794
dtype: float64

Date
1986-12-31    33.911235
1987-12-31    34.107794
1988-12-31    33.986268
1989-12-31    33.251294
1990-12-31    33.460657
1991-12-31    33.239571
1992-12-31    32.801043
1993-12-31    32.487698
1994-12-31    32.761421
1995-12-31    32.995346
1996-12-31    33.467834
1997-12-31    33.568323
1998-12-31    32.613001
1999-12-31    32.585595
2000-12-31    32.354605
2001-12-31    31.150065
2002-12-31    32.079101
2003-12-31    33.117350
2004-12-31    33.935142
2005-12-31    33.062147
2006-12-31    31.609082
2007-12-31    31.112378
2008-12-31    31.366802
2009-12-31    31.724742
2010-12-31    31.710610
2011-12-31    30.752459
2012-12-31    30.249408
2013-12-31    30.473438
2014-12-31    29.880472
2015-12-31    30.313547
2016-12-31    29.492955
2017-12-31    30.637229
2018-12-31    30.426714
Freq: A-DEC, dtype: float64
#+end_example





**** Contribution from others relative to the total
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
D = pd.read_csv("./out/region_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/region_err.csv", index_col=0, parse_dates=True)

D = D.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()
err = err.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

print("\nNO+NE+NW:", (D[['NO','NE','NW']].sum(axis='columns')).describe()[['min','max']])

# print("\nERR NO+NE+NW:", (err[['NO','NE','NW']].sum(axis='columns')).describe()[['mean']])

print("\nNO+NE+NW%:", (D[['NO','NE','NW']].sum(axis='columns')/D.sum(axis='columns')*100).describe()[['mean']])

print("\nNW increase:", D['NW'])
      
# D_SE = D['SE']
# # D_rest = D.drop(columns='SE').sum(axis='columns')
# D_rest = D.sum(axis='columns')

# print((D_SE/D_rest*100).describe())
# print("")
# print(D_SE/D_rest*100)
#+END_SRC

#+RESULTS:
#+begin_example

NO+NE+NW: min    131.370560
max    173.853117
dtype: float64

NO+NE+NW%: mean    31.265545
dtype: float64

NW increase: Date
1986-12-31     89.266607
1987-12-31     87.887951
1988-12-31     87.755948
1989-12-31     88.003587
1990-12-31     87.970189
1991-12-31     88.318031
1992-12-31     90.820857
1993-12-31     93.255615
1994-12-31     92.216468
1995-12-31     89.532762
1996-12-31     89.010885
1997-12-31     90.087152
1998-12-31     89.374234
1999-12-31     89.848937
2000-12-31     91.000814
2001-12-31     91.267369
2002-12-31     92.300144
2003-12-31     93.070720
2004-12-31     95.848259
2005-12-31     96.652487
2006-12-31     97.336640
2007-12-31     97.167896
2008-12-31     98.775077
2009-12-31    100.553273
2010-12-31    103.437822
2011-12-31    106.913302
2012-12-31    104.927507
2013-12-31    107.567319
2014-12-31    111.865685
2015-12-31    109.958876
2016-12-31    112.876595
2017-12-31    115.845978
2018-12-31    116.952454
Freq: A-DEC, Name: NW, dtype: float64
#+end_example

**** Sermeq Kujalleq (Jakobshavn Isbræ)
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
D = pd.read_csv("./out/sector_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/sector_err.csv", index_col=0, parse_dates=True)

D = D['JAKOBSHAVN_ISBRAE'].resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

err = err['JAKOBSHAVN_ISBRAE'].resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

print("Jakobshavn described:\n", D.describe())

df = pd.DataFrame(index=D.index)
df['Jako'] = D
df['err'] = err
df
#+END_SRC

#+RESULTS:
:RESULTS:
: Jakobshavn described:
:  count    33.000000
: mean     36.314192
: std       9.775048
: min      22.612427
: 25%      25.573804
: 50%      37.077133
: 75%      44.844775
: max      51.502856
: Name: JAKOBSHAVN_ISBRAE, dtype: float64
| Date                |    Jako |     err |
|---------------------+---------+---------|
| 1986-12-31 00:00:00 | 28.4839 | 2.96734 |
| 1987-12-31 00:00:00 | 26.5038 | 2.72569 |
| 1988-12-31 00:00:00 | 25.5183 | 2.59409 |
| 1989-12-31 00:00:00 | 27.1045 | 2.72013 |
| 1990-12-31 00:00:00 | 25.5329 | 2.48419 |
| 1991-12-31 00:00:00 | 23.0622 | 2.2238  |
| 1992-12-31 00:00:00 | 22.6124 | 2.20058 |
| 1993-12-31 00:00:00 | 24.2377 | 2.30753 |
| 1994-12-31 00:00:00 | 25.3483 | 2.36384 |
| 1995-12-31 00:00:00 | 24.1369 | 2.22583 |
| 1996-12-31 00:00:00 | 25.4817 | 2.31369 |
| 1997-12-31 00:00:00 | 25.5738 | 2.30127 |
| 1998-12-31 00:00:00 | 29.1732 | 2.63433 |
| 1999-12-31 00:00:00 | 32.4494 | 2.88293 |
| 2000-12-31 00:00:00 | 35.8264 | 3.15287 |
| 2001-12-31 00:00:00 | 37.0771 | 3.25229 |
| 2002-12-31 00:00:00 | 38.567  | 3.3948  |
| 2003-12-31 00:00:00 | 40.8969 | 3.63303 |
| 2004-12-31 00:00:00 | 42.2548 | 3.79803 |
| 2005-12-31 00:00:00 | 43.4011 | 3.95143 |
| 2006-12-31 00:00:00 | 44.8035 | 4.12136 |
| 2007-12-31 00:00:00 | 44.8448 | 4.15351 |
| 2008-12-31 00:00:00 | 44.425  | 4.14756 |
| 2009-12-31 00:00:00 | 46.1771 | 4.31207 |
| 2010-12-31 00:00:00 | 46.6126 | 4.37313 |
| 2011-12-31 00:00:00 | 46.0907 | 4.34717 |
| 2012-12-31 00:00:00 | 51.5029 | 4.91911 |
| 2013-12-31 00:00:00 | 50.5796 | 4.92402 |
| 2014-12-31 00:00:00 | 48.9888 | 4.81342 |
| 2015-12-31 00:00:00 | 48.2833 | 4.81539 |
| 2016-12-31 00:00:00 | 47.9106 | 4.7594  |
| 2017-12-31 00:00:00 | 39.5751 | 3.96052 |
| 2018-12-31 00:00:00 | 35.3322 | 3.52634 |
:END:

*** Discharge Time Series - Sectors
#+BEGIN_SRC jupyter-python :results raw drawer :display text/org
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

plt.close(1)

fig = plt.figure(1, figsize=(9,5)) # w,h
fig.clf()
# fig.set_tight_layout(True)
grid = plt.GridSpec(2, 1, height_ratios=[1,10], hspace=0.1) # h, w


ax_D = fig.add_subplot(grid[1,:])
ax_coverage = fig.add_subplot(grid[0,:], sharex=ax_D)

from adjust_spines import adjust_spines as adj
adj(ax_D, ['left','bottom'])
adj(ax_coverage, ['left'])
ax_coverage.minorticks_off()
ax_coverage.tick_params(length=0, which='both', axis='x')


D = pd.read_csv("./out/sector_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/sector_err.csv", index_col=0, parse_dates=True)
coverage = pd.read_csv("./out/sector_coverage.csv", index_col=0, parse_dates=True)

### Take annual average from daily interpolated rather than the existing samples.
D_day_year = D.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()
err_day_year=err.resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

# largest average for last year
D_sort = D.resample('Y', axis='rows')\
          .mean()\
          .iloc[-1]\
          .sort_values(ascending=False)

LABELS={}
# for k in D_sort.head(8).index: LABELS[k] = k
# Use the last       ^ glaciers

# Make legend pretty
LABELS['JAKOBSHAVN_ISBRAE'] = 'Sermeq Kujalleq (Jakobshavn Isbræ)'
LABELS['HELHEIMGLETSCHER'] = 'Helheim Gletsjer'
LABELS['KANGERLUSSUAQ'] = 'Kangerlussuaq Gletsjer'
LABELS['KOGE_BUGT_C'] = '(Køge Bugt C)'
LABELS['ZACHARIAE_ISSTROM'] = 'Zachariae Isstrøm'
LABELS['RINK_ISBRAE'] = 'Kangilliup Sermia (Rink Isbræ)'
LABELS['NIOGHALVFJERDSFJORDEN'] = '(Nioghalvfjerdsbrae)'
LABELS['PETERMANN_GLETSCHER'] ='Petermann Gletsjer'

SYMBOLS={}
SYMBOLS['JAKOBSHAVN_ISBRAE'] = 'o'
SYMBOLS['HELHEIMGLETSCHER'] = 's'
SYMBOLS['KANGERLUSSUAQ'] = 'v'
SYMBOLS['KOGE_BUGT_C'] = '^'
SYMBOLS['NIOGHALVFJERDSFJORDEN'] = 'v'
SYMBOLS['RINK_ISBRAE'] = 's'
SYMBOLS['ZACHARIAE_ISSTROM'] = 'o'
SYMBOLS['PETERMANN_GLETSCHER'] ='^'

MS=4
Z=99
for g in LABELS.keys(): # for each glacier
    e = ax_D.errorbar(D.loc[:,g].index,
                      D.loc[:,g].values,
                      label=LABELS[g],
                      fmt=SYMBOLS[g],
                      mfc='none',
                      ms=MS)
    C = e.lines[0].get_color()
    D_day_year.loc[:,g].plot(drawstyle='steps', linewidth=2,
                             label='',
                             ax=ax_D,
                             alpha=0.75, color=C, zorder=Z)

    for i,idx in enumerate(D.loc[:,g].index):
        ax_D.errorbar(D.loc[:,g].index[i],
                      D.loc[:,g].values[i],
                      yerr=err.loc[:,g].values[i],
                      alpha=coverage.loc[:,g].values[i],
                      label='',
                      ecolor='grey',
                      mfc=C, mec=C,
                      marker='o', ms=MS)


    if g in ['NIOGHALVFJERDSFJORDEN', 'KANGERLUSSUAQ']: #, 'JAKOBSHAVN_ISBRAE']:
        ax_coverage.plot(D.loc[:,g].index,
                         coverage.loc[:,g].values*100,
                         drawstyle='steps',
                         # alpha=0.5,
                         color=C)

yl = ax_D.get_ylim()

import matplotlib.dates as mdates
ax_D.xaxis.set_major_locator(mdates.YearLocator())

ax_D.legend(fontsize=8, ncol=2, loc=(0.0, 0.82), fancybox=False, frameon=False)
ax_D.set_xlabel('Time [Years]')
ax_D.set_ylabel('Discharge [Gt yr$^{-1}$]')

ax_D.xaxis.set_tick_params(rotation=66)
ax_D.set_xlim(np.array(ax_D.get_xlim())+[365,-365])

ax_coverage.set_ylabel('Coverage [%]')
ax_coverage.set_ylim([0,100])


#plt.savefig('./figs/discharge_ts_topfew.svg', transparent=True, bbox_inches='tight', dpi=300)
plt.savefig('./figs/discharge_ts_topfew.png', transparent=True, bbox_inches='tight', dpi=300)
plt.savefig('./figs/discharge_ts_topfew.pdf', transparent=True, bbox_inches='tight', dpi=300)

Err_pct = (err_day_year / D_day_year*100).round().astype(np.int).astype(np.str)
Err_pct = Err_pct[list(LABELS.keys())]
tbl = D_day_year[list(LABELS.keys())].round().astype(np.int).astype(np.str) + ' (' + Err_pct+')'
tbl.index = tbl.index.year.astype(np.str)
tbl.columns = [_ + ' (%)' for _ in tbl.columns]
tbl
#+END_SRC

#+RESULTS:
:RESULTS:
|   Date | JAKOBSHAVN_ISBRAE (%)   | HELHEIMGLETSCHER (%)   | KANGERLUSSUAQ (%)   | KOGE_BUGT_C (%)   | ZACHARIAE_ISSTROM (%)   | RINK_ISBRAE (%)   | NIOGHALVFJERDSFJORDEN (%)   | PETERMANN_GLETSCHER (%)   |
|--------+-------------------------+------------------------+---------------------+-------------------+-------------------------+-------------------+-----------------------------+---------------------------|
|   1986 | 28 (10)                 | 32 (5)                 | 17 (5)              | 18 (6)            | 9 (9)                   | 12 (6)            | 11 (5)                      | 12 (4)                    |
|   1987 | 27 (10)                 | 32 (6)                 | 17 (5)              | 18 (6)            | 9 (9)                   | 12 (6)            | 11 (5)                      | 13 (4)                    |
|   1988 | 26 (10)                 | 30 (6)                 | 17 (5)              | 18 (6)            | 9 (9)                   | 12 (6)            | 11 (5)                      | 11 (4)                    |
|   1989 | 27 (10)                 | 27 (6)                 | 19 (5)              | 18 (6)            | 9 (9)                   | 12 (6)            | 12 (5)                      | 12 (4)                    |
|   1990 | 26 (10)                 | 27 (6)                 | 19 (5)              | 18 (6)            | 9 (9)                   | 12 (6)            | 12 (5)                      | 11 (4)                    |
|   1991 | 23 (10)                 | 25 (6)                 | 20 (5)              | 18 (6)            | 9 (9)                   | 12 (6)            | 12 (5)                      | 11 (4)                    |
|   1992 | 23 (10)                 | 25 (6)                 | 20 (5)              | 18 (6)            | 9 (9)                   | 12 (6)            | 12 (5)                      | 11 (4)                    |
|   1993 | 24 (10)                 | 25 (6)                 | 20 (5)              | 18 (6)            | 10 (9)                  | 12 (6)            | 12 (5)                      | 11 (4)                    |
|   1994 | 25 (9)                  | 25 (7)                 | 19 (5)              | 18 (6)            | 10 (9)                  | 12 (6)            | 12 (5)                      | 11 (4)                    |
|   1995 | 24 (9)                  | 24 (7)                 | 18 (5)              | 18 (6)            | 10 (9)                  | 13 (6)            | 12 (5)                      | 11 (4)                    |
|   1996 | 25 (9)                  | 23 (7)                 | 19 (5)              | 19 (6)            | 10 (9)                  | 13 (6)            | 12 (5)                      | 11 (4)                    |
|   1997 | 26 (9)                  | 23 (7)                 | 19 (5)              | 18 (6)            | 10 (9)                  | 13 (6)            | 12 (5)                      | 11 (4)                    |
|   1998 | 29 (9)                  | 24 (7)                 | 20 (5)              | 16 (6)            | 10 (9)                  | 13 (6)            | 12 (5)                      | 11 (4)                    |
|   1999 | 32 (9)                  | 24 (7)                 | 20 (5)              | 15 (6)            | 10 (9)                  | 14 (6)            | 12 (5)                      | 12 (4)                    |
|   2000 | 36 (9)                  | 24 (7)                 | 20 (5)              | 16 (6)            | 10 (9)                  | 13 (7)            | 13 (5)                      | 11 (4)                    |
|   2001 | 37 (9)                  | 24 (7)                 | 21 (5)              | 17 (7)            | 12 (9)                  | 14 (7)            | 14 (6)                      | 11 (4)                    |
|   2002 | 39 (9)                  | 28 (7)                 | 21 (5)              | 17 (7)            | 15 (9)                  | 14 (7)            | 13 (5)                      | 13 (4)                    |
|   2003 | 41 (9)                  | 30 (7)                 | 22 (5)              | 18 (7)            | 11 (9)                  | 13 (7)            | 14 (5)                      | 11 (4)                    |
|   2004 | 42 (9)                  | 32 (7)                 | 25 (5)              | 19 (7)            | 11 (9)                  | 13 (7)            | 12 (5)                      | 11 (4)                    |
|   2005 | 43 (9)                  | 34 (7)                 | 35 (5)              | 19 (7)            | 12 (9)                  | 13 (7)            | 12 (5)                      | 12 (4)                    |
|   2006 | 45 (9)                  | 29 (7)                 | 34 (5)              | 18 (7)            | 11 (9)                  | 14 (7)            | 12 (5)                      | 11 (4)                    |
|   2007 | 45 (9)                  | 29 (7)                 | 30 (5)              | 16 (7)            | 12 (9)                  | 13 (7)            | 12 (5)                      | 11 (4)                    |
|   2008 | 44 (9)                  | 27 (7)                 | 28 (5)              | 17 (7)            | 12 (9)                  | 14 (7)            | 12 (5)                      | 11 (4)                    |
|   2009 | 46 (9)                  | 30 (7)                 | 27 (5)              | 19 (7)            | 12 (9)                  | 13 (7)            | 12 (5)                      | 11 (4)                    |
|   2010 | 47 (9)                  | 29 (7)                 | 26 (5)              | 19 (7)            | 12 (9)                  | 13 (7)            | 12 (5)                      | 11 (4)                    |
|   2011 | 46 (9)                  | 28 (7)                 | 28 (5)              | 18 (7)            | 13 (9)                  | 13 (7)            | 12 (5)                      | 12 (4)                    |
|   2012 | 52 (10)                 | 26 (7)                 | 27 (5)              | 18 (7)            | 13 (9)                  | 13 (7)            | 12 (5)                      | 12 (4)                    |
|   2013 | 51 (10)                 | 27 (7)                 | 27 (5)              | 17 (7)            | 13 (9)                  | 13 (7)            | 12 (5)                      | 12 (4)                    |
|   2014 | 49 (10)                 | 26 (7)                 | 25 (5)              | 18 (7)            | 14 (9)                  | 13 (7)            | 12 (5)                      | 12 (4)                    |
|   2015 | 48 (10)                 | 29 (7)                 | 25 (5)              | 17 (8)            | 14 (10)                 | 13 (7)            | 13 (5)                      | 12 (4)                    |
|   2016 | 48 (10)                 | 28 (7)                 | 24 (5)              | 15 (7)            | 14 (10)                 | 13 (7)            | 13 (5)                      | 12 (4)                    |
|   2017 | 40 (10)                 | 33 (7)                 | 26 (5)              | 16 (7)            | 15 (10)                 | 13 (6)            | 13 (5)                      | 12 (4)                    |
|   2018 | 34 (10)                 | 31 (7)                 | 27 (5)              | 16 (7)            | 15 (10)                 | 13 (6)            | 13 (5)                      | 12 (4)                    |
|   2019 | 33 (10)                 | 33 (7)                 | 25 (5)              | 16 (7)            | 16 (10)                 | 12 (7)            | 14 (5)                      | 12 (4)                    |:END:

*** Køge Bugt
:PROPERTIES:
:header-args:jupyter-python+: :tangle no
:END:

#+BEGIN_SRC jupyter-python :session koge_bugt :results drawer
import os
import numpy as np
import shapefile

import matplotlib.pyplot as plt
from grass_session import Session
from grass.pygrass.modules.shortcuts import general as g
from grass.pygrass.modules.shortcuts import raster as r
from grass.pygrass.modules.shortcuts import vector as v
from grass.pygrass.modules.shortcuts import display as d
from grass.script import array as garray
from grass.script import core as gcore

if 'bamber' not in locals():
    PERMANENT = Session()
    PERMANENT.open(gisdb="./", location="G", mapset="tmp_KogeBugt", create_opts="")
    
    x = garray.array("x@PERMANENT", null=np.nan)
    y = garray.array("y@PERMANENT", null=np.nan)
    vel = garray.array("vel_baseline@MEaSUREs.0478", null=np.nan)
    vx = garray.array("vx_baseline@MEaSUREs.0478", null=np.nan)
    vy = garray.array("vy_baseline@MEaSUREs.0478", null=np.nan)
    z_s = garray.array("surface@BedMachine", null=np.nan)
    z_b = garray.array("bed@BedMachine", null=np.nan)
    bamber = garray.array("thickness@Bamber_2013", null=np.nan)
    bamber[bamber == 0] = np.nan
    z_s[z_s == 0] = np.nan
    z_b[z_s == 0] = np.nan
    thick = z_s - z_b
    diff = thick - bamber
    gates = garray.array("gates_final@gates_100_5000", null=np.nan)

    # v.out_ogr(input='gate_lines', output='./tmp/SHP/E2014_gates.shp', format='ESRI_Shapefile', overwrite=True)
    # gates_E2014 = shapefile.Reader("./tmp/SHP/E2014_gates.shp")
    PERMANENT.close()

# get numbers below by zooming in, then plt.xlim(), plt.ylim()    
#     a.set_xlim([160000, 200000])
#     a.set_ylim([-2750000, -2710000])

if 'x_koge' not in locals():
    idx_x0 = np.where(x[0,:] > 170000)[0][0]    
    idx_x1 = np.where(x[0,:] > 200000)[0][0]
    idx_y0 = np.where(y[:,0] < -2710000)[0][0]
    # idx_y1 = np.where(y[:,0] < -2750000)[0][0]
    idx_y1 = np.where(y[:,0] < -2735000)[0][0]
    vel_koge = vel[idx_y0:idx_y1, idx_x0:idx_x1]
    vx_koge = vx[idx_y0:idx_y1, idx_x0:idx_x1]
    vy_koge = vy[idx_y0:idx_y1, idx_x0:idx_x1]
    thick_koge = thick[idx_y0:idx_y1, idx_x0:idx_x1]
    bamber_koge = bamber[idx_y0:idx_y1, idx_x0:idx_x1]
    diff_koge = diff[idx_y0:idx_y1, idx_x0:idx_x1]
    x_koge = x[idx_y0:idx_y1, idx_x0:idx_x1]
    y_koge = y[idx_y0:idx_y1, idx_x0:idx_x1]

# plt.close(1)
plt.clf()
fig = plt.figure(1, figsize=(8,6)) # w,h
# get_current_fig_manager().window.move(0,0)
fig.clf()
fig.set_tight_layout(True)

ax1 = fig.add_subplot(221)
ax2 = fig.add_subplot(222)
ax3 = fig.add_subplot(223)
ax4 = fig.add_subplot(224)

# extent = [x.min(),x.max(),y.min(),y.max()]
extent = [x_koge.min(), x_koge.max(), y_koge.min(), y_koge.max()]

plt.cm.Blues.set_bad('gray', 0.3)
plt.cm.RdBu_r.set_bad('gray', 0.3)
plt.cm.viridis.set_bad('gray', 0.3)
im1 = ax1.imshow(np.log10(vel_koge), extent=extent)
im2 = ax2.imshow(thick_koge, extent=extent, cmap=plt.cm.Blues, vmin=0, vmax=1500)
im3 = ax3.imshow(bamber_koge, extent=extent, cmap=plt.cm.Blues, vmin=0, vmax=1500)
im4 = ax4.imshow(diff_koge, extent=extent, cmap=plt.cm.RdBu_r, vmin=-1500, vmax=1500)

label = ['a','b','c','d']  
for i,a in enumerate([ax1,ax2,ax3,ax4]):
    C = plt.cm.gray_r if a == ax1 else plt.cm.gray
    a.imshow(gates[idx_y0:idx_y1, idx_x0:idx_x1], extent=extent, cmap=C)
    C = 'w' if i <= 2 else 'k'
    a.text(0.03,0.88,label[i], transform=a.transAxes, color=C, fontsize=14)
    a.set_yticklabels([])
    a.set_xticklabels([])

from mpl_toolkits.axes_grid1 import make_axes_locatable
# for i,a in zip([im1,im2,im3,im4],[ax1,ax2,ax3,ax4]):
divider = make_axes_locatable(ax1); cax = divider.append_axes("right", size="5%", pad=0.05)
c = fig.colorbar(im1, cax=cax); c.set_label('log$_{10}$ Speed [m yr$^{-1}$]')
divider = make_axes_locatable(ax2); cax = divider.append_axes("right", size="5%", pad=0.05)
c = fig.colorbar(im2, cax=cax); c.set_label('Thickness [m]')
divider = make_axes_locatable(ax3); cax = divider.append_axes("right", size="5%", pad=0.05)
c = fig.colorbar(im3, cax=cax); c.set_label('Thickness [m]')
divider = make_axes_locatable(ax4); cax = divider.append_axes("right", size="5%", pad=0.05)
c = fig.colorbar(im4, cax=cax); c.set_label('$\Delta$ Thickness [m]')

plt.savefig('./figs/koge_bugt.png', transparent=True, bbox_inches='tight', dpi=300)
plt.savefig('./figs/koge_bugt.pdf', transparent=True, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:
: GRASSBIN: /usr/bin/grass74
: GISBASE: /usr/lib/grass74
: /home/kdm/local/anaconda/envs/sci/lib/python3.6/site-packages/matplotlib/figure.py:2369: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   warnings.warn("This figure includes Axes that are not compatible "

* Supplementary Material                                :noexport:
:PROPERTIES:
:header-args:jupyter-python+: :tangle no
:END:
** Errors by gate sorted by total D, err, err %
**** Top by Discharge
#+BEGIN_SRC jupyter-python :results raw drawer :session discharge :display text/org
import pandas as pd
import numpy as np

err_gate = pd.read_csv('./tmp/err_gate.csv', index_col=0)
err_gate.loc['GIS'] = np.nan
err_gate.dropna(inplace=True)
err_gate.rename(columns = {'D':'D [Gt]', 
                           'E':'Error [Gt]',
                           'E%':'Error [%]'}, inplace=True)
 
err_gate.sort_values('D [Gt]', inplace=True, ascending=False)
err_gate = err_gate.iloc[:25]
err_gate
#+END_SRC

#+RESULTS:
:RESULTS:
|   Gate |   D [Gt] |   Error [Gt] |   Error [%] | name                      |
|--------+----------+--------------+-------------+---------------------------|
|    186 | 45.9601  |     4.27592  |     9.30355 | JAKOBSHAVN_ISBRAE         |
|    233 | 28.307   |     2.12532  |     7.50812 | HELHEIMGLETSCHER          |
|    192 | 25.1468  |     1.2993   |     5.16687 | KANGERLUSSUAQ             |
|    248 | 16.999   |     1.22285  |     7.19366 | KOGE_BUGT_C               |
|     32 | 14.2076  |     1.37361  |     9.6681  | ZACHARIAE_ISSTROM         |
|    143 | 12.9161  |     0.860943 |     6.66564 | RINK_ISBRAE               |
|     30 | 12.2803  |     0.662151 |     5.39196 | NIOGHALVFJERDSFJORDEN     |
|     27 | 11.3084  |     0.48615  |     4.299   | PETERMANN_GLETSCHER       |
|    242 | 10.4887  |     0.949496 |     9.05253 | IKERTIVAQ_M               |
|    140 |  9.50652 |     0.681882 |     7.17278 | DAUGAARD-JENSEN           |
|    213 |  9.04978 |     0.246587 |     2.72478 | UNNAMED_DECEPTION_O_CN_CS |
|     74 |  8.37833 |     0.428752 |     5.1174  | KONG_OSCAR_GLETSCHER      |
|    168 |  8.37552 |     0.669684 |     7.99573 | STORE_GLETSCHER           |
|    317 |  8.04856 |     0.664796 |     8.25982 | TINGMIARMIUT_FJORD        |
|    102 |  7.80946 |     0.861176 |    11.0273  | HAYES_GLETSCHER_N_NN      |
|    241 |  7.8092  |     0.765558 |     9.80329 | IKERTIVAQ_NN              |
|    134 |  7.49194 |     0.770461 |    10.2839  | UPERNAVIK_ISSTROM_N       |
|    172 |  7.45363 |     0.632841 |     8.49037 | SERMEQ_KUJALLEQ           |
|     80 |  7.13103 |     0.41994  |     5.88891 | NORDENSKIOLD_GLESCHER_NW  |
|    135 |  6.64584 |     0.826002 |    12.4289  | UPERNAVIK_ISSTROM_C       |
|     95 |  6.58815 |     0.771847 |    11.7157  | KJER_GLETSCHER            |
|    260 |  6.27373 |     0.338655 |     5.39799 | KOGE_BUGT_S               |
|    129 |  6.21848 |     0.256494 |     4.1247  | KAKIVFAAT_SERMIAT         |
|     28 |  5.81144 |     0.308143 |     5.30235 | HUMBOLDT_GLETSCHER        |
|    113 |  5.67861 |     0.621164 |    10.9387  | ALISON_GLETSCHER          |
:END:

**** Top by Error [Gt]
#+BEGIN_SRC jupyter-python :results table :session discharge :display text/org
from tabulate import tabulate
import pandas as pd
err_gate = pd.read_csv('./tmp/err_gate.csv', index_col=0)
err_gate.loc['GIS'] = np.nan
err_gate.dropna(inplace=True)
err_gate.rename(columns = {'D':'D [Gt]', 
                           'E':'Error [Gt]',
                           'E%':'Error [%]',
                           'Ew':'Error Weighted',
                           'Ew%':'Error Weighted %'}, inplace=True)

err_gate.sort_values('Error [Gt]', inplace=True, ascending=False)
err_gate = err_gate.iloc[:25]
err_gate
#+END_SRC

#+RESULTS:
| :RESULTS: |          |            |           |                             |
|      Gate |   D [Gt] | Error [Gt] | Error [%] | name                        |
|-----------+----------+------------+-----------+-----------------------------|
|       186 |  45.9601 |    4.27592 |   9.30355 | JAKOBSHAVN_ISBRAE           |
|       233 |   28.307 |    2.12532 |   7.50812 | HELHEIMGLETSCHER            |
|        32 |  14.2076 |    1.37361 |    9.6681 | ZACHARIAE_ISSTROM           |
|       192 |  25.1468 |     1.2993 |   5.16687 | KANGERLUSSUAQ               |
|       248 |   16.999 |    1.22285 |   7.19366 | KOGE_BUGT_C                 |
|       352 |  1.87522 |    1.18394 |    63.136 | EQALORUTSIT_KILLIIT_SERMIAT |
|       273 |   4.4488 |    1.04037 |   23.3855 | UMIIVIK_FJORD               |
|       242 |  10.4887 |   0.949496 |   9.05253 | IKERTIVAQ_M                 |
|       102 |  7.80946 |   0.861176 |   11.0273 | HAYES_GLETSCHER_N_NN        |
|       143 |  12.9161 |   0.860943 |   6.66564 | RINK_ISBRAE                 |
|       135 |  6.64584 |   0.826002 |   12.4289 | UPERNAVIK_ISSTROM_C         |
|       226 |  4.65263 |   0.795364 |   17.0949 | KIV_STEENSTRUP_NODRE_BRAE   |
|        95 |  6.58815 |   0.771847 |   11.7157 | KJER_GLETSCHER              |
|       134 |  7.49194 |   0.770461 |   10.2839 | UPERNAVIK_ISSTROM_N         |
|       241 |   7.8092 |   0.765558 |   9.80329 | IKERTIVAQ_NN                |
|       140 |  9.50652 |   0.681882 |   7.17278 | DAUGAARD-JENSEN             |
|       168 |  8.37552 |   0.669684 |   7.99573 | STORE_GLETSCHER             |
|       317 |  8.04856 |   0.664796 |   8.25982 | TINGMIARMIUT_FJORD          |
|        30 |  12.2803 |   0.662151 |   5.39196 | NIOGHALVFJERDSFJORDEN       |
|       172 |  7.45363 |   0.632841 |   8.49037 | SERMEQ_KUJALLEQ             |
|       113 |  5.67861 |   0.621164 |   10.9387 | ALISON_GLETSCHER            |
|       280 |  4.21919 |   0.586934 |   13.9111 | GRAULV                      |
|       247 |  3.90797 |   0.571938 |   14.6352 | KOGE_BUGT_N                 |
|       238 | 0.463106 |   0.567007 |   122.436 | HEIM_GLETSCHER              |
|       240 | 0.734125 |   0.553775 |   75.4333 | NONAME_IKERTIVAQ_N          |

**** Top by Error [%]
#+BEGIN_SRC jupyter-python :results table  :session discharge :display text/org
from tabulate import tabulate
import pandas as pd
err_gate = pd.read_csv('./tmp/err_gate.csv', index_col=0)
err_gate.loc['GIS'] = np.nan
err_gate.dropna(inplace=True)
err_gate.rename(columns = {'D':'D [Gt]', 
                           'E':'Error [Gt]',
                           'E%':'Error [%]',
                           'Ew':'Error Weighted',
                           'Ew%':'Error Weighted %'}, inplace=True)

err_gate.sort_values('Error [%]', inplace=True, ascending=False)
err_gate = err_gate.iloc[:25]
err_gate
#+END_SRC

#+RESULTS:
| :RESULTS: |             |            |           |                                |
|      Gate |      D [Gt] | Error [Gt] | Error [%] | name                           |
|-----------+-------------+------------+-----------+--------------------------------|
|       326 | 0.000160574 |  0.0448814 |   27950.7 | MOGENS_HEINESEN_SS_SSS         |
|       358 | 0.000406285 |   0.101505 |   24983.6 | NAAJAT_SERMIAT                 |
|       286 |  0.00034279 |  0.0719289 |   20983.4 | GYLDENLOVE_SS                  |
|       337 | 7.64832e-05 |  0.0160352 |   20965.6 | NAPASORSUAQ_C_S                |
|       279 | 0.000274782 |  0.0575628 |   20948.5 | APUSEERSERPIA                  |
|       296 |  0.00094828 |   0.185566 |   19568.7 | SE_NONAME2                     |
|       345 |  0.00136043 |   0.259424 |   19069.3 | UNNAMED_ANORITUUP_KANGERLUA_S  |
|       346 | 0.000125657 |  0.0235513 |   18742.6 | UNNAMED_ANORITUUP_KANGERLUA_SS |
|       227 |   0.0003607 |  0.0671429 |   18614.6 | KIV_STEENSTRUP_SONDRE_BRAE     |
|       322 | 0.000286124 |   0.052938 |   18501.8 | MOGENS_HEINESEN_SS_SSS         |
|       263 | 0.000516315 |  0.0950138 |   18402.3 | KOGE_BUGT_SS                   |
|       292 |  0.00051318 |  0.0943305 |   18381.6 | SE_NONAME2                     |
|       370 | 0.000162942 |  0.0297611 |   18264.9 | SOUTHERN_TIP                   |
|       291 | 0.000866503 |   0.154398 |   17818.6 | SE_NONAME2                     |
|       266 | 0.000943541 |   0.165326 |   17521.9 | KOGE_BUGT_SS                   |
|       347 | 0.000157755 |  0.0272631 |   17281.9 | UNNAMED_ANORITUUP_KANGERLUA_SS |
|       225 |  0.00182813 |   0.313983 |   17175.1 | UNNAMED_LAUBE_S                |
|       301 | 0.000255828 |  0.0436295 |   17054.2 | SE_NONAME2                     |
|       304 | 0.000252462 |  0.0424767 |     16825 | SE_NONAME2                     |
|       294 |  0.00212486 |   0.354715 |   16693.6 | FIMBULGETLSCHER                |
|       219 | 0.000245903 |   0.040796 |   16590.2 | UNNAMED_LAUBE_S                |
|       223 | 0.000650151 |   0.104006 |   15997.2 | UNNAMED_LAUBE_S                |
|       338 | 0.000108454 |  0.0172894 |   15941.7 | NAPASORSUAQ_C_S                |
|       289 | 3.29127e-05 | 0.00523086 |   15893.1 | SE_NONAME2                     |
|       287 | 0.000454846 |  0.0715212 |   15724.3 | GYLDENLOVE_S                   |

**** Bottom by Error [%]
#+BEGIN_SRC jupyter-python :results table  :session discharge :display text/org
from tabulate import tabulate
import pandas as pd
err_gate = pd.read_csv('./tmp/err_gate.csv', index_col=0)
err_gate.loc['GIS'] = np.nan
err_gate.dropna(inplace=True)
err_gate.rename(columns = {'D':'D [Gt]', 
                           'E':'Error [Gt]',
                           'E%':'Error [%]',
                           'Ew':'Error Weighted',
                           'Ew%':'Error Weighted %'}, inplace=True)

err_gate.sort_values('Error [%]', inplace=True, ascending=True)
err_gate = err_gate.iloc[:25]
err_gate
#+END_SRC

#+RESULTS:
| :RESULTS: |           |            |           |                            |
|      Gate |    D [Gt] | Error [Gt] | Error [%] | name                       |
|-----------+-----------+------------+-----------+----------------------------|
|       213 |   9.04978 |   0.246587 |   2.72478 | UNNAMED_DECEPTION_O_CN_CS  |
|        71 |    1.0023 |  0.0280765 |    2.8012 | ISSUUARSUIT_SERMIA         |
|        29 | 0.0965288 | 0.00284121 |   2.94338 | HUMBOLDT_GLETSCHER         |
|       189 |  0.121413 |  0.0036937 |   3.04227 | KONG_CHRISTIAN             |
|       321 |   4.72381 |   0.168764 |   3.57263 | MOGENS_HEINESEN_S          |
|        72 |   1.05992 |  0.0392134 |   3.69965 | ISSUUARSUIT_SERMIA         |
|       127 |  0.722277 |  0.0267532 |   3.70401 | QEQERTARSUUP_SERMIA        |
|       194 |  0.797613 |  0.0319491 |   4.00558 | UNNAMED_KANGER_W           |
|        10 |   2.56798 |    0.10477 |   4.07984 | RYDER_GLETSCHER            |
|       129 |   6.21848 |   0.256494 |    4.1247 | KAKIVFAAT_SERMIAT          |
|        79 | 0.0623227 |  0.0026047 |   4.17937 | NORDENSKIOLD_GLESCHER_NW   |
|       255 |  0.286414 |   0.012137 |   4.23755 | KOGE_BUGT_S                |
|        27 |   11.3084 |    0.48615 |     4.299 | PETERMANN_GLETSCHER        |
|       214 |   4.85333 |   0.212367 |   4.37569 | UNNAMED_UUNARTIT_ISLANDS   |
|        48 |  0.887973 |  0.0415969 |   4.68448 | HARALD_MOLTKE_BRAE         |
|        31 |  0.184463 | 0.00874956 |   4.74326 | HUMBOLDT_GLETSCHER         |
|       178 |  0.999155 |  0.0487067 |   4.87479 | KANGILERNGATA_SERMIA       |
|       215 |  0.788414 |  0.0391102 |   4.96062 | KRUUSE_FJORD               |
|       165 |  0.351213 |  0.0176866 |   5.03587 | STORE_GLETSCHER            |
|       101 |  0.764497 |  0.0387761 |   5.07211 | HAYES_GLETSCHER_N_NN       |
|        69 |   3.48082 |   0.177199 |   5.09073 | ISSUUARSUIT_SERMIA         |
|        74 |   8.37833 |   0.428752 |    5.1174 | KONG_OSCAR_GLETSCHER       |
|       365 |  0.131109 | 0.00674577 |   5.14515 | UNNAMED_SOUTH_DANELL_FJORD |
|       192 |   25.1468 |     1.2993 |   5.16687 | KANGERLUSSUAQ              |
|       154 |  0.668659 |  0.0350659 |   5.24422 | SERMEQ_SILARLEQ            |


** Annual averages from observations or linear interpolation
Compute the annual average for all GIS and some other glaciers using two methods, and compare the differences
+ Method 1: Annual average from observations
+ Method 2: Annual average after linearly interpolating observations to daily resolution

NOTE - do this with the "rawest" velocity product prior to any interpolation.

#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt

if 'th' not in locals():
    <<load_data>>
    <<millan_2018>>
    <<adjust_thickness>>
    <<adjust_thickness_fit>>

# What is D where velocity is directly observed, no velocity filling?
D = (fill*vel).apply(lambda c: c * (th['fit'].values * 200), axis=0) * 917 / 1E12

ann = pd.DataFrame(index = D.resample("A", axis='columns').mean().columns)
ann['obs'] = D.sum(axis='rows').resample('A').mean().values
ann['daily'] = np.nan
for y in np.arange(1985,2018):
    ann.loc[ann.index.year == y, 'daily'] \
        = D\
        .T[D.columns.year == y]\
        .T\
        .sum(axis='rows')\
        .resample('1D')\
        .mean()\
        .interpolate(method="time")\
        .resample("A")\
        .mean()\
        .values
    
ann['diff'] = ann['obs'] - ann['daily']
ann['diff [%]'] = 100 - ann['daily']/ann['obs']*100

print(ann.describe())

ann
#+END_SRC

#+RESULTS:
:results:
# Out[85]:
# output
              obs       daily          diff      diff [%]
count   33.000000   33.000000  3.300000e+01  3.300000e+01
mean   199.417777  207.352863 -7.935085e+00 -4.367376e+00
std     98.991770  101.339903  1.059513e+01  5.021360e+00
min     58.748239   61.499036 -2.927808e+01 -1.784780e+01
25%     85.353493   90.873619 -1.593002e+01 -9.058464e+00
50%    215.110409  230.668093 -2.750798e+00 -2.768276e+00
75%    264.767221  286.765963 -5.684342e-14 -2.842171e-14
max    429.197880  427.689462  9.298093e+00  3.180242e+00

# text/org
|                     |      obs |    daily |          diff |      diff [%] |
|---------------------+----------+----------+---------------+---------------|
| 1985-12-31 00:00:00 |  74.9168 |  76.7606 |  -1.84377     |  -2.46109     |
| 1986-12-31 00:00:00 |  69.3422 |  70.4385 |  -1.09632     |  -1.58103     |
| 1987-12-31 00:00:00 |  58.7482 |  61.499  |  -2.7508      |  -4.68235     |
| 1988-12-31 00:00:00 |  65.366  |  70.6574 |  -5.29132     |  -8.0949      |
| 1989-12-31 00:00:00 |  73.2416 |  75.2692 |  -2.02753     |  -2.76828     |
| 1990-12-31 00:00:00 |  64.4267 |  67.0241 |  -2.59743     |  -4.0316      |
| 1991-12-31 00:00:00 |  85.3535 |  85.3535 |   0           |   0           |
| 1992-12-31 00:00:00 | 197.829  | 197.829  |   1.13687e-13 |   5.68434e-14 |
| 1993-12-31 00:00:00 | 117.695  | 117.695  |  -1.27898e-13 |  -1.13687e-13 |
| 1994-12-31 00:00:00 |  77.111  |  90.8736 | -13.7626      | -17.8478      |
| 1995-12-31 00:00:00 | 178.747  | 178.747  |   2.84217e-13 |   1.56319e-13 |
| 1996-12-31 00:00:00 | 255.303  | 255.303  |   2.27374e-13 |   8.52651e-14 |
| 1997-12-31 00:00:00 | 230.668  | 230.668  |  -5.68434e-14 |  -2.84217e-14 |
| 1998-12-31 00:00:00 |  72.0978 |  73.4101 |  -1.31223     |  -1.82007     |
| 1999-12-31 00:00:00 | 129.912  | 141.812  | -11.9005      |  -9.16046     |
| 2000-12-31 00:00:00 | 209.108  | 206.484  |   2.62333     |   1.25453     |
| 2001-12-31 00:00:00 | 215.11   | 231.677  | -16.5666      |  -7.70146     |
| 2002-12-31 00:00:00 | 178.319  | 194.486  | -16.167       |  -9.06635     |
| 2003-12-31 00:00:00 | 162.266  | 182.339  | -20.0724      | -12.37        |
| 2004-12-31 00:00:00 | 267.362  | 295.416  | -28.0538      | -10.4928      |
| 2005-12-31 00:00:00 | 264.767  | 268.027  |  -3.25938     |  -1.23104     |
| 2006-12-31 00:00:00 | 303.666  | 295.971  |   7.69537     |   2.53416     |
| 2007-12-31 00:00:00 | 299.047  | 314.399  | -15.3521      |  -5.13366     |
| 2008-12-31 00:00:00 | 253.288  | 278.197  | -24.9091      |  -9.83432     |
| 2009-12-31 00:00:00 | 302.695  | 318.625  | -15.93        |  -5.26273     |
| 2010-12-31 00:00:00 | 262.313  | 291.591  | -29.2781      | -11.1615      |
| 2011-12-31 00:00:00 | 240.121  | 261.872  | -21.7513      |  -9.05846     |
| 2012-12-31 00:00:00 | 292.371  | 283.072  |   9.29809     |   3.18024     |
| 2013-12-31 00:00:00 | 260.693  | 286.766  | -26.0727      | -10.0013      |
| 2014-12-31 00:00:00 | 242.024  | 247.054  |  -5.02959     |  -2.07813     |
| 2015-12-31 00:00:00 | 319.28   | 334.912  | -15.6324      |  -4.89615     |
| 2016-12-31 00:00:00 | 328.401  | 330.727  |  -2.32597     |  -0.708269    |
| 2017-12-31 00:00:00 | 429.198  | 427.689  |   1.50842     |   0.351451    |
:end:


** Køge Bugt Y2K
#+BEGIN_SRC jupyter-python :exports both :results raw drawer :display text/org
D = pd.read_csv("./out/sector_D.csv", index_col=0, parse_dates=True)
err = pd.read_csv("./out/sector_err.csv", index_col=0, parse_dates=True)

S = "KOGE_BUGT_C"
D = D[S].resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

err = err[S].resample('1D',axis='rows').mean().interpolate(method='time',limit_area='inside').resample('A',axis='rows').mean()

print("Køge Bugt C described:\n", D.describe())

df = pd.DataFrame(index=D.index)
df['KB'] = D
df['err'] = err
df
#+END_SRC

#+RESULTS:
:results:
# Out[324]:
# output
Køge Bugt C described:
 count    32.000000
mean     17.748575
std       1.038257
min      15.098256
25%      17.267486
50%      18.236337
75%      18.443478
max      19.077853
Name: KOGE_BUGT_C, dtype: float64

# text/org
| Date                |      KB |      err |
|---------------------+---------+----------|
| 1986-12-31 00:00:00 | 18.4098 | 1.07501  |
| 1987-12-31 00:00:00 | 18.4098 | 1.07501  |
| 1988-12-31 00:00:00 | 18.4098 | 1.07501  |
| 1989-12-31 00:00:00 | 18.4102 | 1.07501  |
| 1990-12-31 00:00:00 | 18.4204 | 1.07501  |
| 1991-12-31 00:00:00 | 18.4408 | 1.07501  |
| 1992-12-31 00:00:00 | 18.4614 | 1.07501  |
| 1993-12-31 00:00:00 | 18.4516 | 1.07427  |
| 1994-12-31 00:00:00 | 18.4076 | 1.07278  |
| 1995-12-31 00:00:00 | 18.3655 | 1.07129  |
| 1996-12-31 00:00:00 | 18.5187 | 1.07793  |
| 1997-12-31 00:00:00 | 17.8199 | 1.0406   |
| 1998-12-31 00:00:00 | 15.8305 | 0.961628 |
| 1999-12-31 00:00:00 | 17.9254 | 1.05717  |
| 2000-12-31 00:00:00 | 16.3663 | 1.04274  |
| 2001-12-31 00:00:00 | 16.3612 | 1.07282  |
| 2002-12-31 00:00:00 | 17.3179 | 1.14493  |
| 2003-12-31 00:00:00 | 18.6225 | 1.21182  |
| 2004-12-31 00:00:00 | 19.0779 | 1.28109  |
| 2005-12-31 00:00:00 | 18.949  | 1.2861   |
| 2006-12-31 00:00:00 | 18.1071 | 1.2402   |
| 2007-12-31 00:00:00 | 16.0185 | 1.08725  |
| 2008-12-31 00:00:00 | 17.1162 | 1.16671  |
| 2009-12-31 00:00:00 | 18.5829 | 1.29226  |
| 2010-12-31 00:00:00 | 18.7028 | 1.33     |
| 2011-12-31 00:00:00 | 17.5419 | 1.28836  |
| 2012-12-31 00:00:00 | 17.9601 | 1.3163   |
| 2013-12-31 00:00:00 | 17.5486 | 1.2603   |
| 2014-12-31 00:00:00 | 17.5678 | 1.29832  |
| 2015-12-31 00:00:00 | 16.861  | 1.25562  |
| 2016-12-31 00:00:00 | 15.0983 | 1.11953  |
| 2017-12-31 00:00:00 | 15.873  | 1.18621  |
:end:

:results:
# Out[323]:
# output
Køge Bugt C described:
 count    32.000000
mean     17.748575
std       1.038257
min      15.098256
25%      17.267486
50%      18.236337
75%      18.443478
max      19.077853
Name: KOGE_BUGT_C, dtype: float64

# text/plain
:                    KB       err
: Date                           
: 1986-12-31  18.409819  1.075015
: 1987-12-31  18.409819  1.075015
: 1988-12-31  18.409819  1.075015
: 1989-12-31  18.410180  1.075015
: 1990-12-31  18.420363  1.075015
: 1991-12-31  18.440757  1.075015
: 1992-12-31  18.461401  1.075015
: 1993-12-31  18.451643  1.074272
: 1994-12-31  18.407605  1.072781
: 1995-12-31  18.365531  1.071291
: 1996-12-31  18.518715  1.077929
: 1997-12-31  17.819938  1.040603
: 1998-12-31  15.830453  0.961628
: 1999-12-31  17.925379  1.057169
: 2000-12-31  16.366273  1.042741
: 2001-12-31  16.361203  1.072815
: 2002-12-31  17.317914  1.144929
: 2003-12-31  18.622540  1.211825
: 2004-12-31  19.077853  1.281091
: 2005-12-31  18.949015  1.286105
: 2006-12-31  18.107142  1.240200
: 2007-12-31  16.018466  1.087247
: 2008-12-31  17.116200  1.166711
: 2009-12-31  18.582891  1.292257
: 2010-12-31  18.702845  1.330004
: 2011-12-31  17.541919  1.288357
: 2012-12-31  17.960070  1.316301
: 2013-12-31  17.548620  1.260296
: 2014-12-31  17.567834  1.298322
: 2015-12-31  16.860994  1.255622
: 2016-12-31  15.098256  1.119534
: 2017-12-31  15.872952  1.186213


** Grids
+ Display different grids of different products
#+BEGIN_SRC sh :results verbatim
g.mapset -c tmp_grids

g.mapsets -l | tr ' ' '\n' | grep -v "gates_"

# zoom in somewhere arbitracy
g.region n=-2269600 s=-2296400 w=-179600 e=-152800 -p

g.list type=raster mapset=BedMachine
r.to.vect input=bed@BedMachine output=BedMachine type=area --o

g.list type=raster mapset=GIMP.0715 | grep -v tile
r.to.vect input=dem@GIMP.0715 output=GIMP_0715 type=area --o

g.list type=raster mapset=Khan_2016 | head
r.to.vect input=dh_1995@Khan_2016 output=Khan_2016 type=area --o

g.list type=raster mapset=Sentinel1 pattern=vel_eff* | head -n3
r.to.vect input=vy_2016_10_18@Sentinel1 output=Sentinel1 type=area --o

d.mon start=wx0
d.erase
g.list type=vector

d.vect Sentinel1 fill_color=none color=blue
d.vect BedMachine fill_color=none color=green
d.vect GIMP_0715 fill_color=none color=red
d.vect Khan_2016 fill_color=none color=gray width=3
#+END_SRC
#+RESULTS:

* Meta                                                  :noexport:
:PROPERTIES:
:header-args: :tangle no
:END:

This document probably uses code - python, octave, and/or R. Below I provide the version of the software(s) used to create this document in order to support the goal of reproduciblity. 
*** Emacs
#+BEGIN_SRC elisp
(emacs-version)
#+END_SRC
#+RESULTS:
: GNU Emacs 25.2.2 (x86_64-pc-linux-gnu, GTK+ Version 3.22.21)
:  of 2017-09-22, modified by Debian
*** Org Mode
#+BEGIN_SRC elisp
(org-version nil t)
#+END_SRC
#+RESULTS:
: Org mode version 9.2.3 (9.2.3-7-g222408-elpaplus @ /home/kdm/.emacs.d/elpa/org-plus-contrib-20190415/)

*** Python
#+BEGIN_SRC sh :cmdline -i :results verbatim
. /home/kdm/local/anaconda/etc/profile.d/conda.sh
conda env export --name sci
#+END_SRC
#+RESULTS:
#+begin_example
name: sci
channels:
  - conda-forge
  - defaults
dependencies:
  - affine=2.2.1=py_0
  - asn1crypto=0.24.0=py36_1003
  - attrs=18.2.0=py_0
  - backcall=0.1.0=py_0
  - blas=1.0=mkl
  - bokeh=1.0.1=py36_1000
  - boost-cpp=1.68.0=h3a22d5f_0
  - boto3=1.9.58=py_0
  - botocore=1.12.58=py_0
  - bottleneck=1.2.1=py36h7eb728f_1
  - bzip2=1.0.6=h470a237_2
  - ca-certificates=2019.3.9=hecc5488_0
  - cairo=1.14.12=he6fea26_5
  - certifi=2019.3.9=py36_0
  - cffi=1.11.5=py36h5e8e0c9_1
  - cftime=1.0.2.1=py36h7eb728f_0
  - click=7.0=py_0
  - click-plugins=1.0.4=py_0
  - cligj=0.5.0=py_0
  - cloudpickle=0.6.1=py_0
  - cryptography=2.6.1=py36h72c5cf5_0
  - cryptography-vectors=2.3.1=py36_1000
  - curl=7.64.1=hf8cf82a_0
  - cycler=0.10.0=py_1
  - cytoolz=0.9.0.1=py36h470a237_1
  - dask=0.20.0=py_0
  - dask-core=0.20.0=py_0
  - dbus=1.13.0=h3a4f0e9_0
  - decorator=4.3.0=py_0
  - descartes=1.1.0=py_2
  - dill=0.2.8.2=py36_1000
  - distributed=1.24.0=py36_1000
  - docutils=0.14=py36_1001
  - entrypoints=0.2.3=py36_1002
  - expat=2.2.5=hfc679d8_2
  - fontconfig=2.13.1=h65d0f4c_0
  - freetype=2.9.1=h6debe1e_4
  - freexl=1.0.5=h470a237_2
  - geos=3.6.2=hfc679d8_4
  - geotiff=1.4.2=h700e5ad_5
  - gettext=0.19.8.1=h5e8e0c9_1
  - giflib=5.1.4=h470a237_1
  - glib=2.55.0=h464dc38_2
  - gmp=6.1.2=hfc679d8_0
  - gst-plugins-base=1.12.5=hde13a9d_0
  - gstreamer=1.12.5=h61a6719_0
  - h5netcdf=0.6.2=py_0
  - h5py=2.8.0=py36h470a237_0
  - hdf4=4.2.13=h951d187_2
  - hdf5=1.10.1=2
  - heapdict=1.0.0=py36_1000
  - icu=58.2=hfc679d8_0
  - idna=2.8=py36_1000
  - intel-openmp=2019.0=118
  - ipykernel=5.1.0=pyh24bf2e0_0
  - ipython=7.1.1=py36h24bf2e0_1000
  - ipython_genutils=0.2.0=py_1
  - ipywidgets=7.4.2=py_0
  - jedi=0.13.1=py36_1000
  - jinja2=2.10=py_1
  - jmespath=0.9.3=py_1
  - jpeg=9c=h470a237_1
  - json-c=0.12.1=h470a237_1
  - jsonschema=3.0.0a3=py36_1000
  - jupyter=1.0.0=py_1
  - jupyter_client=5.2.3=py_1
  - jupyter_console=6.0.0=py_0
  - jupyter_core=4.4.0=py_0
  - kealib=1.4.8=hfc679d8_0
  - kiwisolver=1.0.1=py36h2d50403_2
  - krb5=1.16.3=h05b26f9_1001
  - libcurl=7.64.1=hda55be3_0
  - libdap4=3.19.1=h8fe5423_1
  - libedit=3.1.20170329=haf1bffa_1
  - libffi=3.2.1=hfc679d8_5
  - libgcc-ng=8.2.0=hdf63c60_1
  - libgfortran=3.0.0=1
  - libgfortran-ng=7.2.0=hdf63c60_3
  - libiconv=1.15=h470a237_3
  - libkml=1.3.0=he469717_9
  - libnetcdf=4.4.1.1=10
  - libpng=1.6.35=ha92aebf_2
  - libpq=11.2=h4770945_0
  - libsodium=1.0.16=h470a237_1
  - libspatialindex=1.8.5=hfc679d8_3
  - libspatialite=4.3.0a=hdfcc80b_23
  - libssh2=1.8.2=h22169c7_2
  - libstdcxx-ng=7.3.0=hdf63c60_0
  - libtiff=4.0.9=he6b73bb_2
  - libuuid=2.32.1=h470a237_2
  - libxcb=1.13=h470a237_2
  - libxml2=2.9.8=h422b904_5
  - locket=0.2.0=py_2
  - markupsafe=1.0=py36h470a237_1
  - matplotlib=3.0.3=py36_0
  - matplotlib-base=3.0.3=py36h167e16e_0
  - mistune=0.8.4=py36h470a237_0
  - mkl=2018.0.3=1
  - mkl_fft=1.0.6=py36_0
  - mkl_random=1.0.1=py36_0
  - msgpack-python=0.5.6=py36h2d50403_3
  - munch=2.3.2=py_0
  - nbconvert=5.3.1=py_1
  - nbformat=4.4.0=py_1
  - ncurses=6.1=hfc679d8_1
  - netcdf4=1.3.1=py36_1
  - notebook=5.7.0=py36_1000
  - numpy=1.14.2=py36hdbf6ddf_0
  - numpy-base=1.15.3=py36h81de0dd_0
  - olefile=0.46=py_0
  - openblas=0.3.3=ha44fe06_1
  - openjpeg=2.3.0=h0e734dc_3
  - openssl=1.1.1b=h14c3975_1
  - packaging=18.0=py_0
  - pandas=0.23.4=py36h637b7d7_1000
  - pandoc=2.3.1=0
  - pandocfilters=1.4.2=py_1
  - parso=0.3.1=py_0
  - partd=0.3.9=py_0
  - patsy=0.5.1=py_0
  - pcre=8.41=hfc679d8_3
  - pexpect=4.6.0=py36_1000
  - pickleshare=0.7.5=py36_1000
  - pillow=5.3.0=py36hc736899_0
  - pint=0.8.1=py_1
  - pip=18.1=py36_1000
  - pixman=0.34.0=h470a237_3
  - poppler=0.61.1=h4d7e492_4
  - poppler-data=0.4.9=0
  - postgresql=11.2=h61314c7_0
  - proj4=4.9.3=h470a237_8
  - prometheus_client=0.4.2=py_0
  - prompt_toolkit=2.0.7=py_0
  - psutil=5.4.8=py36h470a237_0
  - psycopg2=2.8.1=py36h72c5cf5_0
  - pthread-stubs=0.4=h470a237_1
  - ptyprocess=0.6.0=py36_1000
  - pycparser=2.19=py_0
  - pygments=2.2.0=py_1
  - pyopenssl=18.0.0=py36_1000
  - pyparsing=2.3.0=py_0
  - pyproj=1.9.5.1=py36h508ed2a_6
  - pyqt=4.11.4=py36_3
  - pyrsistent=0.14.5=py36h470a237_1
  - pysal=1.14.4.post2=py36_1001
  - pyshp=1.2.12=py_2
  - pysocks=1.6.8=py36_1002
  - python=3.6.7=h0371630_0
  - python-dateutil=2.7.5=py_0
  - pytz=2018.7=py_0
  - pyyaml=3.13=py36h470a237_1
  - pyzmq=17.1.2=py36hae99301_1
  - qt=4.8.7=2
  - qtconsole=4.4.2=py_1
  - readline=7.0=haf1bffa_1
  - rtree=0.8.3=py36_1000
  - s3transfer=0.1.13=py36_1001
  - scipy=1.1.0=py36hc49cb51_0
  - send2trash=1.5.0=py_0
  - setuptools=40.5.0=py36_0
  - shapely=1.6.4=py36h164cb2d_1
  - simplejson=3.16.1=py36h470a237_0
  - simplekml=1.3.0=py_1
  - sip=4.18=py36_1
  - six=1.11.0=py36_1001
  - snuggs=1.4.1=py_1
  - sortedcontainers=2.0.5=py_0
  - sqlalchemy=1.2.15=py36h470a237_0
  - sqlite=3.25.2=hb1c47c0_0
  - statsmodels=0.9.0=py36h7eb728f_0
  - tabulate=0.8.2=py_0
  - tblib=1.3.2=py_1
  - terminado=0.8.1=py36_1001
  - testpath=0.4.2=py36_1000
  - tk=8.6.9=h84994c4_1001
  - toolz=0.9.0=py_1
  - tornado=5.1.1=py36h470a237_0
  - traitlets=4.3.2=py36_1000
  - tzcode=2018g=h14c3975_1001
  - uncertainties=3.0.3=py36_1000
  - urllib3=1.24.1=py36_1000
  - wcwidth=0.1.7=py_1
  - wheel=0.32.2=py36_0
  - widgetsnbextension=3.4.2=py36_1000
  - xarray=0.11.0=py36_1000
  - xerces-c=3.2.0=h5d6a6da_2
  - xorg-kbproto=1.0.7=h470a237_2
  - xorg-libice=1.0.9=h470a237_4
  - xorg-libsm=1.2.3=h8c8a85c_0
  - xorg-libx11=1.6.6=h470a237_0
  - xorg-libxau=1.0.8=h470a237_6
  - xorg-libxdmcp=1.1.2=h470a237_7
  - xorg-libxext=1.3.3=h470a237_4
  - xorg-libxrender=0.9.10=h470a237_2
  - xorg-renderproto=0.11.1=h470a237_2
  - xorg-xextproto=7.3.0=h470a237_2
  - xorg-xproto=7.0.31=h470a237_7
  - xz=5.2.4=h470a237_1
  - yaml=0.1.7=h470a237_1
  - zeromq=4.2.5=hfc679d8_6
  - zict=0.1.3=py_0
  - zlib=1.2.11=h470a237_3
  - pip:
    - bleach==3.0.2
    - chardet==3.0.4
    - future==0.17.1
    - grass-session==0.1
    - ical2orgpy==0.2.0
    - icalendar==4.0.3
    - mpmath==1.1.0
    - pycwt==0.3.0a22
    - seaborn==0.9.0
    - sty==1.0.0b9
    - sympy==1.3
    - tqdm==4.28.1
    - tzlocal==1.5.1
    - webencodings==0.5.1
    - xlrd==1.2.0
prefix: /home/kdm/local/anaconda/envs/sci

#+end_example

*** LaTeX
#+BEGIN_SRC sh :cmdline "-i" :results verbatim
pdflatex --version
#+END_SRC
#+RESULTS:
#+begin_example
pdfTeX 3.14159265-2.6-1.40.18 (TeX Live 2017/Debian)
kpathsea version 6.2.3
Copyright 2017 Han The Thanh (pdfTeX) et al.
There is NO warranty.  Redistribution of this software is
covered by the terms of both the pdfTeX copyright and
the Lesser GNU General Public License.
For more information about these matters, see the file
named COPYING and the pdfTeX source.
Primary author of pdfTeX: Han The Thanh (pdfTeX) et al.
Compiled with libpng 1.6.34; using libpng 1.6.34
Compiled with zlib 1.2.11; using zlib 1.2.11
Compiled with poppler version 0.62.0
#+end_example

* LaTeX Setup                                           :noexport:
#+NAME: copernicus-latex-setup
#+BEGIN_SRC elisp
(add-to-list 'org-latex-classes
               `("copernicus"
                 "\\documentclass{copernicus}
               [NO-DEFAULT-PACKAGES]
               [NO-PACKAGES]
               [NO-EXTRA]"
                 ("\\section{%s}" . "\\section*{%s}")
                 ("\\subsection{%s}" . "\\subsection*{%s}")
                 ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
                 ("\\paragraph{%s}" . "\\paragraph*{%s}")
                 ("\\subparagraph{%s}" . "\\subparagraph*{%s}"))
               )

;; (org-add-link-type
;;  "citet"  (lambda (key) (org-open-file cby-references-file t nil key))
;;  (lambda (path desc format)
;;    (cond
;;     ((eq format 'latex) (format "\\citet{%s}" path))
;;     ((eq format 'ascii) (format "%s" desc))
;;     )))
;; (org-add-link-type
;;  "citep"  (lambda (key) (org-open-file cby-references-file t nil key))
;;  (lambda (path desc format)
;;    (cond
;;     ((eq format 'latex) (format "\\citep{%s}" path))
;;     ((eq format 'ascii) (format "%s" desc))
;;     )))

(setq-local org-latex-title-command "")
#+END_SRC
#+RESULTS: copernicus-latex-setup

* TODO QC                                               :noexport:

(langtool-check)
(langtool-correct-buffer)
(langtool-check-done)

Export as ASCII, then,

#+BEGIN_SRC elisp :results none :eval no-export
(setq org-ascii-text-width 80)
(org-ascii-export-to-ascii)
#+END_SRC

#+BEGIN_SRC bash :cmdline "-i" :results output :eval no-export :exports none
this='ice_discharge.txt'
# aspell list < $this | sort | uniq
echo "\n"

declare -a cmds=("style" "diction -s")
for cmd in "${cmds[@]}"; do
    echo "###\n### $cmd\n###"
    #echo $cmd $this
    ${cmd} ${this}
    echo "\n"
done
#+END_SRC
#+RESULTS:

* LaTeXdiff                                             :noexport:
#+BEGIN_SRC sh :results verbatim :results none :eval no-export

OLD=ice_discharge.2537c3a.tex
NEW=ice_discharge.tex
latexdiff --disable-citation-markup --append-safecmd="textcite,autocite" --config="PICTUREENV=(?:picture|DIFnomarkup|tabular)[\w\d*@]*" $OLD $NEW > diff.tex

# NOTE: Stil requires some manual editing of diff.tex, particularly
# when \DIFDEL and \DIFADD are inside CITE commands.

# latexmk diff.tex
#+END_SRC
#+RESULTS:

* Release                                               :noexport:
** Share w/ Coauthors
#+BEGIN_SRC sh :results verbatim
bibexport -o ice_discharge.bib ice_discharge.aux
pandoc -f latex -i ice_discharge.tex -t DOCX -o ice_discharge.docx --bibliography ./ice_discharge.bib

zip -r ice_discharge.zip ice_discharge.pdf ice_discharge.docx ice_discharge.tex ice_discharge.org
mv ice_discharge.zip ~/Dropbox/out/ice_discharge.zip

alias pbcopy='xsel --clipboard --input'
alias pbpaste='xsel --clipboard --output'
dropbox sharelink ~/Dropbox/out/ice_discharge.zip | pbcopy; pbpaste
#+END_SRC
#+RESULTS:
: This is BibTeX, Version 0.99d (TeX Live 2017/Debian)
: The top-level auxiliary file: bibexp.1548456057.aux
: The style file: export.bst
: Database file #1: /home/kdm/Documents/Papers/library.bib
:   adding: ice_discharge.pdf (deflated 11%)
:   adding: ice_discharge.docx (deflated 0%)
: https://www.dropbox.com/s/opxvww4sty3sv5v/ice_discharge.zip?dl=0
